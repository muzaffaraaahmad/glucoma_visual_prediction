{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:47:26.641075Z",
          "iopub.status.busy": "2025-11-08T09:47:26.640756Z",
          "iopub.status.idle": "2025-11-08T09:47:26.659110Z",
          "shell.execute_reply": "2025-11-08T09:47:26.658596Z",
          "shell.execute_reply.started": "2025-11-08T09:47:26.641051Z"
        },
        "id": "Xh1u7S0S7oiz"
      },
      "source": [
        "# 1. CFP Images ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:12.542505Z",
          "iopub.status.busy": "2025-11-10T05:17:12.542179Z",
          "iopub.status.idle": "2025-11-10T05:17:12.550563Z",
          "shell.execute_reply": "2025-11-10T05:17:12.550006Z",
          "shell.execute_reply.started": "2025-11-10T05:17:12.542470Z"
        },
        "id": "CbDL2jMo7oi3"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "CSV_PATH = \"./filtered_glaucoma.csv\"\n",
        "IMG_ROOT = \"./glaucoma_data/CFPs\"  # <-- CFP images folder\n",
        "CHECK_DIR = \"./checkpoints\"\n",
        "CFP_DIR = \"./glaucoma_data/ROI images\"  # <-- ROI images folder\n",
        "ROI_DIR = \"./glaucoma_data/ROI images\"  # ROI images folder\n",
        "JSON_DIR = \"./glaucoma_data/json\"  # LabelMe JSON files matching image names\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59\n",
        "PATIENCE = 10\n",
        "MIN_DELTA = 0.01\n",
        "\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gqHheCIt7oi6"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(\"./glaucoma_data.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"./\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:15.582725Z",
          "iopub.status.busy": "2025-11-10T05:17:15.582037Z",
          "iopub.status.idle": "2025-11-10T05:17:15.592790Z",
          "shell.execute_reply": "2025-11-10T05:17:15.592080Z",
          "shell.execute_reply.started": "2025-11-10T05:17:15.582701Z"
        },
        "id": "XPxPKwCZ7oi7"
      },
      "outputs": [],
      "source": [
        "# ---- tiny CSV reader (no pandas) ------------------------------------------------\n",
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]  # ensure equal length\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ---- detect columns --------------------------------------------------------------\n",
        "IMAGE_COLS_CANDIDATES = [\"image\", \"image_name\", \"img\", \"image_path\", \"filename\", \"file\"]\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]):\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # find image column\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # pick 59 VF columns: prefer v1..v59\n",
        "    vf_cols = [f\"v{i}\" for i in range(1, NUM_POINTS + 1)]\n",
        "    if all(c in cols for c in vf_cols):\n",
        "        return image_col, vf_cols\n",
        "\n",
        "    # fallback: numeric columns\n",
        "    candidates = []\n",
        "    for c in cols:\n",
        "        if c == image_col:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            candidates.append(c)\n",
        "\n",
        "    if len(candidates) < NUM_POINTS:\n",
        "        raise ValueError(\"Not enough numeric VF columns detected.\")\n",
        "\n",
        "    # sort by trailing number if exists\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    candidates_sorted = sorted(candidates, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, candidates_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:18.937424Z",
          "iopub.status.busy": "2025-11-10T05:17:18.936885Z",
          "iopub.status.idle": "2025-11-10T05:17:18.944011Z",
          "shell.execute_reply": "2025-11-10T05:17:18.943261Z",
          "shell.execute_reply.started": "2025-11-10T05:17:18.937404Z"
        },
        "id": "5jcDmFwV7oi9"
      },
      "outputs": [],
      "source": [
        "class CFPDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows: List[Dict[str, str]], image_col: str, vf_cols: List[str], train: bool\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.train = train\n",
        "\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "\n",
        "        self.tf = transforms.Compose(\n",
        "            [transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(), *aug, normalize]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        name = r[self.image_col]\n",
        "\n",
        "        path = name\n",
        "        if not os.path.isabs(path):\n",
        "            if os.path.basename(path) == path:\n",
        "                path = os.path.join(IMG_ROOT, path)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        x = self.tf(img)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:22.158208Z",
          "iopub.status.busy": "2025-11-10T05:17:22.157482Z",
          "iopub.status.idle": "2025-11-10T05:17:22.163262Z",
          "shell.execute_reply": "2025-11-10T05:17:22.162535Z",
          "shell.execute_reply.started": "2025-11-10T05:17:22.158181Z"
        },
        "id": "dN9j3rRZ7oi-"
      },
      "outputs": [],
      "source": [
        "class ResNet50VF(nn.Module):\n",
        "    def __init__(self, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet50(\n",
        "            weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        )\n",
        "        in_f = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25), nn.Linear(512, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x)\n",
        "        return self.regressor(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:25.577607Z",
          "iopub.status.busy": "2025-11-10T05:17:25.577128Z",
          "iopub.status.idle": "2025-11-10T05:17:25.584146Z",
          "shell.execute_reply": "2025-11-10T05:17:25.583424Z",
          "shell.execute_reply.started": "2025-11-10T05:17:25.577575Z"
        },
        "id": "inKMIwlL7oi_"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = crit(pred, y)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        n += bs\n",
        "\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:29.978088Z",
          "iopub.status.busy": "2025-11-10T05:17:29.977820Z",
          "iopub.status.idle": "2025-11-10T05:28:21.822456Z",
          "shell.execute_reply": "2025-11-10T05:28:21.821528Z",
          "shell.execute_reply.started": "2025-11-10T05:17:29.978068Z"
        },
        "id": "dpFU0ISB7ojA",
        "lines_to_next_cell": 2,
        "outputId": "a1a5d457-6bf0-4d9b-da63-4815065b4e3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] using 59 VF columns: ['AGE', 'CCT', 'IOP_y', 'Interval Years', 'MD'] ... ['VF50', 'VF51', 'VF52', 'VF53', 'VF54']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 189MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 01 | train_loss=5279.4962  train_pMAE=27.798  train_msMAE=27.572 || val_loss=4391.5303  val_pMAE=21.721  val_msMAE=20.175\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_cfp.pth (pMAE=21.721)\n",
            "\n",
            "Epoch 02 | train_loss=3520.6452  train_pMAE=17.991  train_msMAE=11.243 || val_loss=1959.3061  val_pMAE=23.117  val_msMAE=12.440\n",
            "\n",
            "Epoch 03 | train_loss=1519.3102  train_pMAE=13.479  train_msMAE=4.322 || val_loss=629.4594  val_pMAE=8.566  val_msMAE=4.179\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_cfp.pth (pMAE=8.566)\n",
            "\n",
            "Epoch 04 | train_loss=490.9049  train_pMAE=11.761  train_msMAE=4.048 || val_loss=244.2410  val_pMAE=7.332  val_msMAE=4.282\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_cfp.pth (pMAE=7.332)\n",
            "\n",
            "Epoch 05 | train_loss=248.6767  train_pMAE=11.324  train_msMAE=3.659 || val_loss=130.6092  val_pMAE=6.377  val_msMAE=3.671\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_cfp.pth (pMAE=6.377)\n",
            "\n",
            "Epoch 06 | train_loss=217.9609  train_pMAE=11.006  train_msMAE=3.636 || val_loss=110.8673  val_pMAE=6.186  val_msMAE=3.624\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_cfp.pth (pMAE=6.186)\n",
            "\n",
            "Epoch 07 | train_loss=207.3886  train_pMAE=10.717  train_msMAE=3.534 || val_loss=108.5461  val_pMAE=6.527  val_msMAE=4.089\n",
            "\n",
            "Epoch 08 | train_loss=198.6494  train_pMAE=10.528  train_msMAE=3.675 || val_loss=119.1239  val_pMAE=6.565  val_msMAE=4.120\n",
            "\n",
            "Epoch 09 | train_loss=196.8752  train_pMAE=10.410  train_msMAE=3.531 || val_loss=124.3600  val_pMAE=6.420  val_msMAE=3.896\n",
            "\n",
            "Epoch 10 | train_loss=186.8043  train_pMAE=10.152  train_msMAE=3.383 || val_loss=104.6765  val_pMAE=6.093  val_msMAE=3.628\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_cfp.pth (pMAE=6.093)\n",
            "\n",
            "Epoch 11 | train_loss=183.7621  train_pMAE=10.038  train_msMAE=3.577 || val_loss=111.9614  val_pMAE=6.446  val_msMAE=3.985\n",
            "\n",
            "Epoch 12 | train_loss=177.2006  train_pMAE=9.893  train_msMAE=3.398 || val_loss=123.5175  val_pMAE=6.705  val_msMAE=4.289\n",
            "\n",
            "Epoch 13 | train_loss=174.7530  train_pMAE=9.719  train_msMAE=3.413 || val_loss=103.4550  val_pMAE=5.887  val_msMAE=3.248\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_cfp.pth (pMAE=5.887)\n",
            "\n",
            "Epoch 14 | train_loss=169.6928  train_pMAE=9.652  train_msMAE=3.476 || val_loss=103.0292  val_pMAE=5.972  val_msMAE=3.551\n",
            "\n",
            "Epoch 15 | train_loss=164.9832  train_pMAE=9.439  train_msMAE=3.371 || val_loss=100.8618  val_pMAE=6.070  val_msMAE=3.674\n",
            "\n",
            "Epoch 16 | train_loss=161.3050  train_pMAE=9.320  train_msMAE=3.501 || val_loss=103.3329  val_pMAE=6.164  val_msMAE=3.668\n",
            "\n",
            "Epoch 17 | train_loss=154.1514  train_pMAE=9.090  train_msMAE=3.151 || val_loss=109.9972  val_pMAE=6.498  val_msMAE=4.124\n",
            "\n",
            "Epoch 18 | train_loss=151.0119  train_pMAE=8.995  train_msMAE=3.334 || val_loss=98.3943  val_pMAE=5.940  val_msMAE=3.551\n",
            "\n",
            "Epoch 19 | train_loss=152.5653  train_pMAE=8.991  train_msMAE=3.307 || val_loss=101.5169  val_pMAE=6.103  val_msMAE=3.737\n",
            "\n",
            "Epoch 20 | train_loss=149.4273  train_pMAE=8.911  train_msMAE=3.262 || val_loss=99.8344  val_pMAE=6.073  val_msMAE=3.722\n",
            "\n",
            "Epoch 21 | train_loss=146.9899  train_pMAE=8.819  train_msMAE=3.262 || val_loss=105.9807  val_pMAE=6.332  val_msMAE=4.000\n",
            "\n",
            "Epoch 22 | train_loss=148.2129  train_pMAE=8.894  train_msMAE=3.426 || val_loss=96.7733  val_pMAE=5.891  val_msMAE=3.480\n",
            "\n",
            "Epoch 23 | train_loss=143.9693  train_pMAE=8.711  train_msMAE=3.200 || val_loss=100.2670  val_pMAE=6.032  val_msMAE=3.693\n",
            "\n",
            "Epoch 24 | train_loss=146.5555  train_pMAE=8.751  train_msMAE=3.284 || val_loss=97.8615  val_pMAE=5.959  val_msMAE=3.620\n",
            "\n",
            "Early stopping at epoch 24 (best val pMAE=5.887)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- reproducibility (same as before)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class EarlyStopper:\n",
        "    \"\"\"\n",
        "    - Saves whenever val_metric strictly improves over 'best' (tolerance=1e-12).\n",
        "    - Uses 'min_delta' only to decide whether to reset patience (ref metric).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best_save = float(\"inf\")  # for checkpoint saving (any improvement)\n",
        "        self.best_ref = float(\"inf\")  # for patience (needs >= min_delta improvement)\n",
        "        self.bad_epochs = 0\n",
        "        if self.ckpt_path:\n",
        "            os.makedirs(os.path.dirname(self.ckpt_path), exist_ok=True)\n",
        "\n",
        "    def update(self, val_metric, model, epoch_meta=None):\n",
        "        saved = False\n",
        "        # --- Save on ANY strict improvement\n",
        "        if val_metric < self.best_save - 1e-12:\n",
        "            self.best_save = val_metric\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best_save,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            saved = True\n",
        "\n",
        "        # --- Early-stopping patience uses min_delta\n",
        "        if val_metric < self.best_ref - self.min_delta:\n",
        "            self.best_ref = val_metric\n",
        "            self.bad_epochs = 0\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "\n",
        "        should_stop = self.bad_epochs > self.patience\n",
        "        return should_stop, saved\n",
        "\n",
        "\n",
        "def main():\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] using {len(vf_cols)} VF columns: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # split\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    random.shuffle(rows)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    train_ds = CFPDataset(train_rows, image_col, vf_cols, train=True)\n",
        "    val_ds = CFPDataset(val_rows, image_col, vf_cols, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = ResNet50VF(out_dim=NUM_POINTS, pretrained=True).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt_path = os.path.join(CHECK_DIR, \"best_resnet50_original_cfp.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt_path)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        print(\n",
        "            f\"\\nEpoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_msMAE={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_msMAE={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # scheduler on validation MAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        # early stopping + save best\n",
        "        should_stop, saved = stopper.update(\n",
        "            va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch}\n",
        "        )\n",
        "        if saved:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt_path} (pMAE={stopper.best_save:.3f})\")\n",
        "        if should_stop:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (best val pMAE={stopper.best_save:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best before returning\n",
        "    state = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt_path\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, train_dl, val_dl, image_col, vf_cols, CKPT = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:29:03.478902Z",
          "iopub.status.busy": "2025-11-10T05:29:03.478577Z",
          "iopub.status.idle": "2025-11-10T05:29:07.443091Z",
          "shell.execute_reply": "2025-11-10T05:29:07.442164Z",
          "shell.execute_reply.started": "2025-11-10T05:29:03.478871Z"
        },
        "id": "oUgfb42s7ojC",
        "lines_to_next_cell": 2,
        "outputId": "9e2d48b9-ef3c-4074-b0fb-dc7cab0bf31a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Using BEST checkpoint: ./checkpoints/best_resnet50_original_cfp.pth\n",
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n"
          ]
        }
      ],
      "source": [
        "# --- reload BEST checkpoint and evaluate ---\n",
        "assert \"CKPT\" in globals(), (\n",
        "    \"CKPT not found. Make sure you ran the training cell that returns CKPT.\"\n",
        ")\n",
        "assert \"val_dl\" in globals(), (\n",
        "    \"val_dl not found. Make sure you ran the training cell that defines val_dl.\"\n",
        ")\n",
        "\n",
        "# rebuild the exact architecture\n",
        "best_model = ResNet50VF(out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "\n",
        "state = torch.load(CKPT, map_location=DEVICE)\n",
        "best_model.load_state_dict(state[\"model\"])\n",
        "best_model.eval()\n",
        "\n",
        "# collect predictions on the validation set\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in val_dl:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        p = best_model(x)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "\n",
        "print(\"✅ Using BEST checkpoint:\", CKPT)\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:29:10.997372Z",
          "iopub.status.busy": "2025-11-10T05:29:10.997079Z",
          "iopub.status.idle": "2025-11-10T05:29:11.006552Z",
          "shell.execute_reply": "2025-11-10T05:29:11.005959Z",
          "shell.execute_reply.started": "2025-11-10T05:29:10.997345Z"
        },
        "id": "1-sBquo27ojD",
        "lines_to_next_cell": 2,
        "outputId": "22e5d2c4-185c-4ebc-c489-6a998ea3b984"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== POINTWISE ==\n",
            "RMSE: 10.1713 | MAE: 5.8873 | R²: 0.9783\n",
            "== POINTWISE-MEAN ==\n",
            "RMSE: 4.6600 | MAE: 3.2482 | R²: 0.1998\n",
            "== MS (same as pointwise-mean) ==\n",
            "RMSE: 4.6600 | MAE: 3.2482 | R²: 0.1998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_val(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "print(\"\\n== POINTWISE ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(pw_true, pw_pred):.4f} | MAE: {mae_val(pw_true, pw_pred):.4f} | R²: {r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "\n",
        "t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "print(\"== POINTWISE-MEAN ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | MAE: {mae_val(t_mean, p_mean):.4f} | R²: {r2(t_mean, p_mean):.4f}\"\n",
        ")\n",
        "\n",
        "print(\"== MS (same as pointwise-mean) ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | MAE: {mae_val(t_mean, p_mean):.4f} | R²: {r2(t_mean, p_mean):.4f}\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.117107Z",
          "iopub.status.busy": "2025-11-08T09:56:44.116818Z",
          "iopub.status.idle": "2025-11-08T09:56:44.137012Z",
          "shell.execute_reply": "2025-11-08T09:56:44.136317Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.117090Z"
        },
        "id": "Q2BNKhNd7ojE"
      },
      "source": [
        "# 2. ROI Images ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.138029Z",
          "iopub.status.busy": "2025-11-08T09:56:44.137733Z",
          "iopub.status.idle": "2025-11-08T09:56:44.156020Z",
          "shell.execute_reply": "2025-11-08T09:56:44.155523Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.138001Z"
        },
        "id": "gCqdq7Fr7ojE",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.156862Z",
          "iopub.status.busy": "2025-11-08T09:56:44.156704Z",
          "iopub.status.idle": "2025-11-08T09:56:44.195467Z",
          "shell.execute_reply": "2025-11-08T09:56:44.194795Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.156849Z"
        },
        "id": "hlwRj7mu7ojE"
      },
      "outputs": [],
      "source": [
        "# ===================== LABELME JSON → OD/OC MASKS =====================\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _poly_area(pts):\n",
        "    x = [p[0] for p in pts]\n",
        "    y = [p[1] for p in pts]\n",
        "    return 0.5 * abs(\n",
        "        sum(x[i] * y[(i + 1) % len(pts)] - x[(i + 1) % len(pts)] * y[i] for i in range(len(pts)))\n",
        "    )\n",
        "\n",
        "\n",
        "def _read_labelme(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        label = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = [(float(x), float(y)) for x, y in sh.get(\"points\", [])]\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if label in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        elif label in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=_poly_area)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=_poly_area)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json_path(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        cand = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(cand):\n",
        "            return cand\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def build_masks_from_labelme(img_pil: Image.Image, img_name: str, out_size: int):\n",
        "    W, H = img_pil.size\n",
        "    od_mask = Image.new(\"L\", (W, H), 0)\n",
        "    oc_mask = Image.new(\"L\", (W, H), 0)\n",
        "\n",
        "    jpath = _guess_json_path(img_name)\n",
        "    if jpath:\n",
        "        try:\n",
        "            od_polys, oc_polys = _read_labelme(jpath)\n",
        "            d_od = ImageDraw.Draw(od_mask)\n",
        "            d_oc = ImageDraw.Draw(oc_mask)\n",
        "            for poly in od_polys:\n",
        "                d_od.polygon(poly, outline=1, fill=1)\n",
        "            for poly in oc_polys:\n",
        "                d_oc.polygon(poly, outline=1, fill=1)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] parsing {jpath}: {e}\")\n",
        "\n",
        "    od_mask = od_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    oc_mask = oc_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    return od_mask, oc_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.196439Z",
          "iopub.status.busy": "2025-11-08T09:56:44.196246Z",
          "iopub.status.idle": "2025-11-08T09:56:44.218107Z",
          "shell.execute_reply": "2025-11-08T09:56:44.217506Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.196424Z"
        },
        "id": "ejZ-EC7n7ojF"
      },
      "outputs": [],
      "source": [
        "class CFPDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows: List[Dict[str, str]], image_col: str, vf_cols: List[str], train: bool\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.train = train\n",
        "\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "\n",
        "        self.tf = transforms.Compose(\n",
        "            [transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(), *aug, normalize]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        name = r[self.image_col]\n",
        "\n",
        "        path = name\n",
        "        if not os.path.isabs(path):\n",
        "            if os.path.basename(path) == path:\n",
        "                path = os.path.join(CFP_DIR, path)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        x = self.tf(img)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.219231Z",
          "iopub.status.busy": "2025-11-08T09:56:44.218972Z",
          "iopub.status.idle": "2025-11-08T09:56:44.240915Z",
          "shell.execute_reply": "2025-11-08T09:56:44.240375Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.219208Z"
        },
        "id": "yaeL9xPz7ojG"
      },
      "outputs": [],
      "source": [
        "class ResNet50VF(nn.Module):\n",
        "    def __init__(self, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet50(\n",
        "            weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        )\n",
        "        in_f = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25), nn.Linear(512, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x)\n",
        "        return self.regressor(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.243731Z",
          "iopub.status.busy": "2025-11-08T09:56:44.243485Z",
          "iopub.status.idle": "2025-11-08T09:56:44.256968Z",
          "shell.execute_reply": "2025-11-08T09:56:44.256437Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.243714Z"
        },
        "id": "6l3umTu17ojG"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = crit(pred, y)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        n += bs\n",
        "\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:29:55.027957Z",
          "iopub.status.busy": "2025-11-10T05:29:55.027457Z",
          "iopub.status.idle": "2025-11-10T05:40:46.560388Z",
          "shell.execute_reply": "2025-11-10T05:40:46.559609Z",
          "shell.execute_reply.started": "2025-11-10T05:29:55.027932Z"
        },
        "id": "GIZTN-HV7ojG",
        "lines_to_next_cell": 2,
        "outputId": "245c7b1c-d34c-4e59-bced-b7bcaa62dce4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] using 59 VF columns: ['AGE', 'CCT', 'IOP_y', 'Interval Years', 'MD'] ... ['VF50', 'VF51', 'VF52', 'VF53', 'VF54']\n",
            "\n",
            "Epoch 01 | train_loss=5326.8399  train_pMAE=28.032  train_msMAE=27.838 || val_loss=3736.0818  val_pMAE=18.365  val_msMAE=14.419\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_roi.pth (pMAE=18.365)\n",
            "\n",
            "Epoch 02 | train_loss=3787.8721  train_pMAE=18.867  train_msMAE=13.268 || val_loss=2280.5428  val_pMAE=21.949  val_msMAE=10.401\n",
            "\n",
            "Epoch 03 | train_loss=1655.0521  train_pMAE=13.832  train_msMAE=4.517 || val_loss=791.0293  val_pMAE=9.407  val_msMAE=5.353\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_roi.pth (pMAE=9.407)\n",
            "\n",
            "Epoch 04 | train_loss=482.1597  train_pMAE=11.722  train_msMAE=4.105 || val_loss=389.0521  val_pMAE=8.839  val_msMAE=5.990\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_roi.pth (pMAE=8.839)\n",
            "\n",
            "Epoch 05 | train_loss=239.5193  train_pMAE=11.293  train_msMAE=3.684 || val_loss=204.2890  val_pMAE=7.541  val_msMAE=4.984\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_roi.pth (pMAE=7.541)\n",
            "\n",
            "Epoch 06 | train_loss=214.7932  train_pMAE=10.979  train_msMAE=3.638 || val_loss=107.2538  val_pMAE=6.113  val_msMAE=3.646\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_roi.pth (pMAE=6.113)\n",
            "\n",
            "Epoch 07 | train_loss=205.7812  train_pMAE=10.677  train_msMAE=3.466 || val_loss=153.0348  val_pMAE=7.527  val_msMAE=5.225\n",
            "\n",
            "Epoch 08 | train_loss=198.8717  train_pMAE=10.500  train_msMAE=3.748 || val_loss=148.1659  val_pMAE=7.107  val_msMAE=4.708\n",
            "\n",
            "Epoch 09 | train_loss=195.5425  train_pMAE=10.372  train_msMAE=3.538 || val_loss=147.9192  val_pMAE=6.830  val_msMAE=4.274\n",
            "\n",
            "Epoch 10 | train_loss=186.7422  train_pMAE=10.124  train_msMAE=3.400 || val_loss=103.5838  val_pMAE=6.044  val_msMAE=3.605\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_roi.pth (pMAE=6.044)\n",
            "\n",
            "Epoch 11 | train_loss=186.3723  train_pMAE=10.035  train_msMAE=3.540 || val_loss=127.2583  val_pMAE=6.912  val_msMAE=4.506\n",
            "\n",
            "Epoch 12 | train_loss=176.8912  train_pMAE=9.860  train_msMAE=3.419 || val_loss=122.7590  val_pMAE=6.694  val_msMAE=4.255\n",
            "\n",
            "Epoch 13 | train_loss=173.5646  train_pMAE=9.689  train_msMAE=3.426 || val_loss=102.6947  val_pMAE=6.102  val_msMAE=3.640\n",
            "\n",
            "Epoch 14 | train_loss=170.6331  train_pMAE=9.652  train_msMAE=3.531 || val_loss=110.8108  val_pMAE=6.201  val_msMAE=3.843\n",
            "\n",
            "Epoch 15 | train_loss=164.5061  train_pMAE=9.419  train_msMAE=3.395 || val_loss=102.3103  val_pMAE=6.126  val_msMAE=3.778\n",
            "\n",
            "Epoch 16 | train_loss=162.4608  train_pMAE=9.364  train_msMAE=3.507 || val_loss=99.0316  val_pMAE=6.022  val_msMAE=3.675\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_roi.pth (pMAE=6.022)\n",
            "\n",
            "Epoch 17 | train_loss=156.3232  train_pMAE=9.169  train_msMAE=3.117 || val_loss=107.5359  val_pMAE=6.337  val_msMAE=3.991\n",
            "\n",
            "Epoch 18 | train_loss=155.1517  train_pMAE=9.144  train_msMAE=3.350 || val_loss=107.8696  val_pMAE=6.256  val_msMAE=3.914\n",
            "\n",
            "Epoch 19 | train_loss=155.6504  train_pMAE=9.124  train_msMAE=3.306 || val_loss=99.3551  val_pMAE=5.994  val_msMAE=3.625\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_roi.pth (pMAE=5.994)\n",
            "\n",
            "Epoch 20 | train_loss=153.9065  train_pMAE=9.056  train_msMAE=3.319 || val_loss=119.8982  val_pMAE=6.564  val_msMAE=4.221\n",
            "\n",
            "Epoch 21 | train_loss=150.4171  train_pMAE=8.947  train_msMAE=3.271 || val_loss=119.3058  val_pMAE=6.615  val_msMAE=4.246\n",
            "\n",
            "Epoch 22 | train_loss=152.5513  train_pMAE=9.022  train_msMAE=3.434 || val_loss=100.5127  val_pMAE=6.118  val_msMAE=3.704\n",
            "\n",
            "Epoch 23 | train_loss=148.1124  train_pMAE=8.848  train_msMAE=3.304 || val_loss=107.5041  val_pMAE=6.240  val_msMAE=3.862\n",
            "\n",
            "Epoch 24 | train_loss=149.4676  train_pMAE=8.835  train_msMAE=3.382 || val_loss=100.3931  val_pMAE=5.948  val_msMAE=3.586\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_roi.pth (pMAE=5.948)\n",
            "\n",
            "Epoch 25 | train_loss=147.2360  train_pMAE=8.827  train_msMAE=3.290 || val_loss=101.4566  val_pMAE=6.068  val_msMAE=3.712\n",
            "\n",
            "Epoch 26 | train_loss=146.2099  train_pMAE=8.772  train_msMAE=3.294 || val_loss=102.8752  val_pMAE=6.160  val_msMAE=3.837\n",
            "\n",
            "Epoch 27 | train_loss=144.7152  train_pMAE=8.737  train_msMAE=3.274 || val_loss=106.5826  val_pMAE=6.256  val_msMAE=3.923\n",
            "\n",
            "Epoch 28 | train_loss=145.7926  train_pMAE=8.752  train_msMAE=3.256 || val_loss=99.5054  val_pMAE=6.013  val_msMAE=3.606\n",
            "\n",
            "Epoch 29 | train_loss=143.3733  train_pMAE=8.663  train_msMAE=3.181 || val_loss=100.5577  val_pMAE=6.045  val_msMAE=3.703\n",
            "\n",
            "Epoch 30 | train_loss=142.3739  train_pMAE=8.669  train_msMAE=3.275 || val_loss=98.6552  val_pMAE=5.941  val_msMAE=3.552\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_original_roi.pth (pMAE=5.941)\n",
            "\n",
            "Epoch 31 | train_loss=144.9523  train_pMAE=8.735  train_msMAE=3.258 || val_loss=102.4518  val_pMAE=6.048  val_msMAE=3.716\n",
            "\n",
            "Epoch 32 | train_loss=142.0167  train_pMAE=8.650  train_msMAE=3.163 || val_loss=100.3901  val_pMAE=6.025  val_msMAE=3.678\n",
            "\n",
            "Epoch 33 | train_loss=141.5078  train_pMAE=8.598  train_msMAE=3.182 || val_loss=99.7486  val_pMAE=6.078  val_msMAE=3.747\n",
            "\n",
            "Epoch 34 | train_loss=143.6232  train_pMAE=8.632  train_msMAE=3.269 || val_loss=100.4484  val_pMAE=5.986  val_msMAE=3.664\n",
            "\n",
            "Epoch 35 | train_loss=140.5794  train_pMAE=8.571  train_msMAE=3.246 || val_loss=100.1539  val_pMAE=5.983  val_msMAE=3.646\n",
            "\n",
            "Early stopping at epoch 35 (best val pMAE=5.941)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- reproducibility (same as before)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class EarlyStopper:\n",
        "    \"\"\"\n",
        "    - Saves whenever val_metric strictly improves over 'best' (tolerance=1e-12).\n",
        "    - Uses 'min_delta' only to decide whether to reset patience (ref metric).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best_save = float(\"inf\")  # for checkpoint saving (any improvement)\n",
        "        self.best_ref = float(\"inf\")  # for patience (needs >= min_delta improvement)\n",
        "        self.bad_epochs = 0\n",
        "        if self.ckpt_path:\n",
        "            os.makedirs(os.path.dirname(self.ckpt_path), exist_ok=True)\n",
        "\n",
        "    def update(self, val_metric, model, epoch_meta=None):\n",
        "        saved = False\n",
        "        # --- Save on ANY strict improvement\n",
        "        if val_metric < self.best_save - 1e-12:\n",
        "            self.best_save = val_metric\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best_save,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            saved = True\n",
        "\n",
        "        # --- Early-stopping patience uses min_delta\n",
        "        if val_metric < self.best_ref - self.min_delta:\n",
        "            self.best_ref = val_metric\n",
        "            self.bad_epochs = 0\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "\n",
        "        should_stop = self.bad_epochs > self.patience\n",
        "        return should_stop, saved\n",
        "\n",
        "\n",
        "def main():\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] using {len(vf_cols)} VF columns: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # split\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    random.shuffle(rows)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    train_ds = CFPDataset(train_rows, image_col, vf_cols, train=True)\n",
        "    val_ds = CFPDataset(val_rows, image_col, vf_cols, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = ResNet50VF(out_dim=NUM_POINTS, pretrained=True).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt_path = os.path.join(CHECK_DIR, \"best_resnet50_original_roi.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt_path)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        print(\n",
        "            f\"\\nEpoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_msMAE={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_msMAE={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # scheduler on validation MAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        # early stopping + save best\n",
        "        should_stop, saved = stopper.update(\n",
        "            va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch}\n",
        "        )\n",
        "        if saved:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt_path} (pMAE={stopper.best_save:.3f})\")\n",
        "        if should_stop:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (best val pMAE={stopper.best_save:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best before returning\n",
        "    state = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt_path\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, train_dl, val_dl, image_col, vf_cols, CKPT = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:41:26.754434Z",
          "iopub.status.busy": "2025-11-10T05:41:26.753893Z",
          "iopub.status.idle": "2025-11-10T05:41:30.190236Z",
          "shell.execute_reply": "2025-11-10T05:41:30.189451Z",
          "shell.execute_reply.started": "2025-11-10T05:41:26.754411Z"
        },
        "id": "Gj7Nfhww7ojH",
        "lines_to_next_cell": 2,
        "outputId": "e09e8e0f-7a7c-45fb-e410-92ac716107d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ ROI predictions collected: torch.Size([127, 59]) torch.Size([127, 59])\n"
          ]
        }
      ],
      "source": [
        "# reload best ROI checkpoint\n",
        "best_roi = ResNet50VF(out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "state = torch.load(CKPT, map_location=DEVICE)\n",
        "best_roi.load_state_dict(state[\"model\"])\n",
        "best_roi.eval()\n",
        "\n",
        "# collect predictions\n",
        "all_true = []\n",
        "all_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in val_dl:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        p = best_roi(x)\n",
        "\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "\n",
        "print(\"✅ ROI predictions collected:\", y_true.shape, y_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:41:33.799312Z",
          "iopub.status.busy": "2025-11-10T05:41:33.798656Z",
          "iopub.status.idle": "2025-11-10T05:41:33.808988Z",
          "shell.execute_reply": "2025-11-10T05:41:33.808148Z",
          "shell.execute_reply.started": "2025-11-10T05:41:33.799282Z"
        },
        "id": "kV2bjlsf7ojH",
        "lines_to_next_cell": 2,
        "outputId": "c071d9a6-cb26-4b7d-82d2-83a44feff33c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== ROI: POINTWISE ==\n",
            "RMSE: 9.9325\n",
            "MAE : 5.9407\n",
            "R²  : 0.9793\n",
            "\n",
            "== ROI: POINTWISE-MEAN / MS ==\n",
            "RMSE: 4.6355\n",
            "MAE : 3.5521\n",
            "R²  : 0.2082\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# helpers\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_val(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# POINTWISE\n",
        "pw_true = y_true.reshape(-1)\n",
        "pw_pred = y_pred.reshape(-1)\n",
        "\n",
        "print(\"\\n== ROI: POINTWISE ==\")\n",
        "print(f\"RMSE: {rmse(pw_true, pw_pred):.4f}\")\n",
        "print(f\"MAE : {mae_val(pw_true, pw_pred):.4f}\")\n",
        "print(f\"R²  : {r2(pw_true, pw_pred):.4f}\")\n",
        "\n",
        "# POINTWISE-MEAN / MS\n",
        "t_mean = y_true.mean(dim=1)\n",
        "p_mean = y_pred.mean(dim=1)\n",
        "\n",
        "print(\"\\n== ROI: POINTWISE-MEAN / MS ==\")\n",
        "print(f\"RMSE: {rmse(t_mean, p_mean):.4f}\")\n",
        "print(f\"MAE : {mae_val(t_mean, p_mean):.4f}\")\n",
        "print(f\"R²  : {r2(t_mean, p_mean):.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T10:00:28.759901Z",
          "iopub.status.busy": "2025-11-08T10:00:28.759605Z",
          "iopub.status.idle": "2025-11-08T10:00:28.775895Z",
          "shell.execute_reply": "2025-11-08T10:00:28.775339Z",
          "shell.execute_reply.started": "2025-11-08T10:00:28.759884Z"
        },
        "id": "nxGTZ-bF7ojH"
      },
      "source": [
        "# 3. ROI + OD/OD Segmentation ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:45:55.379375Z",
          "iopub.status.busy": "2025-11-10T05:45:55.378706Z",
          "iopub.status.idle": "2025-11-10T05:45:55.386060Z",
          "shell.execute_reply": "2025-11-10T05:45:55.385342Z",
          "shell.execute_reply.started": "2025-11-10T05:45:55.379352Z"
        },
        "id": "RZyvAGyK7ojI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# ---- paths ----\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "# ---- training ----\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:45:58.655021Z",
          "iopub.status.busy": "2025-11-10T05:45:58.654715Z",
          "iopub.status.idle": "2025-11-10T05:45:58.665138Z",
          "shell.execute_reply": "2025-11-10T05:45:58.664425Z",
          "shell.execute_reply.started": "2025-11-10T05:45:58.655000Z"
        },
        "id": "YUFsJmuM7ojI"
      },
      "outputs": [],
      "source": [
        "# ===================== CSV UTILS =====================\n",
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "IMAGE_COLS_CANDIDATES = [\"image\", \"image_name\", \"img\", \"image_path\", \"filename\", \"file\"]\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]) -> Tuple[str, List[str]]:\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # image col\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # VF cols prefer v1..v59 else numeric fallback\n",
        "    vf = [f\"v{i}\" for i in range(1, NUM_POINTS + 1)]\n",
        "    if all(c in cols for c in vf):\n",
        "        return image_col, vf\n",
        "\n",
        "    cand = []\n",
        "    for c in cols:\n",
        "        if c == image_col:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            cand.append(c)\n",
        "    if len(cand) < NUM_POINTS:\n",
        "        raise ValueError(f\"Need {NUM_POINTS} VF cols, found {len(cand)}.\")\n",
        "\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    cand = sorted(cand, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, cand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:02.177236Z",
          "iopub.status.busy": "2025-11-10T05:46:02.176683Z",
          "iopub.status.idle": "2025-11-10T05:46:02.187291Z",
          "shell.execute_reply": "2025-11-10T05:46:02.186682Z",
          "shell.execute_reply.started": "2025-11-10T05:46:02.177213Z"
        },
        "id": "ScTHdm-L7ojI"
      },
      "outputs": [],
      "source": [
        "# ===================== LABELME JSON → OD/OC MASKS =====================\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _poly_area(pts):\n",
        "    x = [p[0] for p in pts]\n",
        "    y = [p[1] for p in pts]\n",
        "    return 0.5 * abs(\n",
        "        sum(x[i] * y[(i + 1) % len(pts)] - x[(i + 1) % len(pts)] * y[i] for i in range(len(pts)))\n",
        "    )\n",
        "\n",
        "\n",
        "def _read_labelme(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        label = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = [(float(x), float(y)) for x, y in sh.get(\"points\", [])]\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if label in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        elif label in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=_poly_area)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=_poly_area)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json_path(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        cand = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(cand):\n",
        "            return cand\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def build_masks_from_labelme(img_pil: Image.Image, img_name: str, out_size: int):\n",
        "    W, H = img_pil.size\n",
        "    od_mask = Image.new(\"L\", (W, H), 0)\n",
        "    oc_mask = Image.new(\"L\", (W, H), 0)\n",
        "\n",
        "    jpath = _guess_json_path(img_name)\n",
        "    if jpath:\n",
        "        try:\n",
        "            od_polys, oc_polys = _read_labelme(jpath)\n",
        "            d_od = ImageDraw.Draw(od_mask)\n",
        "            d_oc = ImageDraw.Draw(oc_mask)\n",
        "            for poly in od_polys:\n",
        "                d_od.polygon(poly, outline=1, fill=1)\n",
        "            for poly in oc_polys:\n",
        "                d_oc.polygon(poly, outline=1, fill=1)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] parsing {jpath}: {e}\")\n",
        "\n",
        "    od_mask = od_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    oc_mask = oc_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    return od_mask, oc_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:05.752315Z",
          "iopub.status.busy": "2025-11-10T05:46:05.751834Z",
          "iopub.status.idle": "2025-11-10T05:46:05.759405Z",
          "shell.execute_reply": "2025-11-10T05:46:05.758759Z",
          "shell.execute_reply.started": "2025-11-10T05:46:05.752295Z"
        },
        "id": "Ssr6ToVr7ojI"
      },
      "outputs": [],
      "source": [
        "# ===================== DATASET (5-channel RGB+OD+OC) =====================\n",
        "class ROI_OD_OC_Dataset(Dataset):\n",
        "    def __init__(self, rows, image_col, vf_cols, train=True, img_root=ROI_DIR, img_size=IMG_SIZE):\n",
        "        self.rows, self.image_col, self.vf_cols = rows, image_col, vf_cols\n",
        "        self.train, self.img_root, self.img_size = train, img_root, img_size\n",
        "\n",
        "        norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.rgb_tf = transforms.Compose(\n",
        "            [transforms.Resize((img_size, img_size)), transforms.ToTensor(), *aug, norm]\n",
        "        )\n",
        "        self.mask_tf = transforms.ToTensor()  # L → (1,H,W) float {0,1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        fn = r[self.image_col]\n",
        "        path = fn if os.path.isabs(fn) else os.path.join(self.img_root, fn)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        od_img, oc_img = build_masks_from_labelme(img, fn, self.img_size)\n",
        "\n",
        "        x_rgb = self.rgb_tf(img)  # (3,H,W)\n",
        "        x_od = self.mask_tf(od_img)  # (1,H,W)\n",
        "        x_oc = self.mask_tf(oc_img)  # (1,H,W)\n",
        "        x = torch.cat([x_rgb, x_od, x_oc], dim=0)  # (5,H,W)\n",
        "\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:11.357022Z",
          "iopub.status.busy": "2025-11-10T05:46:11.356328Z",
          "iopub.status.idle": "2025-11-10T05:46:11.363275Z",
          "shell.execute_reply": "2025-11-10T05:46:11.362574Z",
          "shell.execute_reply.started": "2025-11-10T05:46:11.356999Z"
        },
        "id": "-0aQF7RZ7ojJ"
      },
      "outputs": [],
      "source": [
        "# ===================== MODEL (ResNet-50 with 5-ch input) =====================\n",
        "class ResNet50_5ch_VF(nn.Module):\n",
        "    def __init__(self, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        base = models.resnet50(\n",
        "            weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        )\n",
        "        # adapt conv1: 3→5 channels (init extra channels with mean RGB weights)\n",
        "        old = base.conv1\n",
        "        new = nn.Conv2d(\n",
        "            5,\n",
        "            old.out_channels,\n",
        "            kernel_size=old.kernel_size,\n",
        "            stride=old.stride,\n",
        "            padding=old.padding,\n",
        "            bias=(old.bias is not None),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            new.weight[:, :3, :, :] = old.weight\n",
        "            mean_w = old.weight.mean(dim=1, keepdim=True)\n",
        "            new.weight[:, 3:5, :, :] = mean_w.repeat(1, 2, 1, 1)\n",
        "            if old.bias is not None:\n",
        "                new.bias.copy_(old.bias)\n",
        "        base.conv1 = new\n",
        "\n",
        "        in_f = base.fc.in_features\n",
        "        base.fc = nn.Identity()\n",
        "        self.backbone = base\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25), nn.Linear(512, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x5):\n",
        "        f = self.backbone(x5)\n",
        "        return self.regressor(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:14.956708Z",
          "iopub.status.busy": "2025-11-10T05:46:14.955948Z",
          "iopub.status.idle": "2025-11-10T05:46:14.963048Z",
          "shell.execute_reply": "2025-11-10T05:46:14.962411Z",
          "shell.execute_reply.started": "2025-11-10T05:46:14.956686Z"
        },
        "id": "jLOVwDYE7ojJ"
      },
      "outputs": [],
      "source": [
        "# ===================== METRICS + EPOCH LOOP =====================\n",
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:18.407433Z",
          "iopub.status.busy": "2025-11-10T05:46:18.406736Z",
          "iopub.status.idle": "2025-11-10T05:52:34.919034Z",
          "shell.execute_reply": "2025-11-10T05:52:34.918137Z",
          "shell.execute_reply.started": "2025-11-10T05:46:18.407410Z"
        },
        "id": "rOCNxVAY7ojJ",
        "lines_to_next_cell": 2,
        "outputId": "79678215-b955-42e7-ee6a-04da9cb2b15d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] 59 VF cols: ['AGE', 'CCT', 'IOP_y', 'Interval Years', 'MD'] ... ['VF50', 'VF51', 'VF52', 'VF53', 'VF54']\n",
            "Epoch 01 | train_loss=5343.9326  train_pMAE=28.089  train_MS=27.916 || val_loss=3962.3469  val_pMAE=18.998  val_MS=15.852\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC.pth (pMAE=18.998)\n",
            "\n",
            "Epoch 02 | train_loss=3958.0553  train_pMAE=19.155  train_MS=14.042 || val_loss=2595.1078  val_pMAE=21.002  val_MS=8.539\n",
            "Epoch 03 | train_loss=1918.2791  train_pMAE=14.183  train_MS=4.310 || val_loss=1565.7373  val_pMAE=12.339  val_MS=8.831\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC.pth (pMAE=12.339)\n",
            "\n",
            "Epoch 04 | train_loss=626.9410  train_pMAE=12.173  train_MS=4.398 || val_loss=900.6656  val_pMAE=12.219  val_MS=9.576\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC.pth (pMAE=12.219)\n",
            "\n",
            "Epoch 05 | train_loss=269.6591  train_pMAE=11.560  train_MS=3.763 || val_loss=141.5822  val_pMAE=6.934  val_MS=4.537\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC.pth (pMAE=6.934)\n",
            "\n",
            "Epoch 06 | train_loss=226.0370  train_pMAE=11.195  train_MS=3.529 || val_loss=116.5804  val_pMAE=6.348  val_MS=3.723\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC.pth (pMAE=6.348)\n",
            "\n",
            "Epoch 07 | train_loss=210.0328  train_pMAE=10.901  train_MS=3.612 || val_loss=149.3032  val_pMAE=7.196  val_MS=4.759\n",
            "Epoch 08 | train_loss=203.2349  train_pMAE=10.659  train_MS=3.497 || val_loss=110.7303  val_pMAE=6.504  val_MS=4.057\n",
            "Epoch 09 | train_loss=199.2275  train_pMAE=10.590  train_MS=3.568 || val_loss=115.6406  val_pMAE=6.441  val_MS=3.953\n",
            "Epoch 10 | train_loss=192.7837  train_pMAE=10.308  train_MS=3.472 || val_loss=106.5622  val_pMAE=6.305  val_MS=3.850\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC.pth (pMAE=6.305)\n",
            "\n",
            "Epoch 11 | train_loss=190.9641  train_pMAE=10.189  train_MS=3.663 || val_loss=108.7041  val_pMAE=6.214  val_MS=3.692\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC.pth (pMAE=6.214)\n",
            "\n",
            "Epoch 12 | train_loss=183.5717  train_pMAE=10.053  train_MS=3.486 || val_loss=110.2085  val_pMAE=6.341  val_MS=3.790\n",
            "Epoch 13 | train_loss=178.6610  train_pMAE=9.871  train_MS=3.505 || val_loss=132.9880  val_pMAE=6.902  val_MS=4.351\n",
            "Epoch 14 | train_loss=170.9614  train_pMAE=9.623  train_MS=3.463 || val_loss=105.4733  val_pMAE=6.511  val_MS=4.063\n",
            "Epoch 15 | train_loss=168.2312  train_pMAE=9.527  train_MS=3.389 || val_loss=113.3638  val_pMAE=6.481  val_MS=4.193\n",
            "Epoch 16 | train_loss=164.4371  train_pMAE=9.402  train_MS=3.430 || val_loss=103.2694  val_pMAE=6.062  val_MS=3.670\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC.pth (pMAE=6.062)\n",
            "\n",
            "Epoch 17 | train_loss=161.7085  train_pMAE=9.316  train_MS=3.303 || val_loss=109.8677  val_pMAE=6.404  val_MS=4.008\n",
            "Epoch 18 | train_loss=160.2517  train_pMAE=9.256  train_MS=3.405 || val_loss=106.9072  val_pMAE=6.264  val_MS=3.981\n",
            "Epoch 19 | train_loss=159.2080  train_pMAE=9.260  train_MS=3.469 || val_loss=100.2086  val_pMAE=6.022  val_MS=3.591\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC.pth (pMAE=6.022)\n",
            "\n",
            "Epoch 20 | train_loss=155.5257  train_pMAE=9.142  train_MS=3.415 || val_loss=99.0164  val_pMAE=6.018  val_MS=3.602\n",
            "Epoch 21 | train_loss=153.7487  train_pMAE=9.042  train_MS=3.290 || val_loss=107.2952  val_pMAE=6.305  val_MS=3.905\n",
            "Epoch 22 | train_loss=153.1655  train_pMAE=9.026  train_MS=3.387 || val_loss=100.6519  val_pMAE=5.999  val_MS=3.542\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC.pth (pMAE=5.999)\n",
            "\n",
            "Epoch 23 | train_loss=149.3222  train_pMAE=8.905  train_MS=3.404 || val_loss=99.7720  val_pMAE=5.887  val_MS=3.518\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC.pth (pMAE=5.887)\n",
            "\n",
            "Epoch 24 | train_loss=150.5955  train_pMAE=8.945  train_MS=3.358 || val_loss=100.9753  val_pMAE=6.181  val_MS=3.830\n",
            "Epoch 25 | train_loss=149.5199  train_pMAE=8.838  train_MS=3.417 || val_loss=98.9413  val_pMAE=5.875  val_MS=3.439\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC.pth (pMAE=5.875)\n",
            "\n",
            "Epoch 26 | train_loss=146.7684  train_pMAE=8.782  train_MS=3.362 || val_loss=101.1056  val_pMAE=5.982  val_MS=3.547\n",
            "Epoch 27 | train_loss=145.9017  train_pMAE=8.751  train_MS=3.289 || val_loss=99.8401  val_pMAE=5.995  val_MS=3.553\n",
            "Epoch 28 | train_loss=141.7112  train_pMAE=8.631  train_MS=3.248 || val_loss=103.8396  val_pMAE=6.134  val_MS=3.720\n",
            "Epoch 29 | train_loss=143.0044  train_pMAE=8.580  train_MS=3.273 || val_loss=97.9431  val_pMAE=6.008  val_MS=3.618\n",
            "Epoch 30 | train_loss=143.6505  train_pMAE=8.593  train_MS=3.270 || val_loss=97.3064  val_pMAE=5.837  val_MS=3.496\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC.pth (pMAE=5.837)\n",
            "\n",
            "Epoch 31 | train_loss=141.3271  train_pMAE=8.522  train_MS=3.351 || val_loss=102.6165  val_pMAE=6.095  val_MS=3.697\n",
            "Epoch 32 | train_loss=139.3098  train_pMAE=8.512  train_MS=3.246 || val_loss=105.8696  val_pMAE=6.243  val_MS=3.876\n",
            "Epoch 33 | train_loss=139.1414  train_pMAE=8.464  train_MS=3.188 || val_loss=100.8383  val_pMAE=6.060  val_MS=3.699\n",
            "Epoch 34 | train_loss=138.6669  train_pMAE=8.442  train_MS=3.302 || val_loss=98.3173  val_pMAE=5.959  val_MS=3.565\n",
            "Epoch 35 | train_loss=135.2706  train_pMAE=8.337  train_MS=3.308 || val_loss=100.0812  val_pMAE=6.015  val_MS=3.665\n",
            "Epoch 36 | train_loss=134.7479  train_pMAE=8.333  train_MS=3.243 || val_loss=99.4399  val_pMAE=6.011  val_MS=3.650\n",
            "Epoch 37 | train_loss=136.4282  train_pMAE=8.387  train_MS=3.201 || val_loss=100.9618  val_pMAE=6.023  val_MS=3.668\n",
            "Epoch 38 | train_loss=135.4028  train_pMAE=8.351  train_MS=3.238 || val_loss=97.1985  val_pMAE=5.943  val_MS=3.529\n",
            "Epoch 39 | train_loss=135.7351  train_pMAE=8.376  train_MS=3.243 || val_loss=98.5772  val_pMAE=5.959  val_MS=3.576\n",
            "Epoch 40 | train_loss=134.4000  train_pMAE=8.340  train_MS=3.285 || val_loss=99.5787  val_pMAE=6.000  val_MS=3.615\n",
            "Epoch 41 | train_loss=132.1092  train_pMAE=8.255  train_MS=3.219 || val_loss=101.3998  val_pMAE=6.041  val_MS=3.682\n",
            "Early stopping at epoch 41 (best val pMAE=5.837)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ---- (optional) reproducibility on small data\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---- Early Stopping helper\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best = float(\"inf\")\n",
        "        self.bad_epochs = 0\n",
        "\n",
        "    def step(self, val_metric, model, epoch_meta=None):\n",
        "        # returns True if we should stop\n",
        "        if val_metric < self.best - self.min_delta:\n",
        "            self.best = val_metric\n",
        "            self.bad_epochs = 0\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            return False\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "            return self.bad_epochs > self.patience\n",
        "\n",
        "\n",
        "# ===================== TRAIN =====================\n",
        "def train_resnet50_roi_odoc(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01):\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] {len(vf_cols)} VF cols: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    random.shuffle(rows)\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    train_ds = ROI_OD_OC_Dataset(train_rows, image_col, vf_cols, train=True)\n",
        "    val_ds = ROI_OD_OC_Dataset(val_rows, image_col, vf_cols, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = ResNet50_5ch_VF(out_dim=NUM_POINTS, pretrained=True).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "    # ---- LR scheduler (plateau)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt = os.path.join(CHECK_DIR, \"best_resnet50_ROI_ODOC.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        # step scheduler on validation pMAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # save best (and detect improvement for pretty print)\n",
        "        prev_best = stopper.best\n",
        "        should_stop = stopper.step(va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch})\n",
        "        if stopper.best < prev_best - MIN_DELTA:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt} (pMAE={stopper.best:.3f})\\n\")\n",
        "\n",
        "        if should_stop:\n",
        "            print(f\"Early stopping at epoch {epoch} (best val pMAE={stopper.best:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best weights before returning\n",
        "    state = torch.load(ckpt, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt\n",
        "\n",
        "\n",
        "# run training and expose globals\n",
        "model_odoc, train_dl_odoc, val_dl_odoc, image_col_odoc, vf_cols_odoc, CKPT_ODOC = (\n",
        "    train_resnet50_roi_odoc(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:52:45.952603Z",
          "iopub.status.busy": "2025-11-10T05:52:45.951688Z",
          "iopub.status.idle": "2025-11-10T05:52:47.343625Z",
          "shell.execute_reply": "2025-11-10T05:52:47.342644Z",
          "shell.execute_reply.started": "2025-11-10T05:52:45.952569Z"
        },
        "id": "_S5l0PNo7ojK",
        "outputId": "bca1a5bc-0e9a-44f7-8b00-8cfd7602537e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n",
            "\n",
            "== ROI+OD/OC: POINTWISE ==\n",
            "RMSE: 9.8644\n",
            "MAE : 5.8366\n",
            "R²  : 0.9796\n",
            "\n",
            "== ROI+OD/OC: POINTWISE-MEAN / MS ==\n",
            "RMSE: 4.6664\n",
            "MAE : 3.4957\n",
            "R²  : 0.1976\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ===================== EVALUATE BEST + PAPER METRICS =====================\n",
        "# reload best\n",
        "best_odoc = ResNet50_5ch_VF(out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "state = torch.load(CKPT_ODOC, map_location=DEVICE)\n",
        "best_odoc.load_state_dict(state[\"model\"])\n",
        "best_odoc.eval()\n",
        "\n",
        "# predictions\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in val_dl_odoc:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        p = best_odoc(x)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)\n",
        "\n",
        "\n",
        "# metrics (safe names to avoid clobbering mae/ms_mae)\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_value(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "print(\"\\n== ROI+OD/OC: POINTWISE ==\")\n",
        "print(f\"RMSE: {rmse(pw_true, pw_pred):.4f}\")\n",
        "print(f\"MAE : {mae_value(pw_true, pw_pred):.4f}\")\n",
        "print(f\"R²  : {r2(pw_true, pw_pred):.4f}\")\n",
        "\n",
        "t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "print(\"\\n== ROI+OD/OC: POINTWISE-MEAN / MS ==\")\n",
        "print(f\"RMSE: {rmse(t_mean, p_mean):.4f}\")\n",
        "print(f\"MAE : {mae_value(t_mean, p_mean):.4f}\")\n",
        "print(f\"R²  : {r2(t_mean, p_mean):.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T10:04:17.910859Z",
          "iopub.status.busy": "2025-11-08T10:04:17.910580Z",
          "iopub.status.idle": "2025-11-08T10:04:17.914574Z",
          "shell.execute_reply": "2025-11-08T10:04:17.913924Z",
          "shell.execute_reply.started": "2025-11-08T10:04:17.910833Z"
        },
        "id": "3SsnipCP7ojK"
      },
      "source": [
        "# 4. ROI + Clinical Features ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:04:44.866421Z",
          "iopub.status.busy": "2025-11-10T06:04:44.865721Z",
          "iopub.status.idle": "2025-11-10T06:04:44.873765Z",
          "shell.execute_reply": "2025-11-10T06:04:44.873117Z",
          "shell.execute_reply.started": "2025-11-10T06:04:44.866399Z"
        },
        "id": "HFiFY-zv7ojK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "# ---- training ----\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59  # VF1..VF59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# clinical columns present in your CSV (plus computed ones)\n",
        "# from your columns: AGE, GENDER, IOP_y, MD exist; we’ll add computed CDR & PSD\n",
        "CLIN_NUM_COLS = [\"AGE\", \"IOP_y\", \"CDR\"]  # numeric\n",
        "CLIN_CAT_COLS = [\"GENDER\"]  # categorical (mapped to 0/1)\n",
        "IMAGE_COLS_CANDIDATES = [\n",
        "    \"Corresponding CFP\",\n",
        "    \"image\",\n",
        "    \"image_name\",\n",
        "    \"img\",\n",
        "    \"image_path\",\n",
        "    \"filename\",\n",
        "    \"file\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:04:52.367164Z",
          "iopub.status.busy": "2025-11-10T06:04:52.366737Z",
          "iopub.status.idle": "2025-11-10T06:04:52.378136Z",
          "shell.execute_reply": "2025-11-10T06:04:52.377479Z",
          "shell.execute_reply.started": "2025-11-10T06:04:52.367140Z"
        },
        "id": "lXuIhPrr7ojL"
      },
      "outputs": [],
      "source": [
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]) -> Tuple[str, List[str]]:\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # image column: prefer \"Corresponding CFP\" if present\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # prefer explicit VF1..VF59 (ignore VF0, VF60)\n",
        "    vf_cols_pref = [f\"VF{i}\" for i in range(1, 60)]\n",
        "    if all(c in cols for c in vf_cols_pref):\n",
        "        return image_col, vf_cols_pref\n",
        "\n",
        "    # fallback: numeric detection (exclude clinical & image)\n",
        "    excluded = set([image_col] + CLIN_NUM_COLS + CLIN_CAT_COLS + [\"VF0\", \"VF60\"])\n",
        "    candidates = []\n",
        "    for c in cols:\n",
        "        if c in excluded:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            candidates.append(c)\n",
        "\n",
        "    if len(candidates) < NUM_POINTS:\n",
        "        raise ValueError(\n",
        "            f\"Not enough numeric VF columns; found {len(candidates)}, need {NUM_POINTS}.\"\n",
        "        )\n",
        "\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    candidates_sorted = sorted(candidates, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, candidates_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:01.646023Z",
          "iopub.status.busy": "2025-11-10T06:05:01.645439Z",
          "iopub.status.idle": "2025-11-10T06:05:01.654860Z",
          "shell.execute_reply": "2025-11-10T06:05:01.654051Z",
          "shell.execute_reply.started": "2025-11-10T06:05:01.646002Z"
        },
        "id": "VKIo8sqk7ojL"
      },
      "outputs": [],
      "source": [
        "# ---- CDR from LabelMe polygons (vertical cup/disc ratio) ----\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _read_labelme_polys(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        lab = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = sh.get(\"points\", [])\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if lab in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        if lab in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "\n",
        "    # keep polygon with max vertical height if multiple\n",
        "    def vheight(poly):\n",
        "        ys = [p[1] for p in poly]\n",
        "        return (max(ys) - min(ys)) if ys else 0.0\n",
        "\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=vheight)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=vheight)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        p = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def compute_cdr_from_json(img_name: str):\n",
        "    \"\"\"\n",
        "    CDR = vertical height of OC / vertical height of OD.\n",
        "    Returns None if JSON missing or polygons absent.\n",
        "    \"\"\"\n",
        "    jpath = _guess_json(img_name)\n",
        "    if not jpath:\n",
        "        return None\n",
        "    try:\n",
        "        od_polys, oc_polys = _read_labelme_polys(jpath)\n",
        "        if not od_polys or not oc_polys:\n",
        "            return None\n",
        "\n",
        "        def vheight(poly):\n",
        "            ys = [float(y) for _, y in poly]\n",
        "            return max(ys) - min(ys) if ys else 0.0\n",
        "\n",
        "        h_od = vheight(od_polys[0])\n",
        "        h_oc = vheight(oc_polys[0])\n",
        "        if h_od <= 0:\n",
        "            return None\n",
        "        return float(h_oc / h_od)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] CDR parse failed for {img_name}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:08.135660Z",
          "iopub.status.busy": "2025-11-10T06:05:08.134961Z",
          "iopub.status.idle": "2025-11-10T06:05:08.139853Z",
          "shell.execute_reply": "2025-11-10T06:05:08.139232Z",
          "shell.execute_reply.started": "2025-11-10T06:05:08.135637Z"
        },
        "id": "SPviikLp7ojL"
      },
      "outputs": [],
      "source": [
        "def augment_rows_with_cdr(rows, image_col, vf_cols):\n",
        "    augmented = []\n",
        "    miss_cdr = miss_psd = 0\n",
        "    for r in rows:\n",
        "        r2 = dict(r)\n",
        "        # compute CDR from JSON polygons\n",
        "        cdr = compute_cdr_from_json(r2[image_col])\n",
        "        if cdr is None:\n",
        "            miss_cdr += 1\n",
        "        r2[\"CDR\"] = cdr\n",
        "\n",
        "        augmented.append(r2)\n",
        "    print(f\"✅ Augmented rows: CDR missing={miss_cdr}, PSD missing={miss_psd}\")\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6RqCX4Ju7ojM"
      },
      "outputs": [],
      "source": [
        "# ----------------- AUGMENT ROWS WITH CDR  -----------------\n",
        "def augment_rows_with_cdr_psd(rows, image_col, vf_cols):\n",
        "    augmented = []\n",
        "    miss_cdr = miss_psd = 0\n",
        "    for r in rows:\n",
        "        r2 = dict(r)\n",
        "        cdr = compute_cdr_from_json(r2[image_col])\n",
        "        if cdr is None:\n",
        "            miss_cdr += 1\n",
        "        r2[\"CDR\"] = cdr\n",
        "\n",
        "        augmented.append(r2)\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:11.650178Z",
          "iopub.status.busy": "2025-11-10T06:05:11.649913Z",
          "iopub.status.idle": "2025-11-10T06:05:11.657823Z",
          "shell.execute_reply": "2025-11-10T06:05:11.657129Z",
          "shell.execute_reply.started": "2025-11-10T06:05:11.650160Z"
        },
        "id": "dt2gPuTX7ojM"
      },
      "outputs": [],
      "source": [
        "def to_float(x):\n",
        "    x = str(x).strip()\n",
        "    if x == \"\":\n",
        "        return None\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def fit_clinical_stats(rows, clin_num_cols):\n",
        "    stats = {}\n",
        "    for c in clin_num_cols:\n",
        "        vals = [to_float(r.get(c, \"\")) for r in rows]\n",
        "        vals = [v for v in vals if v is not None]\n",
        "        mean = np.mean(vals) if vals else 0.0\n",
        "        std = np.std(vals) if vals else 1.0\n",
        "        if std == 0:\n",
        "            std = 1.0\n",
        "        stats[c] = (float(mean), float(std))\n",
        "    return stats\n",
        "\n",
        "\n",
        "def encode_gender(x):\n",
        "    s = str(x).strip().lower()\n",
        "    if s in (\"m\", \"male\", \"man\"):\n",
        "        return 1.0\n",
        "    if s in (\"f\", \"female\", \"woman\"):\n",
        "        return 0.0\n",
        "    return 0.5  # unknown/other\n",
        "\n",
        "\n",
        "def build_clinical_vector(r, stats):\n",
        "    vec = []\n",
        "    for c in CLIN_NUM_COLS:\n",
        "        v = to_float(r.get(c, \"\"))\n",
        "        mean, std = stats[c]\n",
        "        v = mean if v is None else v\n",
        "        v = (v - mean) / std\n",
        "        vec.append(v)\n",
        "    for c in CLIN_CAT_COLS:\n",
        "        if c == \"GENDER\":\n",
        "            vec.append(encode_gender(r.get(c, \"\")))\n",
        "        else:\n",
        "            vec.append(0.0)\n",
        "    return torch.tensor(vec, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:17.437004Z",
          "iopub.status.busy": "2025-11-10T06:05:17.436286Z",
          "iopub.status.idle": "2025-11-10T06:05:17.443298Z",
          "shell.execute_reply": "2025-11-10T06:05:17.442595Z",
          "shell.execute_reply.started": "2025-11-10T06:05:17.436978Z"
        },
        "id": "RzbmwljI7ojM"
      },
      "outputs": [],
      "source": [
        "class ROIClinicalDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows, image_col, vf_cols, clin_stats, train=True, img_root=ROI_DIR, img_size=IMG_SIZE\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.clin_stats = clin_stats\n",
        "        self.train = train\n",
        "        self.img_root = img_root\n",
        "\n",
        "        normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.tf = transforms.Compose(\n",
        "            [transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(), *aug, normalize]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.rows[idx]\n",
        "        name = r[self.image_col]\n",
        "        path = name if os.path.isabs(name) else os.path.join(self.img_root, name)\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        x_img = self.tf(img)\n",
        "\n",
        "        x_clin = build_clinical_vector(r, self.clin_stats)  # (clin_dim,)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)  # (59,)\n",
        "\n",
        "        return x_img, x_clin, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BDr4wdzu7ojM"
      },
      "outputs": [],
      "source": [
        "# ----------------- DATASET: 5-CH ROI + CLINICAL -----------------\n",
        "class ROI_ODOC_Clinical_Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows, image_col, vf_cols, clin_stats, train=True, img_root=ROI_DIR, img_size=IMG_SIZE\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.clin_stats = clin_stats\n",
        "        self.train = train\n",
        "        self.img_root = img_root\n",
        "        self.img_size = img_size\n",
        "\n",
        "        norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.rgb_tf = transforms.Compose(\n",
        "            [transforms.Resize((img_size, img_size)), transforms.ToTensor(), *aug, norm]\n",
        "        )\n",
        "        self.mask_tf = transforms.ToTensor()  # L→(1,H,W) float {0,1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.rows[idx]\n",
        "        fn = r[self.image_col]\n",
        "        path = fn if os.path.isabs(fn) else os.path.join(self.img_root, fn)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        od_img, oc_img = build_masks_from_labelme(img, fn, self.img_size)\n",
        "\n",
        "        x_rgb = self.rgb_tf(img)  # (3,H,W)\n",
        "        x_od = self.mask_tf(od_img)  # (1,H,W)\n",
        "        x_oc = self.mask_tf(oc_img)  # (1,H,W)\n",
        "        x5 = torch.cat([x_rgb, x_od, x_oc], dim=0)  # (5,H,W)\n",
        "\n",
        "        x_clin = build_clinical_vector(r, self.clin_stats)  # (clin_dim,)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)  # (59,)\n",
        "\n",
        "        return x5, x_clin, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:23.335737Z",
          "iopub.status.busy": "2025-11-10T06:05:23.335473Z",
          "iopub.status.idle": "2025-11-10T06:05:23.342292Z",
          "shell.execute_reply": "2025-11-10T06:05:23.341478Z",
          "shell.execute_reply.started": "2025-11-10T06:05:23.335717Z"
        },
        "id": "_MuFTBVv7ojN"
      },
      "outputs": [],
      "source": [
        "class ResNet50_ROI_Clinical(nn.Module):\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet50(\n",
        "            weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        )\n",
        "        in_f = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        self.img_head = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.10),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(512 + 64, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_img, x_clin):\n",
        "        f = self.backbone(x_img)  # (B, 2048)\n",
        "        f = self.img_head(f)  # (B, 512)\n",
        "        g = self.clin_head(x_clin)  # (B, 64)\n",
        "        z = torch.cat([f, g], dim=1)  # (B, 576)\n",
        "        out = self.fuse(z)  # (B, 59)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eHvXSGGo7ojN"
      },
      "outputs": [],
      "source": [
        "# ----------------- MODEL: 5-CH RESNET50 + CLINICAL MLP (FUSION) -----------------\n",
        "class ResNet50_5ch_Clinical(nn.Module):\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        base = models.resnet50(\n",
        "            weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        )\n",
        "        # adapt conv1: 3→5 channels by copying weights & using mean for extra channels\n",
        "        old = base.conv1\n",
        "        new = nn.Conv2d(\n",
        "            5,\n",
        "            old.out_channels,\n",
        "            kernel_size=old.kernel_size,\n",
        "            stride=old.stride,\n",
        "            padding=old.padding,\n",
        "            bias=(old.bias is not None),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            new.weight[:, :3, :, :] = old.weight\n",
        "            mean_w = old.weight.mean(dim=1, keepdim=True)\n",
        "            new.weight[:, 3:5, :, :] = mean_w.repeat(1, 2, 1, 1)\n",
        "            if old.bias is not None:\n",
        "                new.bias.copy_(old.bias)\n",
        "        base.conv1 = new\n",
        "\n",
        "        in_f = base.fc.in_features\n",
        "        base.fc = nn.Identity()\n",
        "        self.backbone = base\n",
        "\n",
        "        self.img_head = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25)\n",
        "        )\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.10),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(512 + 64, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x5, xclin):\n",
        "        f = self.backbone(x5)  # (B, 2048)\n",
        "        f = self.img_head(f)  # (B, 512)\n",
        "        g = self.clin_head(xclin)  # (B, 64)\n",
        "        z = torch.cat([f, g], dim=1)\n",
        "        out = self.fuse(z)  # (B, 59)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:28.785280Z",
          "iopub.status.busy": "2025-11-10T06:05:28.785012Z",
          "iopub.status.idle": "2025-11-10T06:05:28.792074Z",
          "shell.execute_reply": "2025-11-10T06:05:28.791195Z",
          "shell.execute_reply.started": "2025-11-10T06:05:28.785262Z"
        },
        "id": "WrhJSFKQ7ojN"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "    for x_img, x_clin, y in loader:\n",
        "        x_img = x_img.to(DEVICE)\n",
        "        x_clin = x_clin.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "        pred = model(x_img, x_clin)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x_img.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:52.866937Z",
          "iopub.status.busy": "2025-11-10T06:05:52.866250Z",
          "iopub.status.idle": "2025-11-10T06:09:44.041224Z",
          "shell.execute_reply": "2025-11-10T06:09:44.040337Z",
          "shell.execute_reply.started": "2025-11-10T06:05:52.866915Z"
        },
        "id": "B9RyR99O7ojN",
        "lines_to_next_cell": 2,
        "outputId": "6242417d-866d-4cb7-8e40-5eace22c1c1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] 59 VF cols: ['VF1', 'VF2', 'VF3', 'VF4', 'VF5'] ... ['VF55', 'VF56', 'VF57', 'VF58', 'VF59']\n",
            "\n",
            "Epoch 01 | train_loss=469.7573  train_pMAE=20.051  train_MS=19.870 || val_loss=231.2028  val_pMAE=13.418  val_MS=12.536\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC_CDR.pth (pMAE=13.418)\n",
            "\n",
            "Epoch 02 | train_loss=169.4940  train_pMAE=10.697  train_MS=8.587 || val_loss=62.7907  val_pMAE=6.537  val_MS=4.654\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC_CDR.pth (pMAE=6.537)\n",
            "\n",
            "Epoch 03 | train_loss=57.1025  train_pMAE=6.044  train_MS=3.513 || val_loss=43.0421  val_pMAE=4.936  val_MS=3.271\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC_CDR.pth (pMAE=4.936)\n",
            "\n",
            "Epoch 04 | train_loss=48.5160  train_pMAE=5.531  train_MS=2.781 || val_loss=43.4791  val_pMAE=5.267  val_MS=3.519\n",
            "\n",
            "Epoch 05 | train_loss=45.9752  train_pMAE=5.334  train_MS=2.690 || val_loss=46.2415  val_pMAE=5.571  val_MS=3.865\n",
            "\n",
            "Epoch 06 | train_loss=41.7267  train_pMAE=5.096  train_MS=2.375 || val_loss=41.8612  val_pMAE=5.006  val_MS=3.291\n",
            "\n",
            "Epoch 07 | train_loss=41.0789  train_pMAE=5.017  train_MS=2.323 || val_loss=41.8927  val_pMAE=5.120  val_MS=3.276\n",
            "\n",
            "Epoch 08 | train_loss=38.7362  train_pMAE=4.871  train_MS=2.082 || val_loss=38.4084  val_pMAE=4.674  val_MS=2.816\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC_CDR.pth (pMAE=4.674)\n",
            "\n",
            "Epoch 09 | train_loss=36.8523  train_pMAE=4.722  train_MS=1.903 || val_loss=37.7329  val_pMAE=4.425  val_MS=2.544\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_resnet50_ROI_ODOC_CDR.pth (pMAE=4.425)\n",
            "\n",
            "Epoch 10 | train_loss=36.9108  train_pMAE=4.744  train_MS=1.958 || val_loss=39.3562  val_pMAE=4.610  val_MS=2.758\n",
            "\n",
            "Epoch 11 | train_loss=36.3233  train_pMAE=4.694  train_MS=1.826 || val_loss=38.9657  val_pMAE=4.699  val_MS=2.826\n",
            "\n",
            "Epoch 12 | train_loss=35.9160  train_pMAE=4.656  train_MS=1.871 || val_loss=38.7866  val_pMAE=4.602  val_MS=2.696\n",
            "\n",
            "Epoch 13 | train_loss=35.4938  train_pMAE=4.632  train_MS=1.758 || val_loss=39.9872  val_pMAE=4.450  val_MS=2.622\n",
            "\n",
            "Epoch 14 | train_loss=34.2750  train_pMAE=4.552  train_MS=1.740 || val_loss=37.2977  val_pMAE=4.512  val_MS=2.584\n",
            "\n",
            "Epoch 15 | train_loss=34.8134  train_pMAE=4.590  train_MS=1.728 || val_loss=38.5918  val_pMAE=4.443  val_MS=2.580\n",
            "\n",
            "Epoch 16 | train_loss=34.5527  train_pMAE=4.549  train_MS=1.700 || val_loss=37.4768  val_pMAE=4.504  val_MS=2.636\n",
            "\n",
            "Epoch 17 | train_loss=33.1889  train_pMAE=4.441  train_MS=1.583 || val_loss=38.6344  val_pMAE=4.694  val_MS=2.899\n",
            "\n",
            "Epoch 18 | train_loss=33.0174  train_pMAE=4.443  train_MS=1.536 || val_loss=37.8939  val_pMAE=4.507  val_MS=2.632\n",
            "\n",
            "Epoch 19 | train_loss=33.7532  train_pMAE=4.487  train_MS=1.659 || val_loss=38.1160  val_pMAE=4.657  val_MS=2.802\n",
            "\n",
            "Epoch 20 | train_loss=33.0621  train_pMAE=4.436  train_MS=1.565 || val_loss=38.5476  val_pMAE=4.636  val_MS=2.800\n",
            "\n",
            "Early stopping at epoch 20 (best val pMAE=4.425)\n"
          ]
        }
      ],
      "source": [
        "def train_resnet50_roi_odoc_with_cdr(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01):\n",
        "    import os\n",
        "    import random\n",
        "\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    # --- Early stopper that saves best on val pMAE ---\n",
        "    class EarlyStopper:\n",
        "        def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "            self.patience = int(patience)\n",
        "            self.min_delta = float(min_delta)\n",
        "            self.ckpt_path = ckpt_path\n",
        "            self.best = float(\"inf\")\n",
        "            self.bad_epochs = 0\n",
        "\n",
        "        def step(self, val_pmae, model, epoch_meta=None):\n",
        "            if val_pmae < self.best - self.min_delta:\n",
        "                self.best = val_pmae\n",
        "                self.bad_epochs = 0\n",
        "                if self.ckpt_path:\n",
        "                    torch.save(\n",
        "                        {\n",
        "                            \"model\": model.state_dict(),\n",
        "                            \"val_pointwise_mae\": self.best,\n",
        "                            **(epoch_meta or {}),\n",
        "                        },\n",
        "                        self.ckpt_path,\n",
        "                    )\n",
        "                return False\n",
        "            else:\n",
        "                self.bad_epochs += 1\n",
        "                return self.bad_epochs > self.patience\n",
        "\n",
        "    # --- read & detect columns ---\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] {len(vf_cols)} VF cols: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # --- ADD CDR BEFORE SPLIT ---\n",
        "    rows = augment_rows_with_cdr_psd(rows, image_col, vf_cols)  # <-- adds CDR fields per row\n",
        "\n",
        "    random.shuffle(rows)\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    # --- fit clinical stats on TRAIN only (handles imputation/encoding) ---\n",
        "    clin_stats = fit_clinical_stats(train_rows, CLIN_NUM_COLS)\n",
        "    clin_dim = len(CLIN_NUM_COLS) + len(CLIN_CAT_COLS)\n",
        "\n",
        "    # --- datasets / loaders: use the dataset that returns (x5, x_clin, y) ---\n",
        "    train_ds = ROI_ODOC_Clinical_Dataset(train_rows, image_col, vf_cols, clin_stats, train=True)\n",
        "    val_ds = ROI_ODOC_Clinical_Dataset(val_rows, image_col, vf_cols, clin_stats, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # --- model that accepts 5ch image + clinical vector ---\n",
        "    model = ResNet50_5ch_Clinical(clin_dim=clin_dim, out_dim=NUM_POINTS, pretrained=True).to(\n",
        "        DEVICE\n",
        "    )\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt = os.path.join(CHECK_DIR, \"best_resnet50_ROI_ODOC_CDR.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)  # should read (x5, x_clin, y) inside\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        print(\n",
        "            f\"\\nEpoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # step LR on validation pMAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        # save best & decide stopping\n",
        "        improved = va[\"pointwise_mae\"] < stopper.best - MIN_DELTA\n",
        "        should_stop = stopper.step(va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch})\n",
        "        if improved:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt} (pMAE={stopper.best:.3f})\")\n",
        "\n",
        "        if should_stop:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (best val pMAE={stopper.best:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best before returning\n",
        "    state = torch.load(ckpt, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt, clin_dim\n",
        "\n",
        "\n",
        "# run training and expose globals\n",
        "model_odoc, train_dl_odoc, val_dl_odoc, image_col_odoc, vf_cols_odoc, CKPT_ODOC, CLIN_DIM_OD = (\n",
        "    train_resnet50_roi_odoc_with_cdr(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T06:31:38.896271Z",
          "iopub.status.busy": "2025-11-10T06:31:38.895650Z",
          "iopub.status.idle": "2025-11-10T06:31:39.930251Z",
          "shell.execute_reply": "2025-11-10T06:31:39.929374Z",
          "shell.execute_reply.started": "2025-11-10T06:31:38.896246Z"
        },
        "id": "86bHOnLc7ojO",
        "lines_to_next_cell": 2,
        "outputId": "afa7463c-6d6d-4ac8-d6c7-209809169479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n",
            "\n",
            "== ROI+Clinical (with CDR): POINTWISE ==\n",
            "RMSE: 6.1427 | MAE: 4.4255 | R²: 0.5477\n",
            "\n",
            "== ROI+Clinical (with CDR): MEAN SENSITIVITY ==\n",
            "RMSE: 3.5776 | MAE: 2.5437 | R²: 0.6429\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------\n",
        "# ✅ Corrected Cell 10: Reload best model & evaluate\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Use the model already trained in Cell 9\n",
        "# model_odoc → best model returned by train_resnet50_roi_odoc_with_cdr\n",
        "# val_dl_odoc → validation dataloader\n",
        "# CKPT_ODOC → checkpoint path\n",
        "# CLIN_DIM_OD → clinical feature dimension\n",
        "# image_col_odoc, vf_cols_odoc already created\n",
        "\n",
        "best_roi_clin = model_odoc  # model already returned from training\n",
        "best_roi_clin.eval()\n",
        "\n",
        "# Load the best checkpoint\n",
        "state = torch.load(CKPT_ODOC, map_location=DEVICE)\n",
        "best_roi_clin.load_state_dict(state[\"model\"])\n",
        "best_roi_clin.eval()\n",
        "\n",
        "# Collect predictions\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x_img, x_clin, y in val_dl_odoc:\n",
        "        x_img = x_img.to(DEVICE)\n",
        "        x_clin = x_clin.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        p = best_roi_clin(x_img, x_clin)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)\n",
        "\n",
        "\n",
        "# --- Metrics ---\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_value(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# Pointwise metrics (VF1..VF59 flattened)\n",
        "pw_true = y_true.reshape(-1)\n",
        "pw_pred = y_pred.reshape(-1)\n",
        "\n",
        "print(\"\\n== ROI+Clinical (with CDR): POINTWISE ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(pw_true, pw_pred):.4f} | \"\n",
        "    f\"MAE: {mae_value(pw_true, pw_pred):.4f} | \"\n",
        "    f\"R²: {r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "\n",
        "# Mean Sensitivity metrics\n",
        "t_mean = y_true.mean(dim=1)\n",
        "p_mean = y_pred.mean(dim=1)\n",
        "\n",
        "print(\"\\n== ROI+Clinical (with CDR): MEAN SENSITIVITY ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | \"\n",
        "    f\"MAE: {mae_value(t_mean, p_mean):.4f} | \"\n",
        "    f\"R²: {r2(t_mean, p_mean):.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T10:08:07.655701Z",
          "iopub.status.busy": "2025-11-08T10:08:07.655425Z",
          "iopub.status.idle": "2025-11-08T10:08:07.659428Z"
        },
        "id": "f5EerqUJ7ojO"
      },
      "source": [
        "# 5. ROI+ OD/OC + Clinical ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:33.300575Z",
          "iopub.status.busy": "2025-11-10T04:06:33.299912Z",
          "iopub.status.idle": "2025-11-10T04:06:33.432177Z",
          "shell.execute_reply": "2025-11-10T04:06:33.431611Z",
          "shell.execute_reply.started": "2025-11-10T04:06:33.300551Z"
        },
        "id": "LXQg62sC7ojP",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# FULL FUSION: ROI + OD/OC + Clinical   →  VF (59)\n",
        "# ==========================\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# ----------------- PATHS & CONFIG -----------------\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59  # VF1..VF59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:36.730860Z",
          "iopub.status.busy": "2025-11-10T04:06:36.730557Z",
          "iopub.status.idle": "2025-11-10T04:06:36.740736Z",
          "shell.execute_reply": "2025-11-10T04:06:36.740178Z",
          "shell.execute_reply.started": "2025-11-10T04:06:36.730834Z"
        },
        "id": "gNGGkad97ojP"
      },
      "outputs": [],
      "source": [
        "# Clinical features for fusion (present or computed)\n",
        "CLIN_NUM_COLS = [\"AGE\", \"IOP_y\", \"CDR\"]\n",
        "CLIN_CAT_COLS = [\"GENDER\"]\n",
        "IMAGE_COLS_CANDIDATES = [\n",
        "    \"Corresponding CFP\",\n",
        "    \"image\",\n",
        "    \"image_name\",\n",
        "    \"img\",\n",
        "    \"image_path\",\n",
        "    \"filename\",\n",
        "    \"file\",\n",
        "]\n",
        "\n",
        "\n",
        "# ----------------- CSV UTILS -----------------\n",
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]) -> Tuple[str, List[str]]:\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # image column (prefer Corresponding CFP)\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # prefer explicit VF1..VF59\n",
        "    vf_pref = [f\"VF{i}\" for i in range(1, 60)]\n",
        "    if all(c in cols for c in vf_pref):\n",
        "        return image_col, vf_pref\n",
        "\n",
        "    # fallback: detect numeric columns (exclude clinical & image & VF0/VF60)\n",
        "    excluded = set([image_col] + CLIN_NUM_COLS + CLIN_CAT_COLS + [\"VF0\", \"VF60\"])\n",
        "    cand = []\n",
        "    for c in cols:\n",
        "        if c in excluded:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            cand.append(c)\n",
        "    if len(cand) < NUM_POINTS:\n",
        "        raise ValueError(f\"Not enough numeric VF columns; found {len(cand)}, need {NUM_POINTS}.\")\n",
        "\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    cand = sorted(cand, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, cand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:40.269978Z",
          "iopub.status.busy": "2025-11-10T04:06:40.269252Z",
          "iopub.status.idle": "2025-11-10T04:06:40.280558Z",
          "shell.execute_reply": "2025-11-10T04:06:40.279748Z",
          "shell.execute_reply.started": "2025-11-10T04:06:40.269944Z"
        },
        "id": "Ri-X0iKI7ojP"
      },
      "outputs": [],
      "source": [
        "# ----------------- OD/OC POLYGONS → CDR & MASKS -----------------\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _read_labelme_polys(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        lab = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = sh.get(\"points\", [])\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if lab in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        if lab in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "\n",
        "    # keep polygon with max vertical height\n",
        "    def vheight(poly):\n",
        "        ys = [p[1] for p in poly]\n",
        "        return (max(ys) - min(ys)) if ys else 0.0\n",
        "\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=vheight)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=vheight)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        p = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def compute_cdr_from_json(img_name: str):\n",
        "    \"\"\"CDR = vertical cup height / vertical disc height (from polygons).\"\"\"\n",
        "    jpath = _guess_json(img_name)\n",
        "    if not jpath:\n",
        "        return None\n",
        "    try:\n",
        "        od_polys, oc_polys = _read_labelme_polys(jpath)\n",
        "        if not od_polys or not oc_polys:\n",
        "            return None\n",
        "\n",
        "        def vheight(poly):\n",
        "            ys = [float(y) for _, y in poly]\n",
        "            return max(ys) - min(ys) if ys else 0.0\n",
        "\n",
        "        h_od = vheight(od_polys[0])\n",
        "        h_oc = vheight(oc_polys[0])\n",
        "        if h_od <= 0:\n",
        "            return None\n",
        "        return float(h_oc / h_od)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] CDR parse failed for {img_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def build_masks_from_labelme(img_pil: Image.Image, img_name: str, out_size: int):\n",
        "    \"\"\"Binary OD/OC masks (L mode), resized to out_size.\"\"\"\n",
        "    W, H = img_pil.size\n",
        "    od_mask = Image.new(\"L\", (W, H), 0)\n",
        "    oc_mask = Image.new(\"L\", (W, H), 0)\n",
        "    jpath = _guess_json(img_name)\n",
        "    if jpath:\n",
        "        try:\n",
        "            od_polys, oc_polys = _read_labelme_polys(jpath)\n",
        "            d_od = ImageDraw.Draw(od_mask)\n",
        "            d_oc = ImageDraw.Draw(oc_mask)\n",
        "            for poly in od_polys:\n",
        "                d_od.polygon(poly, outline=1, fill=1)\n",
        "            for poly in oc_polys:\n",
        "                d_oc.polygon(poly, outline=1, fill=1)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] mask parse {jpath}: {e}\")\n",
        "    od_mask = od_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    oc_mask = oc_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    return od_mask, oc_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:43.829520Z",
          "iopub.status.busy": "2025-11-10T04:06:43.828915Z",
          "iopub.status.idle": "2025-11-10T04:06:43.833579Z",
          "shell.execute_reply": "2025-11-10T04:06:43.832931Z",
          "shell.execute_reply.started": "2025-11-10T04:06:43.829498Z"
        },
        "id": "ADiMeunz7ojP"
      },
      "outputs": [],
      "source": [
        "# ----------------- AUGMENT ROWS WITH CDR  -----------------\n",
        "def augment_rows_with_cdr_psd(rows, image_col, vf_cols):\n",
        "    augmented = []\n",
        "    miss_cdr = miss_psd = 0\n",
        "    for r in rows:\n",
        "        r2 = dict(r)\n",
        "        cdr = compute_cdr_from_json(r2[image_col])\n",
        "        if cdr is None:\n",
        "            miss_cdr += 1\n",
        "        r2[\"CDR\"] = cdr\n",
        "\n",
        "        augmented.append(r2)\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:47.264094Z",
          "iopub.status.busy": "2025-11-10T04:06:47.263774Z",
          "iopub.status.idle": "2025-11-10T04:06:47.271273Z",
          "shell.execute_reply": "2025-11-10T04:06:47.270696Z",
          "shell.execute_reply.started": "2025-11-10T04:06:47.264074Z"
        },
        "id": "nGAdQId47ojQ"
      },
      "outputs": [],
      "source": [
        "# ----------------- CLINICAL PREPROCESS -----------------\n",
        "def to_float(x):\n",
        "    x = str(x).strip()\n",
        "    if x == \"\":\n",
        "        return None\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def fit_clinical_stats(rows, clin_num_cols):\n",
        "    stats = {}\n",
        "    for c in clin_num_cols:\n",
        "        vals = [to_float(r.get(c, \"\")) for r in rows]\n",
        "        vals = [v for v in vals if v is not None]\n",
        "        mean = np.mean(vals) if vals else 0.0\n",
        "        std = np.std(vals) if vals else 1.0\n",
        "        if std == 0:\n",
        "            std = 1.0\n",
        "        stats[c] = (float(mean), float(std))\n",
        "    return stats\n",
        "\n",
        "\n",
        "def encode_gender(x):\n",
        "    s = str(x).strip().lower()\n",
        "    if s in (\"m\", \"male\", \"man\"):\n",
        "        return 1.0\n",
        "    if s in (\"f\", \"female\", \"woman\"):\n",
        "        return 0.0\n",
        "    return 0.5  # unknown/other\n",
        "\n",
        "\n",
        "def build_clinical_vector(r, stats):\n",
        "    vec = []\n",
        "    for c in CLIN_NUM_COLS:\n",
        "        v = to_float(r.get(c, \"\"))\n",
        "        mean, std = stats[c]\n",
        "        v = mean if v is None else v\n",
        "        v = (v - mean) / std\n",
        "        vec.append(v)\n",
        "    for c in CLIN_CAT_COLS:\n",
        "        if c == \"GENDER\":\n",
        "            vec.append(encode_gender(r.get(c, \"\")))\n",
        "        else:\n",
        "            vec.append(0.0)\n",
        "    return torch.tensor(vec, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:50.779769Z",
          "iopub.status.busy": "2025-11-10T04:06:50.779086Z",
          "iopub.status.idle": "2025-11-10T04:06:50.787036Z",
          "shell.execute_reply": "2025-11-10T04:06:50.786326Z",
          "shell.execute_reply.started": "2025-11-10T04:06:50.779743Z"
        },
        "id": "jRPoWzI57ojQ"
      },
      "outputs": [],
      "source": [
        "# ----------------- DATASET: 5-CH ROI + CLINICAL -----------------\n",
        "class ROI_ODOC_Clinical_Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows, image_col, vf_cols, clin_stats, train=True, img_root=ROI_DIR, img_size=IMG_SIZE\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.clin_stats = clin_stats\n",
        "        self.train = train\n",
        "        self.img_root = img_root\n",
        "        self.img_size = img_size\n",
        "\n",
        "        norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.rgb_tf = transforms.Compose(\n",
        "            [transforms.Resize((img_size, img_size)), transforms.ToTensor(), *aug, norm]\n",
        "        )\n",
        "        self.mask_tf = transforms.ToTensor()  # L→(1,H,W) float {0,1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.rows[idx]\n",
        "        fn = r[self.image_col]\n",
        "        path = fn if os.path.isabs(fn) else os.path.join(self.img_root, fn)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        od_img, oc_img = build_masks_from_labelme(img, fn, self.img_size)\n",
        "\n",
        "        x_rgb = self.rgb_tf(img)  # (3,H,W)\n",
        "        x_od = self.mask_tf(od_img)  # (1,H,W)\n",
        "        x_oc = self.mask_tf(oc_img)  # (1,H,W)\n",
        "        x5 = torch.cat([x_rgb, x_od, x_oc], dim=0)  # (5,H,W)\n",
        "\n",
        "        x_clin = build_clinical_vector(r, self.clin_stats)  # (clin_dim,)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)  # (59,)\n",
        "\n",
        "        return x5, x_clin, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:54.064124Z",
          "iopub.status.busy": "2025-11-10T04:06:54.063332Z",
          "iopub.status.idle": "2025-11-10T04:06:54.071929Z",
          "shell.execute_reply": "2025-11-10T04:06:54.071421Z",
          "shell.execute_reply.started": "2025-11-10T04:06:54.064093Z"
        },
        "id": "qSZLOrT47ojQ"
      },
      "outputs": [],
      "source": [
        "# ----------------- MODEL: 5-CH RESNET50 + CLINICAL MLP (FUSION) -----------------\n",
        "class ResNet50_5ch_Clinical(nn.Module):\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        base = models.resnet50(\n",
        "            weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        )\n",
        "        # adapt conv1: 3→5 channels by copying weights & using mean for extra channels\n",
        "        old = base.conv1\n",
        "        new = nn.Conv2d(\n",
        "            5,\n",
        "            old.out_channels,\n",
        "            kernel_size=old.kernel_size,\n",
        "            stride=old.stride,\n",
        "            padding=old.padding,\n",
        "            bias=(old.bias is not None),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            new.weight[:, :3, :, :] = old.weight\n",
        "            mean_w = old.weight.mean(dim=1, keepdim=True)\n",
        "            new.weight[:, 3:5, :, :] = mean_w.repeat(1, 2, 1, 1)\n",
        "            if old.bias is not None:\n",
        "                new.bias.copy_(old.bias)\n",
        "        base.conv1 = new\n",
        "\n",
        "        in_f = base.fc.in_features\n",
        "        base.fc = nn.Identity()\n",
        "        self.backbone = base\n",
        "\n",
        "        self.img_head = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25)\n",
        "        )\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.10),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(512 + 64, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x5, xclin):\n",
        "        f = self.backbone(x5)  # (B, 2048)\n",
        "        f = self.img_head(f)  # (B, 512)\n",
        "        g = self.clin_head(xclin)  # (B, 64)\n",
        "        z = torch.cat([f, g], dim=1)\n",
        "        out = self.fuse(z)  # (B, 59)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:57.415483Z",
          "iopub.status.busy": "2025-11-10T04:06:57.414915Z",
          "iopub.status.idle": "2025-11-10T04:06:57.422155Z",
          "shell.execute_reply": "2025-11-10T04:06:57.421329Z",
          "shell.execute_reply.started": "2025-11-10T04:06:57.415462Z"
        },
        "id": "KGhCQFvJ7ojQ"
      },
      "outputs": [],
      "source": [
        "# ----------------- METRICS & EPOCH LOOP (same logic as your earlier code) -----------------\n",
        "@torch.no_grad()\n",
        "def mae(pred, true):  # pointwise MAE\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):  # MS per-sample then MAE\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "    for x5, xclin, y in loader:\n",
        "        x5, xclin, y = x5.to(DEVICE), xclin.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "        pred = model(x5, xclin)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x5.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T04:08:38.414507Z",
          "iopub.status.busy": "2025-11-10T04:08:38.414201Z",
          "iopub.status.idle": "2025-11-10T04:17:55.812102Z",
          "shell.execute_reply": "2025-11-10T04:17:55.810937Z",
          "shell.execute_reply.started": "2025-11-10T04:08:38.414480Z"
        },
        "id": "OPb8du7U7ojR",
        "lines_to_next_cell": 2,
        "outputId": "a2ee0dce-0f14-47c4-8e75-8132a527df2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] 59 VF cols detected: ['VF1', 'VF2', 'VF3', 'VF4', 'VF5'] ... ['VF55', 'VF56', 'VF57', 'VF58', 'VF59']\n",
            "Epoch 01 | train_loss=469.1644 train_pMAE=20.039 train_MS=19.857 || val_loss=209.7161 val_pMAE=12.561 val_MS=11.563\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=12.561)\n",
            "Epoch 02 | train_loss=176.0501 train_pMAE=11.028 train_MS=9.015 || val_loss=87.4085 val_pMAE=7.957 val_MS=6.214\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=7.957)\n",
            "Epoch 03 | train_loss=61.6177 train_pMAE=6.307 train_MS=3.914 || val_loss=44.6840 val_pMAE=5.011 val_MS=3.401\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=5.011)\n",
            "Epoch 04 | train_loss=49.8062 train_pMAE=5.612 train_MS=2.981 || val_loss=42.8643 val_pMAE=5.130 val_MS=3.550\n",
            "Epoch 05 | train_loss=45.5235 train_pMAE=5.333 train_MS=2.656 || val_loss=42.4870 val_pMAE=5.066 val_MS=3.397\n",
            "Epoch 06 | train_loss=41.8714 train_pMAE=5.119 train_MS=2.371 || val_loss=39.9114 val_pMAE=4.763 val_MS=2.970\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.763)\n",
            "Epoch 07 | train_loss=40.9529 train_pMAE=5.024 train_MS=2.297 || val_loss=41.8800 val_pMAE=4.582 val_MS=3.039\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.582)\n",
            "Epoch 08 | train_loss=40.3893 train_pMAE=5.001 train_MS=2.278 || val_loss=37.0138 val_pMAE=4.498 val_MS=2.598\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.498)\n",
            "Epoch 09 | train_loss=39.0425 train_pMAE=4.885 train_MS=2.252 || val_loss=36.1235 val_pMAE=4.497 val_MS=2.633\n",
            "Epoch 10 | train_loss=37.6274 train_pMAE=4.819 train_MS=2.118 || val_loss=40.1653 val_pMAE=5.047 val_MS=3.223\n",
            "Epoch 11 | train_loss=36.1267 train_pMAE=4.687 train_MS=1.917 || val_loss=38.6326 val_pMAE=4.801 val_MS=3.048\n",
            "Epoch 12 | train_loss=36.2757 train_pMAE=4.680 train_MS=2.053 || val_loss=40.9138 val_pMAE=5.078 val_MS=3.383\n",
            "Epoch 13 | train_loss=34.9499 train_pMAE=4.588 train_MS=1.976 || val_loss=35.9886 val_pMAE=4.207 val_MS=2.469\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.207)\n",
            "Epoch 14 | train_loss=35.0209 train_pMAE=4.567 train_MS=1.981 || val_loss=35.9708 val_pMAE=4.405 val_MS=2.490\n",
            "Epoch 15 | train_loss=34.1652 train_pMAE=4.518 train_MS=1.840 || val_loss=34.7076 val_pMAE=4.246 val_MS=2.299\n",
            "Epoch 16 | train_loss=34.3646 train_pMAE=4.543 train_MS=1.930 || val_loss=38.3541 val_pMAE=4.622 val_MS=2.893\n",
            "Epoch 17 | train_loss=33.8114 train_pMAE=4.482 train_MS=1.904 || val_loss=39.8553 val_pMAE=4.943 val_MS=3.240\n",
            "Epoch 18 | train_loss=32.0755 train_pMAE=4.373 train_MS=1.663 || val_loss=36.5285 val_pMAE=4.383 val_MS=2.525\n",
            "Epoch 19 | train_loss=31.2242 train_pMAE=4.295 train_MS=1.544 || val_loss=36.0923 val_pMAE=4.386 val_MS=2.516\n",
            "Epoch 20 | train_loss=31.5069 train_pMAE=4.322 train_MS=1.601 || val_loss=36.3113 val_pMAE=4.418 val_MS=2.538\n",
            "Epoch 21 | train_loss=31.6650 train_pMAE=4.323 train_MS=1.627 || val_loss=36.0479 val_pMAE=4.382 val_MS=2.505\n",
            "Epoch 22 | train_loss=30.8424 train_pMAE=4.262 train_MS=1.503 || val_loss=36.2525 val_pMAE=4.355 val_MS=2.490\n",
            "Epoch 23 | train_loss=30.3674 train_pMAE=4.210 train_MS=1.419 || val_loss=35.8281 val_pMAE=4.339 val_MS=2.452\n",
            "Epoch 24 | train_loss=30.7257 train_pMAE=4.257 train_MS=1.520 || val_loss=35.0518 val_pMAE=4.314 val_MS=2.373\n",
            "Early stopping at epoch 24 (best val pMAE=4.207)\n"
          ]
        }
      ],
      "source": [
        "# ----------------- TRAIN -----------------\n",
        "def train_full_fusion(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01):\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] {len(vf_cols)} VF cols detected: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # augment with CDR BEFORE splitting\n",
        "    rows = augment_rows_with_cdr_psd(rows, image_col, vf_cols)\n",
        "\n",
        "    import random\n",
        "\n",
        "    random.shuffle(rows)\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    # fit clinical stats on train (handles None via mean imputation)\n",
        "    clin_stats = fit_clinical_stats(train_rows, CLIN_NUM_COLS)\n",
        "    clin_dim = len(CLIN_NUM_COLS) + len(CLIN_CAT_COLS)\n",
        "\n",
        "    # datasets / loaders\n",
        "    train_ds = ROI_ODOC_Clinical_Dataset(train_rows, image_col, vf_cols, clin_stats, train=True)\n",
        "    val_ds = ROI_ODOC_Clinical_Dataset(val_rows, image_col, vf_cols, clin_stats, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # model / optimizer\n",
        "    model = ResNet50_5ch_Clinical(clin_dim=clin_dim, out_dim=NUM_POINTS, pretrained=True).to(\n",
        "        DEVICE\n",
        "    )\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    best = float(\"inf\")\n",
        "    no_improve = 0\n",
        "    ckpt = os.path.join(CHECK_DIR, \"best_full_fusion_ROI_ODOC_CLIN1.pth\")\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f} train_pMAE={tr['pointwise_mae']:.3f} train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f} val_pMAE={va['pointwise_mae']:.3f} val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # early stopping on val pointwise MAE\n",
        "        if va[\"pointwise_mae\"] < best - MIN_DELTA:\n",
        "            best = va[\"pointwise_mae\"]\n",
        "            no_improve = 0\n",
        "            torch.save(\n",
        "                {\"epoch\": epoch, \"model\": model.state_dict(), \"val_pointwise_mae\": best}, ckpt\n",
        "            )\n",
        "            print(f\"  -> saved new best to {ckpt} (val pMAE={best:.3f})\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve > PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch} (best val pMAE={best:.3f})\")\n",
        "                break\n",
        "\n",
        "    # expose objects / paths like before\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt, clin_dim\n",
        "\n",
        "\n",
        "# run training and expose globals\n",
        "model_full, train_dl_full, val_dl_full, image_col_full, vf_cols_full, CKPT_FULL, CLIN_DIM = (\n",
        "    train_full_fusion(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T04:18:02.710520Z",
          "iopub.status.busy": "2025-11-10T04:18:02.709945Z",
          "iopub.status.idle": "2025-11-10T04:18:04.452619Z",
          "shell.execute_reply": "2025-11-10T04:18:04.451838Z",
          "shell.execute_reply.started": "2025-11-10T04:18:02.710498Z"
        },
        "id": "ThvoY2147ojR",
        "outputId": "12b6a774-ca4d-4388-d42f-3a2b864c4e3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n",
            "\n",
            "== FULL FUSION (ROI + OD/OC + Clinical ): POINTWISE ==\n",
            "RMSE: 5.9991 | MAE: 4.2068 | R²: 0.5686\n",
            "== FULL FUSION: POINTWISE-MEAN / MS ==\n",
            "RMSE: 3.3549 | MAE: 2.4694 | R²: 0.6860\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -------EVALUATE BEST + PRINT METRICS -----------------\n",
        "# reload best\n",
        "best_full = ResNet50_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(\n",
        "    DEVICE\n",
        ")\n",
        "state = torch.load(CKPT_FULL, map_location=DEVICE)\n",
        "best_full.load_state_dict(state[\"model\"])\n",
        "best_full.eval()\n",
        "\n",
        "# collect preds\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x5, xclin, y in val_dl_full:\n",
        "        x5, xclin, y = x5.to(DEVICE), xclin.to(DEVICE), y.to(DEVICE)\n",
        "        p = best_full(x5, xclin)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)\n",
        "\n",
        "\n",
        "# paper-style metrics\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_value(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "print(\"\\n== FULL FUSION (ROI + OD/OC + Clinical ): POINTWISE ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(pw_true, pw_pred):.4f} | MAE: {mae_value(pw_true, pw_pred):.4f} | R²: {r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "\n",
        "t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "print(\"== FULL FUSION: POINTWISE-MEAN / MS ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | MAE: {mae_value(t_mean, p_mean):.4f} | R²: {r2(t_mean, p_mean):.4f}\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EjMW3087ojR"
      },
      "source": [
        "# 6. ROI + OD/OC + Clinical SWIN-T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:20:07.695024Z",
          "iopub.status.busy": "2025-11-10T04:20:07.694736Z",
          "iopub.status.idle": "2025-11-10T04:20:07.713069Z",
          "shell.execute_reply": "2025-11-10T04:20:07.712345Z",
          "shell.execute_reply.started": "2025-11-10T04:20:07.695004Z"
        },
        "id": "RiEDOoeF7ojS"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# SWIN-T for FULL FUSION (ROI + OD/OC + Clinical) → VF (59)\n",
        "# Reuses train_dl_full, val_dl_full, and CLIN_DIM from your existing setup\n",
        "# ==========================\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "# ---- metrics used inside the loop\n",
        "@torch.no_grad()\n",
        "def _mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def _run_epoch_ff(model, loader, device, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "\n",
        "    for batch in loader:\n",
        "        # x5: (B,5,H,W) ; x_clin: (B, CLIN_DIM) ; y: (B,59)\n",
        "        x5, x_clin, y = batch\n",
        "        x5 = x5.to(device, non_blocking=True)\n",
        "        x_clin = x_clin.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "        pred = model(x5, x_clin)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x5.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += _mae(pred, y) * bs\n",
        "        msmae_sum += _ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss_sum / max(1, n),\n",
        "        \"pointwise_mae\": pmae_sum / max(1, n),\n",
        "        \"ms_mae\": msmae_sum / max(1, n),\n",
        "    }\n",
        "\n",
        "\n",
        "# ---- Model: Swin-T (RGB) + small CNN for OD/OC masks + clinical MLP → fused regressor\n",
        "class SwinT_5ch_Clinical(nn.Module):\n",
        "    \"\"\"\n",
        "    Uses Swin-T on RGB (x5[:, :3]),\n",
        "    Encodes OD/OC masks (x5[:, 3:5]) with a lightweight CNN,\n",
        "    Encodes clinical features with an MLP,\n",
        "    Concats [swin_feat, mask_feat, clin_feat] → MLP → 59-D VF regression.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True, dropout=0.25):\n",
        "        super().__init__()\n",
        "        # Swin-T backbone (ImageNet weights) → (B, feat_dim)\n",
        "        self.backbone = models.swin_t(\n",
        "            weights=models.Swin_T_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        feat_dim = self.backbone.head.in_features\n",
        "        self.backbone.head = nn.Identity()  # keep pooled features\n",
        "\n",
        "        # OD/OC mask encoder (2xHxW → vector)\n",
        "        self.mask_enc = nn.Sequential(\n",
        "            nn.Conv2d(2, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),  # (B,128)\n",
        "        )\n",
        "        mask_dim = 128\n",
        "\n",
        "        # clinical encoder\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        clin_out = 64\n",
        "\n",
        "        # fusion & regressor\n",
        "        fused_in = feat_dim + mask_dim + clin_out\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(fused_in, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x5, xclin):\n",
        "        x_rgb = x5[:, :3, :, :]\n",
        "        x_msk = x5[:, 3:, :, :]\n",
        "        f_rgb = self.backbone(x_rgb)\n",
        "        f_msk = self.mask_enc(x_msk)\n",
        "        f_cln = self.clin_head(xclin)\n",
        "        z = torch.cat([f_rgb, f_msk, f_cln], dim=1)\n",
        "        return self.regressor(z)\n",
        "\n",
        "\n",
        "# ---- Early Stopping helper\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = patience\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best = float(\"inf\")\n",
        "        self.bad_epochs = 0\n",
        "\n",
        "    def step(self, current, model, epoch_meta=None):\n",
        "        if current < self.best - self.min_delta:\n",
        "            self.best = current\n",
        "            self.bad_epochs = 0\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            return False  # do not stop\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "            return self.bad_epochs > self.patience  # stop if exceeded\n",
        "\n",
        "\n",
        "# ---- Train\n",
        "def train_full_fusion_swinT(\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    clin_dim,\n",
        "    out_dim=59,\n",
        "    epochs=80,\n",
        "    lr=1e-4,\n",
        "    wd=1e-4,\n",
        "    device=\"cuda\",\n",
        "    pretrained=True,\n",
        "    patience=10,\n",
        "    min_delta=0.01,\n",
        "    ckpt_dir=\"./checkpoints\",\n",
        "):\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    ckpt = os.path.join(ckpt_dir, \"best_full_fusion_swint.pth\")\n",
        "\n",
        "    model = SwinT_5ch_Clinical(clin_dim=clin_dim, out_dim=out_dim, pretrained=pretrained).to(\n",
        "        device\n",
        "    )\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    # reduce LR when val pMAE plateaus\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    stopper = EarlyStopper(patience=patience, min_delta=min_delta, ckpt_path=ckpt)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        tr = _run_epoch_ff(model, train_loader, device, opt=opt)\n",
        "        va = _run_epoch_ff(model, val_loader, device, opt=None)\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f} train_pMAE={tr['pointwise_mae']:.3f} train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f} val_pMAE={va['pointwise_mae']:.3f} val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        should_stop = stopper.step(va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch})\n",
        "        if should_stop:\n",
        "            print(f\"Early stopping at epoch {epoch} (best val pMAE={stopper.best:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best\n",
        "    state = torch.load(ckpt, map_location=device)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T04:22:55.770223Z",
          "iopub.status.busy": "2025-11-10T04:22:55.769445Z",
          "iopub.status.idle": "2025-11-10T04:35:02.461704Z",
          "shell.execute_reply": "2025-11-10T04:35:02.461076Z",
          "shell.execute_reply.started": "2025-11-10T04:22:55.770197Z"
        },
        "id": "V06wEXLV7ojS",
        "lines_to_next_cell": 2,
        "outputId": "51d29779-1bde-4399-ecaa-2f626c8b4df9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 108M/108M [00:00<00:00, 170MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | train_loss=386.1182 train_pMAE=18.012 train_MS=17.687 || val_loss=222.4738 val_pMAE=13.476 val_MS=12.304\n",
            "Epoch 02 | train_loss=113.4438 train_pMAE=8.883 train_MS=7.452 || val_loss=66.0766 val_pMAE=5.796 val_MS=4.560\n",
            "Epoch 03 | train_loss=61.6774 train_pMAE=5.900 train_MS=4.427 || val_loss=61.4749 val_pMAE=6.219 val_MS=5.061\n",
            "Epoch 04 | train_loss=59.9508 train_pMAE=5.846 train_MS=4.434 || val_loss=61.7425 val_pMAE=5.571 val_MS=4.560\n",
            "Epoch 05 | train_loss=59.0507 train_pMAE=5.731 train_MS=4.305 || val_loss=60.6739 val_pMAE=5.643 val_MS=4.605\n",
            "Epoch 06 | train_loss=58.5785 train_pMAE=5.668 train_MS=4.293 || val_loss=59.8473 val_pMAE=5.976 val_MS=4.843\n",
            "Epoch 07 | train_loss=57.7981 train_pMAE=5.721 train_MS=4.319 || val_loss=59.0121 val_pMAE=5.629 val_MS=4.551\n",
            "Epoch 08 | train_loss=58.4501 train_pMAE=5.693 train_MS=4.276 || val_loss=60.3202 val_pMAE=5.465 val_MS=4.425\n",
            "Epoch 09 | train_loss=55.9027 train_pMAE=5.579 train_MS=4.136 || val_loss=56.8516 val_pMAE=5.652 val_MS=4.517\n",
            "Epoch 10 | train_loss=53.8313 train_pMAE=5.522 train_MS=3.943 || val_loss=61.7398 val_pMAE=5.201 val_MS=4.050\n",
            "Epoch 11 | train_loss=51.9078 train_pMAE=5.370 train_MS=3.850 || val_loss=44.2420 val_pMAE=4.970 val_MS=3.502\n",
            "Epoch 12 | train_loss=44.5384 train_pMAE=5.039 train_MS=3.237 || val_loss=61.4955 val_pMAE=5.146 val_MS=4.040\n",
            "Epoch 13 | train_loss=44.6086 train_pMAE=4.986 train_MS=3.251 || val_loss=37.5184 val_pMAE=4.391 val_MS=2.516\n",
            "Epoch 14 | train_loss=37.3293 train_pMAE=4.575 train_MS=2.631 || val_loss=39.4001 val_pMAE=4.845 val_MS=3.064\n",
            "Epoch 15 | train_loss=40.2251 train_pMAE=4.730 train_MS=2.962 || val_loss=48.2994 val_pMAE=4.858 val_MS=3.537\n",
            "Epoch 16 | train_loss=36.5648 train_pMAE=4.614 train_MS=2.713 || val_loss=37.2676 val_pMAE=4.303 val_MS=2.688\n",
            "Epoch 17 | train_loss=33.7268 train_pMAE=4.400 train_MS=2.417 || val_loss=43.2625 val_pMAE=4.498 val_MS=3.024\n",
            "Epoch 18 | train_loss=34.5975 train_pMAE=4.374 train_MS=2.538 || val_loss=40.1070 val_pMAE=4.330 val_MS=2.765\n",
            "Epoch 19 | train_loss=31.2320 train_pMAE=4.189 train_MS=2.163 || val_loss=37.3927 val_pMAE=4.360 val_MS=2.602\n",
            "Epoch 20 | train_loss=31.3942 train_pMAE=4.187 train_MS=2.189 || val_loss=35.3125 val_pMAE=4.227 val_MS=2.517\n",
            "Epoch 21 | train_loss=30.0447 train_pMAE=4.100 train_MS=1.995 || val_loss=33.8200 val_pMAE=4.227 val_MS=2.370\n",
            "Epoch 22 | train_loss=28.7202 train_pMAE=4.033 train_MS=1.877 || val_loss=36.0683 val_pMAE=4.220 val_MS=2.536\n",
            "Epoch 23 | train_loss=27.9874 train_pMAE=3.970 train_MS=1.786 || val_loss=35.0002 val_pMAE=4.192 val_MS=2.438\n",
            "Epoch 24 | train_loss=28.1103 train_pMAE=3.968 train_MS=1.801 || val_loss=35.0010 val_pMAE=4.142 val_MS=2.414\n",
            "Epoch 25 | train_loss=27.1186 train_pMAE=3.907 train_MS=1.646 || val_loss=33.7937 val_pMAE=4.174 val_MS=2.344\n",
            "Epoch 26 | train_loss=26.7408 train_pMAE=3.883 train_MS=1.655 || val_loss=33.6512 val_pMAE=4.135 val_MS=2.310\n",
            "Epoch 27 | train_loss=26.6278 train_pMAE=3.876 train_MS=1.630 || val_loss=33.7680 val_pMAE=4.283 val_MS=2.423\n",
            "Epoch 28 | train_loss=26.0273 train_pMAE=3.827 train_MS=1.561 || val_loss=34.7955 val_pMAE=4.105 val_MS=2.414\n",
            "Epoch 29 | train_loss=25.4716 train_pMAE=3.774 train_MS=1.419 || val_loss=33.5685 val_pMAE=4.119 val_MS=2.281\n",
            "Epoch 30 | train_loss=24.7712 train_pMAE=3.704 train_MS=1.362 || val_loss=33.2885 val_pMAE=4.145 val_MS=2.328\n",
            "Epoch 31 | train_loss=25.6402 train_pMAE=3.780 train_MS=1.491 || val_loss=35.9135 val_pMAE=4.161 val_MS=2.561\n",
            "Epoch 32 | train_loss=25.3495 train_pMAE=3.754 train_MS=1.483 || val_loss=33.3441 val_pMAE=4.157 val_MS=2.361\n",
            "Epoch 33 | train_loss=24.1785 train_pMAE=3.650 train_MS=1.261 || val_loss=34.2588 val_pMAE=4.175 val_MS=2.447\n",
            "Epoch 34 | train_loss=24.1927 train_pMAE=3.659 train_MS=1.270 || val_loss=33.4643 val_pMAE=4.110 val_MS=2.323\n",
            "Epoch 35 | train_loss=23.8270 train_pMAE=3.619 train_MS=1.211 || val_loss=33.6440 val_pMAE=4.131 val_MS=2.360\n",
            "Epoch 36 | train_loss=23.8447 train_pMAE=3.645 train_MS=1.174 || val_loss=33.3032 val_pMAE=4.159 val_MS=2.340\n",
            "Epoch 37 | train_loss=23.4129 train_pMAE=3.583 train_MS=1.077 || val_loss=33.8248 val_pMAE=4.132 val_MS=2.364\n",
            "Epoch 38 | train_loss=23.3630 train_pMAE=3.591 train_MS=1.115 || val_loss=34.0517 val_pMAE=4.118 val_MS=2.365\n",
            "Epoch 39 | train_loss=23.1121 train_pMAE=3.567 train_MS=1.074 || val_loss=33.7570 val_pMAE=4.106 val_MS=2.318\n",
            "Early stopping at epoch 39 (best val pMAE=4.105)\n",
            "\n",
            "[OK] Training finished. Best checkpoint: ./checkpoints/best_full_fusion_swint.pth\n",
            "\n",
            "== SWIN-T FULL FUSION: POINTWISE ==\n",
            "RMSE: 5.8988 | MAE: 4.1052 | R²: 0.5829\n",
            "== SWIN-T FULL FUSION: MEAN (MS) ==\n",
            "RMSE: 3.4719 | MAE: 2.4142 | R²: 0.6637\n"
          ]
        }
      ],
      "source": [
        "# ===== RUN: SWIN-T FULL-FUSION TRAIN + EVAL =====\n",
        "from math import sqrt\n",
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "# ---- config (tweak if you like)\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "PATIENCE = 10\n",
        "MIN_DELTA = 0.01\n",
        "CHECK_DIR = \"./checkpoints\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "# ---- quick checks\n",
        "assert \"train_dl_full\" in globals() and \"val_dl_full\" in globals(), (\n",
        "    \"Missing loaders. Make sure you created train_dl_full and val_dl_full.\"\n",
        ")\n",
        "assert \"CLIN_DIM\" in globals(), \"Missing CLIN_DIM.\"\n",
        "OUT_DIM = 59 if \"NUM_POINTS\" not in globals() else int(NUM_POINTS)\n",
        "\n",
        "# ---- train\n",
        "swinT_model, SWINT_CKPT = train_full_fusion_swinT(\n",
        "    train_loader=train_dl_full,\n",
        "    val_loader=val_dl_full,\n",
        "    clin_dim=CLIN_DIM,\n",
        "    out_dim=OUT_DIM,\n",
        "    epochs=EPOCHS,\n",
        "    lr=LR,\n",
        "    wd=WD,\n",
        "    device=DEVICE,\n",
        "    patience=PATIENCE,\n",
        "    min_delta=MIN_DELTA,\n",
        "    ckpt_dir=CHECK_DIR,\n",
        "    pretrained=True,\n",
        ")\n",
        "\n",
        "print(f\"\\n[OK] Training finished. Best checkpoint: {SWINT_CKPT}\")\n",
        "\n",
        "\n",
        "# ---- evaluation helpers (pointwise + mean-of-points/“MS”)\n",
        "@torch.no_grad()\n",
        "def _rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _mae_scalar(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1.0 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# ---- load best and evaluate on val\n",
        "best_swinT = SwinT_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=OUT_DIM, pretrained=False).to(DEVICE)\n",
        "state = torch.load(SWINT_CKPT, map_location=DEVICE)\n",
        "best_swinT.load_state_dict(state[\"model\"])\n",
        "best_swinT.eval()\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x5, xclin, y in val_dl_full:\n",
        "        x5 = x5.to(DEVICE)\n",
        "        xclin = xclin.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        p = best_swinT(x5, xclin)\n",
        "        y_true.append(y.cpu())\n",
        "        y_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(y_true, dim=0)\n",
        "y_pred = torch.cat(y_pred, dim=0)\n",
        "\n",
        "# pointwise metrics\n",
        "pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "print(\"\\n== SWIN-T FULL FUSION: POINTWISE ==\")\n",
        "print(\n",
        "    f\"RMSE: {_rmse(pw_true, pw_pred):.4f} | MAE: {_mae_scalar(pw_true, pw_pred):.4f} | R²: {_r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "\n",
        "# mean-of-points (“MS”) metrics\n",
        "t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "print(\"== SWIN-T FULL FUSION: MEAN (MS) ==\")\n",
        "print(\n",
        "    f\"RMSE: {_rmse(t_mean, p_mean):.4f} | MAE: {_mae_scalar(t_mean, p_mean):.4f} | R²: {_r2(t_mean, p_mean):.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T11:59:52.375643Z",
          "iopub.status.busy": "2025-11-08T11:59:52.375380Z",
          "iopub.status.idle": "2025-11-08T11:59:52.379449Z",
          "shell.execute_reply": "2025-11-08T11:59:52.378654Z",
          "shell.execute_reply.started": "2025-11-08T11:59:52.375625Z"
        },
        "id": "z0PAdMkE7ojS"
      },
      "source": [
        "# 7. Weighted Averaging Ensemble Technique (ResNet50 + SWIN-T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T04:36:47.133505Z",
          "iopub.status.busy": "2025-11-10T04:36:47.132776Z",
          "iopub.status.idle": "2025-11-10T04:36:50.037569Z",
          "shell.execute_reply": "2025-11-10T04:36:50.036982Z",
          "shell.execute_reply.started": "2025-11-10T04:36:47.133480Z"
        },
        "id": "w03dyHHp7ojS",
        "lines_to_next_cell": 2,
        "outputId": "d9bc11be-12d1-400c-9eae-3048cf837af5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== INDIVIDUAL MODELS (pointwise) ==\n",
            "ResNet  → RMSE: 5.9991 | MAE: 4.2068 | R²: 0.5686\n",
            "Swin-B  → RMSE: 5.8988 | MAE: 4.1052 | R²: 0.5829\n",
            "\n",
            "== ENSEMBLE (pointwise) ==\n",
            "Avg (α=0.50) → RMSE: 5.7192 | MAE: 3.9813 | R²: 0.6079\n",
            "\n",
            "== INDIVIDUAL MODELS (MS) ==\n",
            "ResNet  → RMSE: 3.3549 | MAE: 2.4694 | R²: 0.6860\n",
            "Swin-B  → RMSE: 3.4719 | MAE: 2.4142 | R²: 0.6637\n",
            "\n",
            "== ENSEMBLE (MS) ==\n",
            "Avg (α=0.50) → RMSE: 3.1138 | MAE: 2.1537 | R²: 0.7295\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# Simple Ensemble: ResNet50_5ch_Clinical + Swint_5ch_Clinical\n",
        "# Uses the SAME loaders (val_dl_full / test_dl_full) as your full-fusion setup\n",
        "# ==========================\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ---- your checkpoint paths\n",
        "RESNET_CKPT_PATH = CHECK_DIR + \"/best_full_fusion_ROI_ODOC_CLIN1.pth\"\n",
        "SWIN_CKPT_PATH = CHECK_DIR + \"/best_full_fusion_swint.pth\"\n",
        "\n",
        "\n",
        "# ---- small metric helpers (names won't clash with your existing ones)\n",
        "@torch.no_grad()\n",
        "def _rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _mae(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "def _load_model_states():\n",
        "    # Build the exact architectures (no pretrain needed when loading checkpoints)\n",
        "    resnet = ResNet50_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(\n",
        "        DEVICE\n",
        "    )\n",
        "    swin = SwinT_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "\n",
        "    r_state = torch.load(RESNET_CKPT_PATH, map_location=DEVICE)\n",
        "    s_state = torch.load(SWIN_CKPT_PATH, map_location=DEVICE)\n",
        "\n",
        "    resnet.load_state_dict(r_state[\"model\"])\n",
        "    resnet.eval()\n",
        "    swin.load_state_dict(s_state[\"model\"])\n",
        "    swin.eval()\n",
        "    return resnet, swin\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ensemble_eval(loader, alpha=0.5):\n",
        "    \"\"\"\n",
        "    alpha: weight for SWIN (0..1). 0.5 = simple average\n",
        "    pred = (1-alpha)*resnet + alpha*swin\n",
        "    \"\"\"\n",
        "    assert 0.0 <= alpha <= 1.0\n",
        "    resnet, swin = _load_model_states()\n",
        "\n",
        "    y_true_chunks, y_pred_res_chunks, y_pred_swin_chunks, y_pred_ens_chunks = [], [], [], []\n",
        "\n",
        "    for x5, xclin, y in loader:\n",
        "        x5 = x5.to(DEVICE, non_blocking=True)\n",
        "        xcli = xclin.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        p_r = resnet(x5, xcli)  # (B, 59)\n",
        "        p_s = swin(x5, xcli)  # (B, 59)\n",
        "        p_e = (1.0 - alpha) * p_r + alpha * p_s\n",
        "\n",
        "        y_true_chunks.append(y.cpu())\n",
        "        y_pred_res_chunks.append(p_r.cpu())\n",
        "        y_pred_swin_chunks.append(p_s.cpu())\n",
        "        y_pred_ens_chunks.append(p_e.cpu())\n",
        "\n",
        "    y_true = torch.cat(y_true_chunks, dim=0)\n",
        "    p_res = torch.cat(y_pred_res_chunks, dim=0)\n",
        "    p_swin = torch.cat(y_pred_swin_chunks, dim=0)\n",
        "    p_ens = torch.cat(y_pred_ens_chunks, dim=0)\n",
        "\n",
        "    # pointwise metrics (flatten all 59 points)\n",
        "    pw_true, pw_res, pw_swin, pw_ens = (\n",
        "        y_true.reshape(-1),\n",
        "        p_res.reshape(-1),\n",
        "        p_swin.reshape(-1),\n",
        "        p_ens.reshape(-1),\n",
        "    )\n",
        "\n",
        "    print(\"\\n== INDIVIDUAL MODELS (pointwise) ==\")\n",
        "    print(\n",
        "        f\"ResNet  → RMSE: {_rmse(pw_true, pw_res):.4f} | MAE: {_mae(pw_true, pw_res):.4f} | R²: {_r2(pw_true, pw_res):.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Swin-B  → RMSE: {_rmse(pw_true, pw_swin):.4f} | MAE: {_mae(pw_true, pw_swin):.4f} | R²: {_r2(pw_true, pw_swin):.4f}\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n== ENSEMBLE (pointwise) ==\")\n",
        "    print(\n",
        "        f\"Avg (α={alpha:.2f}) → RMSE: {_rmse(pw_true, pw_ens):.4f} | MAE: {_mae(pw_true, pw_ens):.4f} | R²: {_r2(pw_true, pw_ens):.4f}\"\n",
        "    )\n",
        "\n",
        "    # MS metrics = mean of 59 points per sample\n",
        "    t_mean, r_mean, s_mean, e_mean = (\n",
        "        y_true.mean(dim=1),\n",
        "        p_res.mean(dim=1),\n",
        "        p_swin.mean(dim=1),\n",
        "        p_ens.mean(dim=1),\n",
        "    )\n",
        "\n",
        "    print(\"\\n== INDIVIDUAL MODELS (MS) ==\")\n",
        "    print(\n",
        "        f\"ResNet  → RMSE: {_rmse(t_mean, r_mean):.4f} | MAE: {_mae(t_mean, r_mean):.4f} | R²: {_r2(t_mean, r_mean):.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Swin-B  → RMSE: {_rmse(t_mean, s_mean):.4f} | MAE: {_mae(t_mean, s_mean):.4f} | R²: {_r2(t_mean, s_mean):.4f}\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n== ENSEMBLE (MS) ==\")\n",
        "    print(\n",
        "        f\"Avg (α={alpha:.2f}) → RMSE: {_rmse(t_mean, e_mean):.4f} | MAE: {_mae(t_mean, e_mean):.4f} | R²: {_r2(t_mean, e_mean):.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"y_true\": y_true,\n",
        "        \"pred_resnet\": p_res,\n",
        "        \"pred_swin\": p_swin,\n",
        "        \"pred_ensemble\": p_ens,\n",
        "    }\n",
        "\n",
        "\n",
        "# ===== RUN on your existing loaders =====\n",
        "# Use val set:\n",
        "assert \"val_dl_full\" in globals(), \"val_dl_full not found. Run your full-fusion data cell first.\"\n",
        "_ = ensemble_eval(val_dl_full, alpha=0.5)  # try alpha=0.3, 0.7, etc.\n",
        "\n",
        "# If you also have test_dl_full:\n",
        "# assert 'test_dl_full' in globals()\n",
        "# _ = ensemble_eval(test_dl_full, alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T04:37:07.068710Z",
          "iopub.status.busy": "2025-11-10T04:37:07.068455Z",
          "iopub.status.idle": "2025-11-10T04:37:09.933957Z",
          "shell.execute_reply": "2025-11-10T04:37:09.933312Z",
          "shell.execute_reply.started": "2025-11-10T04:37:07.068691Z"
        },
        "id": "F_dq7rpa7ojT",
        "lines_to_next_cell": 2,
        "outputId": "f016fc9e-9dda-42bb-f412-56e6f493a07c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Ensemble] best alpha=0.55 (weights: SWIN=0.55, RESNET=0.45)\n",
            "POINTWISE:\n",
            "  RMSE=5.7164 | MAE=3.9793 | R²=0.6083\n",
            "MS (mean sensitivity):\n",
            "  RMSE=3.1234 | MAE=2.1561 | R²=0.7278\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# SIMPLE ENSEMBLE: ResNet (full fusion) + Swin-B (full fusion)\n",
        "# Grid-search alpha on VAL to weight SWIN higher/lower\n",
        "# ==========================\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# --- Metrics (same style you used)\n",
        "@torch.no_grad()\n",
        "def _rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _mae(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# --- Load both models\n",
        "RESNET_CKPT = CHECK_DIR + \"/best_full_fusion_ROI_ODOC_CLIN1.pth\"\n",
        "SWIN_CKPT = CHECK_DIR + \"/best_full_fusion_swint.pth\"\n",
        "\n",
        "resnet = ResNet50_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "swin = SwinT_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "\n",
        "resnet.load_state_dict(torch.load(RESNET_CKPT, map_location=DEVICE)[\"model\"])\n",
        "swin.load_state_dict(torch.load(SWIN_CKPT, map_location=DEVICE)[\"model\"])\n",
        "resnet.eval()\n",
        "swin.eval()\n",
        "\n",
        "# --- Collect full VAL predictions once for speed\n",
        "y_true_list, pred_resnet_list, pred_swin_list = [], [], []\n",
        "with torch.no_grad():\n",
        "    for x5, xclin, y in val_dl_full:\n",
        "        x5 = x5.to(DEVICE)\n",
        "        xclin = xclin.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        pr = resnet(x5, xclin)\n",
        "        ps = swin(x5, xclin)\n",
        "        y_true_list.append(y.cpu())\n",
        "        pred_resnet_list.append(pr.cpu())\n",
        "        pred_swin_list.append(ps.cpu())\n",
        "\n",
        "y_true = torch.cat(y_true_list, dim=0)  # (N, 59)\n",
        "pred_r = torch.cat(pred_resnet_list, dim=0)  # (N, 59)\n",
        "pred_s = torch.cat(pred_swin_list, dim=0)  # (N, 59)\n",
        "\n",
        "# --- Grid-search alpha in [0,1] to minimize pointwise MAE (you can switch to MS if preferred)\n",
        "best_alpha, best_mae = None, float(\"inf\")\n",
        "for a in [i / 20 for i in range(21)]:  # 0.00, 0.05, ..., 1.00\n",
        "    ens = a * pred_s + (1 - a) * pred_r\n",
        "    mae_pw = _mae(ens.reshape(-1), y_true.reshape(-1))\n",
        "    if mae_pw < best_mae:\n",
        "        best_mae = mae_pw\n",
        "        best_alpha = a\n",
        "\n",
        "# --- Final ensemble metrics with the chosen alpha\n",
        "ens = best_alpha * pred_s + (1 - best_alpha) * pred_r\n",
        "pw_true, pw_pred = y_true.reshape(-1), ens.reshape(-1)\n",
        "print(\n",
        "    f\"\\n[Ensemble] best alpha={best_alpha:.2f} (weights: SWIN={best_alpha:.2f}, RESNET={(1 - best_alpha):.2f})\"\n",
        ")\n",
        "print(\"POINTWISE:\")\n",
        "print(\n",
        "    f\"  RMSE={_rmse(pw_true, pw_pred):.4f} | MAE={_mae(pw_true, pw_pred):.4f} | R²={_r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "t_mean, p_mean = y_true.mean(dim=1), ens.mean(dim=1)\n",
        "print(\"MS (mean sensitivity):\")\n",
        "print(\n",
        "    f\"  RMSE={_rmse(t_mean, p_mean):.4f} | MAE={_mae(t_mean, p_mean):.4f} | R²={_r2(t_mean, p_mean):.4f}\"\n",
        ")\n",
        "\n",
        "# Save alpha if you want to reuse for test-time ensembling\n",
        "BEST_ALPHA = best_alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T06:35:33.870233Z",
          "iopub.status.busy": "2025-11-10T06:35:33.869481Z",
          "iopub.status.idle": "2025-11-10T06:35:34.053693Z",
          "shell.execute_reply": "2025-11-10T06:35:34.052990Z",
          "shell.execute_reply.started": "2025-11-10T06:35:33.870203Z"
        },
        "id": "T_tbtCgC7ojT",
        "outputId": "f8d18e1d-c048-463e-95b2-1bdad1183140"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_full_fusion_ROI_ODOC_CLIN1.pth  best_resnet50_original_roi.pth\n",
            "best_full_fusion_swint.pth\t     best_resnet50_ROI_ODOC_CDR.pth\n",
            "best_resnet50_original_cfp.pth\t     best_resnet50_ROI_ODOC.pth\n"
          ]
        }
      ],
      "source": [
        "!ls $CHECK_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T06:37:19.823311Z",
          "iopub.status.busy": "2025-11-10T06:37:19.823011Z",
          "iopub.status.idle": "2025-11-10T06:37:49.186221Z",
          "shell.execute_reply": "2025-11-10T06:37:49.185475Z",
          "shell.execute_reply.started": "2025-11-10T06:37:19.823284Z"
        },
        "id": "GCknyzaY7ojT",
        "outputId": "178d98dc-8d57-43e4-fa84-a89a0c7cebe3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<a href='final_checkpoints_archive.zip' target='_blank'>final_checkpoints_archive.zip</a><br>"
            ],
            "text/plain": [
              "/content/final_checkpoints_archive.zip"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "from IPython.display import FileLink\n",
        "\n",
        "shutil.make_archive(\"final_checkpoints_archive\", \"zip\", CHECK_DIR)\n",
        "FileLink(\"final_checkpoints_archive.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "formats": "ipynb,py:percent"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 8671492,
          "sourceId": 13641781,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
