{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:47:26.641075Z",
          "iopub.status.busy": "2025-11-08T09:47:26.640756Z",
          "iopub.status.idle": "2025-11-08T09:47:26.659110Z",
          "shell.execute_reply": "2025-11-08T09:47:26.658596Z",
          "shell.execute_reply.started": "2025-11-08T09:47:26.641051Z"
        },
        "id": "5dQwmSaRFbhg"
      },
      "source": [
        "# 1. CFP Images EfficientNetB3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:12.542505Z",
          "iopub.status.busy": "2025-11-10T05:17:12.542179Z",
          "iopub.status.idle": "2025-11-10T05:17:12.550563Z",
          "shell.execute_reply": "2025-11-10T05:17:12.550006Z",
          "shell.execute_reply.started": "2025-11-10T05:17:12.542470Z"
        },
        "id": "ThXhR4j6Fbhh"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "CSV_PATH = \"./filtered_glaucoma.csv\"\n",
        "IMG_ROOT = \"./glaucoma_data/CFPs\"  # <-- CFP images folder\n",
        "CHECK_DIR = \"./checkpoints\"\n",
        "CFP_DIR = \"./glaucoma_data/ROI images\"  # <-- ROI images folder\n",
        "ROI_DIR = \"./glaucoma_data/ROI images\"  # ROI images folder\n",
        "JSON_DIR = \"./glaucoma_data/json\"  # LabelMe JSON files matching image names\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59\n",
        "PATIENCE = 10\n",
        "MIN_DELTA = 0.01\n",
        "\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zf_dmo_LFbhj"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(\"./glaucoma_data.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"./\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:15.582725Z",
          "iopub.status.busy": "2025-11-10T05:17:15.582037Z",
          "iopub.status.idle": "2025-11-10T05:17:15.592790Z",
          "shell.execute_reply": "2025-11-10T05:17:15.592080Z",
          "shell.execute_reply.started": "2025-11-10T05:17:15.582701Z"
        },
        "id": "51cFzswLFbhk"
      },
      "outputs": [],
      "source": [
        "# ---- tiny CSV reader (no pandas) ------------------------------------------------\n",
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]  # ensure equal length\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ---- detect columns --------------------------------------------------------------\n",
        "IMAGE_COLS_CANDIDATES = [\"image\", \"image_name\", \"img\", \"image_path\", \"filename\", \"file\"]\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]):\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # find image column\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # pick 59 VF columns: prefer v1..v59\n",
        "    vf_cols = [f\"v{i}\" for i in range(1, NUM_POINTS + 1)]\n",
        "    if all(c in cols for c in vf_cols):\n",
        "        return image_col, vf_cols\n",
        "\n",
        "    # fallback: numeric columns\n",
        "    candidates = []\n",
        "    for c in cols:\n",
        "        if c == image_col:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            candidates.append(c)\n",
        "\n",
        "    if len(candidates) < NUM_POINTS:\n",
        "        raise ValueError(\"Not enough numeric VF columns detected.\")\n",
        "\n",
        "    # sort by trailing number if exists\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    candidates_sorted = sorted(candidates, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, candidates_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:18.937424Z",
          "iopub.status.busy": "2025-11-10T05:17:18.936885Z",
          "iopub.status.idle": "2025-11-10T05:17:18.944011Z",
          "shell.execute_reply": "2025-11-10T05:17:18.943261Z",
          "shell.execute_reply.started": "2025-11-10T05:17:18.937404Z"
        },
        "id": "u5shLYBpFbhk"
      },
      "outputs": [],
      "source": [
        "class CFPDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows: List[Dict[str, str]], image_col: str, vf_cols: List[str], train: bool\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.train = train\n",
        "\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "\n",
        "        self.tf = transforms.Compose(\n",
        "            [transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(), *aug, normalize]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        name = r[self.image_col]\n",
        "\n",
        "        path = name\n",
        "        if not os.path.isabs(path):\n",
        "            if os.path.basename(path) == path:\n",
        "                path = os.path.join(IMG_ROOT, path)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        x = self.tf(img)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:22.158208Z",
          "iopub.status.busy": "2025-11-10T05:17:22.157482Z",
          "iopub.status.idle": "2025-11-10T05:17:22.163262Z",
          "shell.execute_reply": "2025-11-10T05:17:22.162535Z",
          "shell.execute_reply.started": "2025-11-10T05:17:22.158181Z"
        },
        "id": "m274qCUbFbhl"
      },
      "outputs": [],
      "source": [
        "class EfficientNetB3VF(nn.Module):\n",
        "    def __init__(self, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.efficientnet_b3(\n",
        "            weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        in_f = self.backbone.classifier[-1].in_features\n",
        "        self.backbone.classifier[-1] = nn.Identity()\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25), nn.Linear(512, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x)\n",
        "        return self.regressor(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:25.577607Z",
          "iopub.status.busy": "2025-11-10T05:17:25.577128Z",
          "iopub.status.idle": "2025-11-10T05:17:25.584146Z",
          "shell.execute_reply": "2025-11-10T05:17:25.583424Z",
          "shell.execute_reply.started": "2025-11-10T05:17:25.577575Z"
        },
        "id": "qarFysw3Fbhl"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = crit(pred, y)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        n += bs\n",
        "\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:29.978088Z",
          "iopub.status.busy": "2025-11-10T05:17:29.977820Z",
          "iopub.status.idle": "2025-11-10T05:28:21.822456Z",
          "shell.execute_reply": "2025-11-10T05:28:21.821528Z",
          "shell.execute_reply.started": "2025-11-10T05:17:29.978068Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcYO8trNFbhl",
        "outputId": "3c7665e5-e0bb-4067-8bf1-b4e900dad926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] using 59 VF columns: ['AGE', 'CCT', 'IOP_y', 'Interval Years', 'MD'] ... ['VF50', 'VF51', 'VF52', 'VF53', 'VF54']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47.2M/47.2M [00:00<00:00, 220MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 01 | train_loss=5429.3386  train_pMAE=28.527  train_msMAE=28.352 || val_loss=5374.4848  val_pMAE=27.257  val_msMAE=26.981\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=27.257)\n",
            "\n",
            "Epoch 02 | train_loss=5038.9699  train_pMAE=24.966  train_msMAE=24.432 || val_loss=4623.2760  val_pMAE=19.466  val_msMAE=17.707\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=19.466)\n",
            "\n",
            "Epoch 03 | train_loss=4313.7342  train_pMAE=17.110  train_msMAE=13.731 || val_loss=4187.3719  val_pMAE=14.574  val_msMAE=10.534\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=14.574)\n",
            "\n",
            "Epoch 04 | train_loss=3610.0818  train_pMAE=14.237  train_msMAE=5.805 || val_loss=3281.0313  val_pMAE=13.281  val_msMAE=4.819\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=13.281)\n",
            "\n",
            "Epoch 05 | train_loss=2885.8161  train_pMAE=12.948  train_msMAE=5.952 || val_loss=2452.0151  val_pMAE=11.673  val_msMAE=5.346\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=11.673)\n",
            "\n",
            "Epoch 06 | train_loss=2111.0998  train_pMAE=11.890  train_msMAE=5.787 || val_loss=1613.1224  val_pMAE=10.436  val_msMAE=4.311\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=10.436)\n",
            "\n",
            "Epoch 07 | train_loss=1444.7066  train_pMAE=11.302  train_msMAE=5.224 || val_loss=1036.8915  val_pMAE=9.370  val_msMAE=4.485\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=9.370)\n",
            "\n",
            "Epoch 08 | train_loss=921.8359  train_pMAE=10.546  train_msMAE=4.687 || val_loss=574.0353  val_pMAE=8.226  val_msMAE=3.811\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=8.226)\n",
            "\n",
            "Epoch 09 | train_loss=548.1595  train_pMAE=9.847  train_msMAE=4.177 || val_loss=243.7025  val_pMAE=6.967  val_msMAE=3.363\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=6.967)\n",
            "\n",
            "Epoch 10 | train_loss=329.7928  train_pMAE=9.252  train_msMAE=3.946 || val_loss=241.4443  val_pMAE=7.075  val_msMAE=3.889\n",
            "\n",
            "Epoch 11 | train_loss=212.8298  train_pMAE=8.670  train_msMAE=3.625 || val_loss=208.0232  val_pMAE=6.977  val_msMAE=4.085\n",
            "\n",
            "Epoch 12 | train_loss=168.0402  train_pMAE=8.438  train_msMAE=3.654 || val_loss=132.1661  val_pMAE=6.435  val_msMAE=3.741\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=6.435)\n",
            "\n",
            "Epoch 13 | train_loss=149.0993  train_pMAE=8.252  train_msMAE=3.606 || val_loss=107.2868  val_pMAE=5.919  val_msMAE=3.431\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=5.919)\n",
            "\n",
            "Epoch 14 | train_loss=138.8453  train_pMAE=8.125  train_msMAE=3.592 || val_loss=118.3505  val_pMAE=5.979  val_msMAE=3.624\n",
            "\n",
            "Epoch 15 | train_loss=137.9331  train_pMAE=8.039  train_msMAE=3.518 || val_loss=110.8770  val_pMAE=6.528  val_msMAE=4.032\n",
            "\n",
            "Epoch 16 | train_loss=134.6642  train_pMAE=8.064  train_msMAE=3.614 || val_loss=100.3420  val_pMAE=6.218  val_msMAE=3.780\n",
            "\n",
            "Epoch 17 | train_loss=131.3404  train_pMAE=7.879  train_msMAE=3.365 || val_loss=106.4591  val_pMAE=5.985  val_msMAE=3.481\n",
            "\n",
            "Epoch 18 | train_loss=126.8479  train_pMAE=7.780  train_msMAE=3.369 || val_loss=97.2913  val_pMAE=5.964  val_msMAE=3.504\n",
            "\n",
            "Epoch 19 | train_loss=125.9930  train_pMAE=7.724  train_msMAE=3.284 || val_loss=97.2029  val_pMAE=5.859  val_msMAE=3.400\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=5.859)\n",
            "\n",
            "Epoch 20 | train_loss=123.8198  train_pMAE=7.706  train_msMAE=3.303 || val_loss=101.8910  val_pMAE=6.117  val_msMAE=3.692\n",
            "\n",
            "Epoch 21 | train_loss=126.1282  train_pMAE=7.739  train_msMAE=3.424 || val_loss=97.3895  val_pMAE=5.805  val_msMAE=3.468\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=5.805)\n",
            "\n",
            "Epoch 22 | train_loss=124.3815  train_pMAE=7.671  train_msMAE=3.424 || val_loss=97.7868  val_pMAE=6.040  val_msMAE=3.613\n",
            "\n",
            "Epoch 23 | train_loss=124.0786  train_pMAE=7.654  train_msMAE=3.361 || val_loss=99.8838  val_pMAE=5.803  val_msMAE=3.243\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=5.803)\n",
            "\n",
            "Epoch 24 | train_loss=123.2037  train_pMAE=7.611  train_msMAE=3.427 || val_loss=97.4573  val_pMAE=5.935  val_msMAE=3.440\n",
            "\n",
            "Epoch 25 | train_loss=122.0119  train_pMAE=7.617  train_msMAE=3.401 || val_loss=99.9910  val_pMAE=6.030  val_msMAE=3.545\n",
            "\n",
            "Epoch 26 | train_loss=121.9809  train_pMAE=7.609  train_msMAE=3.292 || val_loss=96.0171  val_pMAE=5.790  val_msMAE=3.333\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=5.790)\n",
            "\n",
            "Epoch 27 | train_loss=120.4763  train_pMAE=7.550  train_msMAE=3.328 || val_loss=95.7376  val_pMAE=5.931  val_msMAE=3.484\n",
            "\n",
            "Epoch 28 | train_loss=120.0006  train_pMAE=7.480  train_msMAE=3.326 || val_loss=96.7243  val_pMAE=5.918  val_msMAE=3.453\n",
            "\n",
            "Epoch 29 | train_loss=118.7078  train_pMAE=7.448  train_msMAE=3.205 || val_loss=96.4852  val_pMAE=5.949  val_msMAE=3.517\n",
            "\n",
            "Epoch 30 | train_loss=118.5006  train_pMAE=7.487  train_msMAE=3.309 || val_loss=94.8791  val_pMAE=5.838  val_msMAE=3.369\n",
            "\n",
            "Epoch 31 | train_loss=117.6830  train_pMAE=7.422  train_msMAE=3.273 || val_loss=95.7134  val_pMAE=5.891  val_msMAE=3.506\n",
            "\n",
            "Epoch 32 | train_loss=117.8911  train_pMAE=7.402  train_msMAE=3.272 || val_loss=94.0194  val_pMAE=5.831  val_msMAE=3.398\n",
            "\n",
            "Epoch 33 | train_loss=118.0288  train_pMAE=7.471  train_msMAE=3.395 || val_loss=95.6829  val_pMAE=5.875  val_msMAE=3.451\n",
            "\n",
            "Epoch 34 | train_loss=116.2708  train_pMAE=7.386  train_msMAE=3.201 || val_loss=95.4828  val_pMAE=5.930  val_msMAE=3.538\n",
            "\n",
            "Epoch 35 | train_loss=116.8343  train_pMAE=7.415  train_msMAE=3.345 || val_loss=93.9819  val_pMAE=5.763  val_msMAE=3.350\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_cfp.pth (pMAE=5.763)\n",
            "\n",
            "Epoch 36 | train_loss=116.3322  train_pMAE=7.340  train_msMAE=3.232 || val_loss=93.7676  val_pMAE=5.870  val_msMAE=3.466\n",
            "\n",
            "Epoch 37 | train_loss=115.4588  train_pMAE=7.399  train_msMAE=3.331 || val_loss=93.7132  val_pMAE=5.817  val_msMAE=3.400\n",
            "\n",
            "Epoch 38 | train_loss=114.8895  train_pMAE=7.339  train_msMAE=3.318 || val_loss=94.0252  val_pMAE=5.841  val_msMAE=3.412\n",
            "\n",
            "Epoch 39 | train_loss=114.2725  train_pMAE=7.309  train_msMAE=3.306 || val_loss=95.2568  val_pMAE=5.903  val_msMAE=3.490\n",
            "\n",
            "Epoch 40 | train_loss=115.3555  train_pMAE=7.340  train_msMAE=3.339 || val_loss=93.8215  val_pMAE=5.835  val_msMAE=3.409\n",
            "\n",
            "Epoch 41 | train_loss=117.1452  train_pMAE=7.387  train_msMAE=3.348 || val_loss=93.8686  val_pMAE=5.818  val_msMAE=3.398\n",
            "\n",
            "Epoch 42 | train_loss=115.9654  train_pMAE=7.296  train_msMAE=3.208 || val_loss=94.4800  val_pMAE=5.862  val_msMAE=3.464\n",
            "\n",
            "Epoch 43 | train_loss=112.9378  train_pMAE=7.270  train_msMAE=3.358 || val_loss=93.7590  val_pMAE=5.846  val_msMAE=3.433\n",
            "\n",
            "Epoch 44 | train_loss=113.0148  train_pMAE=7.263  train_msMAE=3.193 || val_loss=94.0471  val_pMAE=5.864  val_msMAE=3.451\n",
            "\n",
            "Epoch 45 | train_loss=113.0860  train_pMAE=7.285  train_msMAE=3.270 || val_loss=94.0443  val_pMAE=5.852  val_msMAE=3.445\n",
            "\n",
            "Epoch 46 | train_loss=115.3510  train_pMAE=7.373  train_msMAE=3.399 || val_loss=93.6235  val_pMAE=5.815  val_msMAE=3.390\n",
            "\n",
            "Early stopping at epoch 46 (best val pMAE=5.763)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- reproducibility (same as before)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class EarlyStopper:\n",
        "    \"\"\"\n",
        "    - Saves whenever val_metric strictly improves over 'best' (tolerance=1e-12).\n",
        "    - Uses 'min_delta' only to decide whether to reset patience (ref metric).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best_save = float(\"inf\")  # for checkpoint saving (any improvement)\n",
        "        self.best_ref = float(\"inf\")  # for patience (needs >= min_delta improvement)\n",
        "        self.bad_epochs = 0\n",
        "        if self.ckpt_path:\n",
        "            os.makedirs(os.path.dirname(self.ckpt_path), exist_ok=True)\n",
        "\n",
        "    def update(self, val_metric, model, epoch_meta=None):\n",
        "        saved = False\n",
        "        # --- Save on ANY strict improvement\n",
        "        if val_metric < self.best_save - 1e-12:\n",
        "            self.best_save = val_metric\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best_save,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            saved = True\n",
        "\n",
        "        # --- Early-stopping patience uses min_delta\n",
        "        if val_metric < self.best_ref - self.min_delta:\n",
        "            self.best_ref = val_metric\n",
        "            self.bad_epochs = 0\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "\n",
        "        should_stop = self.bad_epochs > self.patience\n",
        "        return should_stop, saved\n",
        "\n",
        "\n",
        "def main():\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] using {len(vf_cols)} VF columns: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # split\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    random.shuffle(rows)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    train_ds = CFPDataset(train_rows, image_col, vf_cols, train=True)\n",
        "    val_ds = CFPDataset(val_rows, image_col, vf_cols, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = EfficientNetB3VF(out_dim=NUM_POINTS, pretrained=True).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt_path = os.path.join(CHECK_DIR, \"best_efficientnetb3_original_cfp.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt_path)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        print(\n",
        "            f\"\\nEpoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_msMAE={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_msMAE={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # scheduler on validation MAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        # early stopping + save best\n",
        "        should_stop, saved = stopper.update(\n",
        "            va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch}\n",
        "        )\n",
        "        if saved:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt_path} (pMAE={stopper.best_save:.3f})\")\n",
        "        if should_stop:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (best val pMAE={stopper.best_save:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best before returning\n",
        "    state = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt_path\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, train_dl, val_dl, image_col, vf_cols, CKPT = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:29:03.478902Z",
          "iopub.status.busy": "2025-11-10T05:29:03.478577Z",
          "iopub.status.idle": "2025-11-10T05:29:07.443091Z",
          "shell.execute_reply": "2025-11-10T05:29:07.442164Z",
          "shell.execute_reply.started": "2025-11-10T05:29:03.478871Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbDafnf0Fbhm",
        "outputId": "625597b9-b844-456e-f538-07569007b5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using BEST checkpoint: ./checkpoints/best_efficientnetb3_original_cfp.pth\n",
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n"
          ]
        }
      ],
      "source": [
        "# --- reload BEST checkpoint and evaluate ---\n",
        "assert \"CKPT\" in globals(), (\n",
        "    \"CKPT not found. Make sure you ran the training cell that returns CKPT.\"\n",
        ")\n",
        "assert \"val_dl\" in globals(), (\n",
        "    \"val_dl not found. Make sure you ran the training cell that defines val_dl.\"\n",
        ")\n",
        "\n",
        "# rebuild the exact architecture\n",
        "best_model = EfficientNetB3VF(out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "\n",
        "state = torch.load(CKPT, map_location=DEVICE)\n",
        "best_model.load_state_dict(state[\"model\"])\n",
        "best_model.eval()\n",
        "\n",
        "# collect predictions on the validation set\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in val_dl:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        p = best_model(x)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "\n",
        "print(\"✅ Using BEST checkpoint:\", CKPT)\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:29:10.997372Z",
          "iopub.status.busy": "2025-11-10T05:29:10.997079Z",
          "iopub.status.idle": "2025-11-10T05:29:11.006552Z",
          "shell.execute_reply": "2025-11-10T05:29:11.005959Z",
          "shell.execute_reply.started": "2025-11-10T05:29:10.997345Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr5jlMMpFbhn",
        "outputId": "dd8b29d9-bfe6-48a7-b029-30708263aa70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== POINTWISE ==\n",
            "RMSE: 9.6944 | MAE: 5.7634 | R²: 0.9803\n",
            "== POINTWISE-MEAN ==\n",
            "RMSE: 4.4893 | MAE: 3.3497 | R²: 0.2574\n",
            "== MS (same as pointwise-mean) ==\n",
            "RMSE: 4.4893 | MAE: 3.3497 | R²: 0.2574\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_val(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "print(\"\\n== POINTWISE ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(pw_true, pw_pred):.4f} | MAE: {mae_val(pw_true, pw_pred):.4f} | R²: {r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "\n",
        "t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "print(\"== POINTWISE-MEAN ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | MAE: {mae_val(t_mean, p_mean):.4f} | R²: {r2(t_mean, p_mean):.4f}\"\n",
        ")\n",
        "\n",
        "print(\"== MS (same as pointwise-mean) ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | MAE: {mae_val(t_mean, p_mean):.4f} | R²: {r2(t_mean, p_mean):.4f}\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.117107Z",
          "iopub.status.busy": "2025-11-08T09:56:44.116818Z",
          "iopub.status.idle": "2025-11-08T09:56:44.137012Z",
          "shell.execute_reply": "2025-11-08T09:56:44.136317Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.117090Z"
        },
        "id": "9jO8WiU2Fbho"
      },
      "source": [
        "# 2. ROI Images EfficientNetB3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.138029Z",
          "iopub.status.busy": "2025-11-08T09:56:44.137733Z",
          "iopub.status.idle": "2025-11-08T09:56:44.156020Z",
          "shell.execute_reply": "2025-11-08T09:56:44.155523Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.138001Z"
        },
        "lines_to_next_cell": 2,
        "id": "s0C6lJXYFbho"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.156862Z",
          "iopub.status.busy": "2025-11-08T09:56:44.156704Z",
          "iopub.status.idle": "2025-11-08T09:56:44.195467Z",
          "shell.execute_reply": "2025-11-08T09:56:44.194795Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.156849Z"
        },
        "id": "CY-dmAzVFbho"
      },
      "outputs": [],
      "source": [
        "# ===================== LABELME JSON → OD/OC MASKS =====================\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _poly_area(pts):\n",
        "    x = [p[0] for p in pts]\n",
        "    y = [p[1] for p in pts]\n",
        "    return 0.5 * abs(\n",
        "        sum(x[i] * y[(i + 1) % len(pts)] - x[(i + 1) % len(pts)] * y[i] for i in range(len(pts)))\n",
        "    )\n",
        "\n",
        "\n",
        "def _read_labelme(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        label = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = [(float(x), float(y)) for x, y in sh.get(\"points\", [])]\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if label in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        elif label in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=_poly_area)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=_poly_area)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json_path(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        cand = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(cand):\n",
        "            return cand\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def build_masks_from_labelme(img_pil: Image.Image, img_name: str, out_size: int):\n",
        "    W, H = img_pil.size\n",
        "    od_mask = Image.new(\"L\", (W, H), 0)\n",
        "    oc_mask = Image.new(\"L\", (W, H), 0)\n",
        "\n",
        "    jpath = _guess_json_path(img_name)\n",
        "    if jpath:\n",
        "        try:\n",
        "            od_polys, oc_polys = _read_labelme(jpath)\n",
        "            d_od = ImageDraw.Draw(od_mask)\n",
        "            d_oc = ImageDraw.Draw(oc_mask)\n",
        "            for poly in od_polys:\n",
        "                d_od.polygon(poly, outline=1, fill=1)\n",
        "            for poly in oc_polys:\n",
        "                d_oc.polygon(poly, outline=1, fill=1)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] parsing {jpath}: {e}\")\n",
        "\n",
        "    od_mask = od_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    oc_mask = oc_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    return od_mask, oc_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.196439Z",
          "iopub.status.busy": "2025-11-08T09:56:44.196246Z",
          "iopub.status.idle": "2025-11-08T09:56:44.218107Z",
          "shell.execute_reply": "2025-11-08T09:56:44.217506Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.196424Z"
        },
        "id": "lNIqwULdFbhp"
      },
      "outputs": [],
      "source": [
        "class CFPDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows: List[Dict[str, str]], image_col: str, vf_cols: List[str], train: bool\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.train = train\n",
        "\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "\n",
        "        self.tf = transforms.Compose(\n",
        "            [transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(), *aug, normalize]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        name = r[self.image_col]\n",
        "\n",
        "        path = name\n",
        "        if not os.path.isabs(path):\n",
        "            if os.path.basename(path) == path:\n",
        "                path = os.path.join(CFP_DIR, path)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        x = self.tf(img)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.219231Z",
          "iopub.status.busy": "2025-11-08T09:56:44.218972Z",
          "iopub.status.idle": "2025-11-08T09:56:44.240915Z",
          "shell.execute_reply": "2025-11-08T09:56:44.240375Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.219208Z"
        },
        "id": "U86UPQviFbhp"
      },
      "outputs": [],
      "source": [
        "class EfficientNetB3VF(nn.Module):\n",
        "    def __init__(self, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.efficientnet_b3(\n",
        "            weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        in_f = self.backbone.classifier[-1].in_features\n",
        "        self.backbone.classifier[-1] = nn.Identity()\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25), nn.Linear(512, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x)\n",
        "        return self.regressor(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.243731Z",
          "iopub.status.busy": "2025-11-08T09:56:44.243485Z",
          "iopub.status.idle": "2025-11-08T09:56:44.256968Z",
          "shell.execute_reply": "2025-11-08T09:56:44.256437Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.243714Z"
        },
        "id": "zGnOCxHKFbhp"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = crit(pred, y)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        n += bs\n",
        "\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:29:55.027957Z",
          "iopub.status.busy": "2025-11-10T05:29:55.027457Z",
          "iopub.status.idle": "2025-11-10T05:40:46.560388Z",
          "shell.execute_reply": "2025-11-10T05:40:46.559609Z",
          "shell.execute_reply.started": "2025-11-10T05:29:55.027932Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcELCgaAFbhq",
        "outputId": "e6d01687-f12c-4daa-b97f-0d3eeb7b6129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] using 59 VF columns: ['AGE', 'CCT', 'IOP_y', 'Interval Years', 'MD'] ... ['VF50', 'VF51', 'VF52', 'VF53', 'VF54']\n",
            "\n",
            "Epoch 01 | train_loss=5432.5966  train_pMAE=28.545  train_msMAE=28.374 || val_loss=5385.1048  val_pMAE=27.339  val_msMAE=27.058\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_roi.pth (pMAE=27.339)\n",
            "\n",
            "Epoch 02 | train_loss=5066.9832  train_pMAE=25.216  train_msMAE=24.701 || val_loss=4714.2336  val_pMAE=20.361  val_msMAE=18.884\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_roi.pth (pMAE=20.361)\n",
            "\n",
            "Epoch 03 | train_loss=4362.2197  train_pMAE=17.629  train_msMAE=14.391 || val_loss=4189.4331  val_pMAE=14.545  val_msMAE=10.513\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_roi.pth (pMAE=14.545)\n",
            "\n",
            "Epoch 04 | train_loss=3665.1053  train_pMAE=14.675  train_msMAE=6.533 || val_loss=3390.3623  val_pMAE=13.161  val_msMAE=5.892\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_roi.pth (pMAE=13.161)\n",
            "\n",
            "Epoch 05 | train_loss=2924.9175  train_pMAE=13.165  train_msMAE=6.221 || val_loss=2438.1470  val_pMAE=11.688  val_msMAE=5.088\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_roi.pth (pMAE=11.688)\n",
            "\n",
            "Epoch 06 | train_loss=2142.2876  train_pMAE=12.016  train_msMAE=5.865 || val_loss=1672.9144  val_pMAE=10.549  val_msMAE=4.681\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_roi.pth (pMAE=10.549)\n",
            "\n",
            "Epoch 07 | train_loss=1459.1504  train_pMAE=11.317  train_msMAE=5.310 || val_loss=1150.9195  val_pMAE=9.769  val_msMAE=5.138\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_roi.pth (pMAE=9.769)\n",
            "\n",
            "Epoch 08 | train_loss=921.7107  train_pMAE=10.522  train_msMAE=4.667 || val_loss=522.5303  val_pMAE=8.173  val_msMAE=3.651\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_roi.pth (pMAE=8.173)\n",
            "\n",
            "Epoch 09 | train_loss=544.0979  train_pMAE=9.826  train_msMAE=4.133 || val_loss=390.4305  val_pMAE=7.830  val_msMAE=4.389\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_roi.pth (pMAE=7.830)\n",
            "\n",
            "Epoch 10 | train_loss=324.2114  train_pMAE=9.223  train_msMAE=3.980 || val_loss=389.8602  val_pMAE=8.294  val_msMAE=5.191\n",
            "\n",
            "Epoch 11 | train_loss=213.3730  train_pMAE=8.700  train_msMAE=3.780 || val_loss=200.3208  val_pMAE=6.957  val_msMAE=4.203\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_roi.pth (pMAE=6.957)\n",
            "\n",
            "Epoch 12 | train_loss=167.4429  train_pMAE=8.409  train_msMAE=3.588 || val_loss=184.4582  val_pMAE=7.210  val_msMAE=4.600\n",
            "\n",
            "Epoch 13 | train_loss=151.3995  train_pMAE=8.285  train_msMAE=3.666 || val_loss=109.8240  val_pMAE=5.976  val_msMAE=3.549\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_roi.pth (pMAE=5.976)\n",
            "\n",
            "Epoch 14 | train_loss=139.2247  train_pMAE=8.123  train_msMAE=3.602 || val_loss=109.8105  val_pMAE=5.938  val_msMAE=3.533\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_roi.pth (pMAE=5.938)\n",
            "\n",
            "Epoch 15 | train_loss=134.4735  train_pMAE=8.018  train_msMAE=3.492 || val_loss=100.8836  val_pMAE=6.237  val_msMAE=3.819\n",
            "\n",
            "Epoch 16 | train_loss=132.1827  train_pMAE=7.997  train_msMAE=3.498 || val_loss=101.0276  val_pMAE=6.278  val_msMAE=3.951\n",
            "\n",
            "Epoch 17 | train_loss=133.3642  train_pMAE=7.909  train_msMAE=3.484 || val_loss=103.7439  val_pMAE=5.935  val_msMAE=3.554\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_roi.pth (pMAE=5.935)\n",
            "\n",
            "Epoch 18 | train_loss=130.3861  train_pMAE=7.840  train_msMAE=3.430 || val_loss=109.6194  val_pMAE=6.248  val_msMAE=3.885\n",
            "\n",
            "Epoch 19 | train_loss=124.3810  train_pMAE=7.686  train_msMAE=3.283 || val_loss=100.6630  val_pMAE=5.955  val_msMAE=3.502\n",
            "\n",
            "Epoch 20 | train_loss=122.8446  train_pMAE=7.660  train_msMAE=3.341 || val_loss=115.6515  val_pMAE=6.596  val_msMAE=4.208\n",
            "\n",
            "Epoch 21 | train_loss=127.6845  train_pMAE=7.696  train_msMAE=3.491 || val_loss=100.5264  val_pMAE=5.943  val_msMAE=3.608\n",
            "\n",
            "Epoch 22 | train_loss=121.6479  train_pMAE=7.573  train_msMAE=3.450 || val_loss=97.8947  val_pMAE=6.018  val_msMAE=3.632\n",
            "\n",
            "Epoch 23 | train_loss=121.5563  train_pMAE=7.553  train_msMAE=3.375 || val_loss=98.5569  val_pMAE=5.875  val_msMAE=3.406\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_original_roi.pth (pMAE=5.875)\n",
            "\n",
            "Epoch 24 | train_loss=120.2191  train_pMAE=7.508  train_msMAE=3.407 || val_loss=100.9174  val_pMAE=6.040  val_msMAE=3.654\n",
            "\n",
            "Epoch 25 | train_loss=120.7162  train_pMAE=7.529  train_msMAE=3.406 || val_loss=100.4655  val_pMAE=6.061  val_msMAE=3.709\n",
            "\n",
            "Epoch 26 | train_loss=121.3089  train_pMAE=7.532  train_msMAE=3.338 || val_loss=97.8535  val_pMAE=5.877  val_msMAE=3.583\n",
            "\n",
            "Epoch 27 | train_loss=119.3333  train_pMAE=7.466  train_msMAE=3.333 || val_loss=97.7943  val_pMAE=5.947  val_msMAE=3.569\n",
            "\n",
            "Epoch 28 | train_loss=117.8162  train_pMAE=7.366  train_msMAE=3.299 || val_loss=98.2492  val_pMAE=6.013  val_msMAE=3.701\n",
            "\n",
            "Epoch 29 | train_loss=116.6846  train_pMAE=7.370  train_msMAE=3.280 || val_loss=98.3828  val_pMAE=5.960  val_msMAE=3.632\n",
            "\n",
            "Epoch 30 | train_loss=115.8770  train_pMAE=7.377  train_msMAE=3.276 || val_loss=96.9094  val_pMAE=5.909  val_msMAE=3.586\n",
            "\n",
            "Epoch 31 | train_loss=116.2765  train_pMAE=7.346  train_msMAE=3.238 || val_loss=97.7851  val_pMAE=5.963  val_msMAE=3.665\n",
            "\n",
            "Epoch 32 | train_loss=118.4673  train_pMAE=7.379  train_msMAE=3.315 || val_loss=96.9982  val_pMAE=5.896  val_msMAE=3.586\n",
            "\n",
            "Epoch 33 | train_loss=116.5075  train_pMAE=7.396  train_msMAE=3.414 || val_loss=97.6668  val_pMAE=5.976  val_msMAE=3.658\n",
            "\n",
            "Epoch 34 | train_loss=115.9160  train_pMAE=7.339  train_msMAE=3.192 || val_loss=97.5640  val_pMAE=5.966  val_msMAE=3.649\n",
            "\n",
            "Early stopping at epoch 34 (best val pMAE=5.875)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- reproducibility (same as before)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class EarlyStopper:\n",
        "    \"\"\"\n",
        "    - Saves whenever val_metric strictly improves over 'best' (tolerance=1e-12).\n",
        "    - Uses 'min_delta' only to decide whether to reset patience (ref metric).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best_save = float(\"inf\")  # for checkpoint saving (any improvement)\n",
        "        self.best_ref = float(\"inf\")  # for patience (needs >= min_delta improvement)\n",
        "        self.bad_epochs = 0\n",
        "        if self.ckpt_path:\n",
        "            os.makedirs(os.path.dirname(self.ckpt_path), exist_ok=True)\n",
        "\n",
        "    def update(self, val_metric, model, epoch_meta=None):\n",
        "        saved = False\n",
        "        # --- Save on ANY strict improvement\n",
        "        if val_metric < self.best_save - 1e-12:\n",
        "            self.best_save = val_metric\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best_save,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            saved = True\n",
        "\n",
        "        # --- Early-stopping patience uses min_delta\n",
        "        if val_metric < self.best_ref - self.min_delta:\n",
        "            self.best_ref = val_metric\n",
        "            self.bad_epochs = 0\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "\n",
        "        should_stop = self.bad_epochs > self.patience\n",
        "        return should_stop, saved\n",
        "\n",
        "\n",
        "def main():\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] using {len(vf_cols)} VF columns: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # split\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    random.shuffle(rows)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    train_ds = CFPDataset(train_rows, image_col, vf_cols, train=True)\n",
        "    val_ds = CFPDataset(val_rows, image_col, vf_cols, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = EfficientNetB3VF(out_dim=NUM_POINTS, pretrained=True).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt_path = os.path.join(CHECK_DIR, \"best_efficientnetb3_original_roi.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt_path)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        print(\n",
        "            f\"\\nEpoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_msMAE={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_msMAE={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # scheduler on validation MAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        # early stopping + save best\n",
        "        should_stop, saved = stopper.update(\n",
        "            va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch}\n",
        "        )\n",
        "        if saved:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt_path} (pMAE={stopper.best_save:.3f})\")\n",
        "        if should_stop:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (best val pMAE={stopper.best_save:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best before returning\n",
        "    state = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt_path\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, train_dl, val_dl, image_col, vf_cols, CKPT = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:41:26.754434Z",
          "iopub.status.busy": "2025-11-10T05:41:26.753893Z",
          "iopub.status.idle": "2025-11-10T05:41:30.190236Z",
          "shell.execute_reply": "2025-11-10T05:41:30.189451Z",
          "shell.execute_reply.started": "2025-11-10T05:41:26.754411Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcdENyA-Fbhq",
        "outputId": "a9cba70a-479b-4f3c-8314-e316307a0d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ROI predictions collected: torch.Size([127, 59]) torch.Size([127, 59])\n"
          ]
        }
      ],
      "source": [
        "# reload best ROI checkpoint\n",
        "best_roi = EfficientNetB3VF(out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "state = torch.load(CKPT, map_location=DEVICE)\n",
        "best_roi.load_state_dict(state[\"model\"])\n",
        "best_roi.eval()\n",
        "\n",
        "# collect predictions\n",
        "all_true = []\n",
        "all_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in val_dl:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        p = best_roi(x)\n",
        "\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "\n",
        "print(\"✅ ROI predictions collected:\", y_true.shape, y_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:41:33.799312Z",
          "iopub.status.busy": "2025-11-10T05:41:33.798656Z",
          "iopub.status.idle": "2025-11-10T05:41:33.808988Z",
          "shell.execute_reply": "2025-11-10T05:41:33.808148Z",
          "shell.execute_reply.started": "2025-11-10T05:41:33.799282Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4dayUZRFbhq",
        "outputId": "f8e770dc-2dad-4dd9-d2b4-0cc4f3aa7635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== ROI: POINTWISE ==\n",
            "RMSE: 9.9276\n",
            "MAE : 5.8750\n",
            "R²  : 0.9794\n",
            "\n",
            "== ROI: POINTWISE-MEAN / MS ==\n",
            "RMSE: 4.6096\n",
            "MAE : 3.4060\n",
            "R²  : 0.2170\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# helpers\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_val(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# POINTWISE\n",
        "pw_true = y_true.reshape(-1)\n",
        "pw_pred = y_pred.reshape(-1)\n",
        "\n",
        "print(\"\\n== ROI: POINTWISE ==\")\n",
        "print(f\"RMSE: {rmse(pw_true, pw_pred):.4f}\")\n",
        "print(f\"MAE : {mae_val(pw_true, pw_pred):.4f}\")\n",
        "print(f\"R²  : {r2(pw_true, pw_pred):.4f}\")\n",
        "\n",
        "# POINTWISE-MEAN / MS\n",
        "t_mean = y_true.mean(dim=1)\n",
        "p_mean = y_pred.mean(dim=1)\n",
        "\n",
        "print(\"\\n== ROI: POINTWISE-MEAN / MS ==\")\n",
        "print(f\"RMSE: {rmse(t_mean, p_mean):.4f}\")\n",
        "print(f\"MAE : {mae_val(t_mean, p_mean):.4f}\")\n",
        "print(f\"R²  : {r2(t_mean, p_mean):.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T10:00:28.759901Z",
          "iopub.status.busy": "2025-11-08T10:00:28.759605Z",
          "iopub.status.idle": "2025-11-08T10:00:28.775895Z",
          "shell.execute_reply": "2025-11-08T10:00:28.775339Z",
          "shell.execute_reply.started": "2025-11-08T10:00:28.759884Z"
        },
        "id": "5DFJrqEHFbhq"
      },
      "source": [
        "# 3. ROI + OD/OD Segmentation EfficientNetB3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:45:55.379375Z",
          "iopub.status.busy": "2025-11-10T05:45:55.378706Z",
          "iopub.status.idle": "2025-11-10T05:45:55.386060Z",
          "shell.execute_reply": "2025-11-10T05:45:55.385342Z",
          "shell.execute_reply.started": "2025-11-10T05:45:55.379352Z"
        },
        "id": "h6XFGx5-Fbhr"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# ---- paths ----\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "# ---- training ----\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:45:58.655021Z",
          "iopub.status.busy": "2025-11-10T05:45:58.654715Z",
          "iopub.status.idle": "2025-11-10T05:45:58.665138Z",
          "shell.execute_reply": "2025-11-10T05:45:58.664425Z",
          "shell.execute_reply.started": "2025-11-10T05:45:58.655000Z"
        },
        "id": "xDSu7OUhFbhr"
      },
      "outputs": [],
      "source": [
        "# ===================== CSV UTILS =====================\n",
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "IMAGE_COLS_CANDIDATES = [\"image\", \"image_name\", \"img\", \"image_path\", \"filename\", \"file\"]\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]) -> Tuple[str, List[str]]:\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # image col\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # VF cols prefer v1..v59 else numeric fallback\n",
        "    vf = [f\"v{i}\" for i in range(1, NUM_POINTS + 1)]\n",
        "    if all(c in cols for c in vf):\n",
        "        return image_col, vf\n",
        "\n",
        "    cand = []\n",
        "    for c in cols:\n",
        "        if c == image_col:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            cand.append(c)\n",
        "    if len(cand) < NUM_POINTS:\n",
        "        raise ValueError(f\"Need {NUM_POINTS} VF cols, found {len(cand)}.\")\n",
        "\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    cand = sorted(cand, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, cand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:02.177236Z",
          "iopub.status.busy": "2025-11-10T05:46:02.176683Z",
          "iopub.status.idle": "2025-11-10T05:46:02.187291Z",
          "shell.execute_reply": "2025-11-10T05:46:02.186682Z",
          "shell.execute_reply.started": "2025-11-10T05:46:02.177213Z"
        },
        "id": "4BHl8asqFbhr"
      },
      "outputs": [],
      "source": [
        "# ===================== LABELME JSON → OD/OC MASKS =====================\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _poly_area(pts):\n",
        "    x = [p[0] for p in pts]\n",
        "    y = [p[1] for p in pts]\n",
        "    return 0.5 * abs(\n",
        "        sum(x[i] * y[(i + 1) % len(pts)] - x[(i + 1) % len(pts)] * y[i] for i in range(len(pts)))\n",
        "    )\n",
        "\n",
        "\n",
        "def _read_labelme(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        label = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = [(float(x), float(y)) for x, y in sh.get(\"points\", [])]\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if label in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        elif label in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=_poly_area)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=_poly_area)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json_path(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        cand = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(cand):\n",
        "            return cand\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def build_masks_from_labelme(img_pil: Image.Image, img_name: str, out_size: int):\n",
        "    W, H = img_pil.size\n",
        "    od_mask = Image.new(\"L\", (W, H), 0)\n",
        "    oc_mask = Image.new(\"L\", (W, H), 0)\n",
        "\n",
        "    jpath = _guess_json_path(img_name)\n",
        "    if jpath:\n",
        "        try:\n",
        "            od_polys, oc_polys = _read_labelme(jpath)\n",
        "            d_od = ImageDraw.Draw(od_mask)\n",
        "            d_oc = ImageDraw.Draw(oc_mask)\n",
        "            for poly in od_polys:\n",
        "                d_od.polygon(poly, outline=1, fill=1)\n",
        "            for poly in oc_polys:\n",
        "                d_oc.polygon(poly, outline=1, fill=1)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] parsing {jpath}: {e}\")\n",
        "\n",
        "    od_mask = od_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    oc_mask = oc_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    return od_mask, oc_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:05.752315Z",
          "iopub.status.busy": "2025-11-10T05:46:05.751834Z",
          "iopub.status.idle": "2025-11-10T05:46:05.759405Z",
          "shell.execute_reply": "2025-11-10T05:46:05.758759Z",
          "shell.execute_reply.started": "2025-11-10T05:46:05.752295Z"
        },
        "id": "7oMMj35DFbhs"
      },
      "outputs": [],
      "source": [
        "# ===================== DATASET (5-channel RGB+OD+OC) =====================\n",
        "class ROI_OD_OC_Dataset(Dataset):\n",
        "    def __init__(self, rows, image_col, vf_cols, train=True, img_root=ROI_DIR, img_size=IMG_SIZE):\n",
        "        self.rows, self.image_col, self.vf_cols = rows, image_col, vf_cols\n",
        "        self.train, self.img_root, self.img_size = train, img_root, img_size\n",
        "\n",
        "        norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.rgb_tf = transforms.Compose(\n",
        "            [transforms.Resize((img_size, img_size)), transforms.ToTensor(), *aug, norm]\n",
        "        )\n",
        "        self.mask_tf = transforms.ToTensor()  # L → (1,H,W) float {0,1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        fn = r[self.image_col]\n",
        "        path = fn if os.path.isabs(fn) else os.path.join(self.img_root, fn)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        od_img, oc_img = build_masks_from_labelme(img, fn, self.img_size)\n",
        "\n",
        "        x_rgb = self.rgb_tf(img)  # (3,H,W)\n",
        "        x_od = self.mask_tf(od_img)  # (1,H,W)\n",
        "        x_oc = self.mask_tf(oc_img)  # (1,H,W)\n",
        "        x = torch.cat([x_rgb, x_od, x_oc], dim=0)  # (5,H,W)\n",
        "\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:11.357022Z",
          "iopub.status.busy": "2025-11-10T05:46:11.356328Z",
          "iopub.status.idle": "2025-11-10T05:46:11.363275Z",
          "shell.execute_reply": "2025-11-10T05:46:11.362574Z",
          "shell.execute_reply.started": "2025-11-10T05:46:11.356999Z"
        },
        "id": "QRllxZILFbhs"
      },
      "outputs": [],
      "source": [
        "# ===================== MODEL (ResNet-50 with 5-ch input) =====================\n",
        "class EfficientNetB3_5ch_VF(nn.Module):\n",
        "    def __init__(self, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        base = models.efficientnet_b3(\n",
        "            weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        # adapt Conv2dNormActivation: 3→5 channels (init extra channels with mean RGB weights)\n",
        "        old = base.features[0][0]\n",
        "        new = nn.Conv2d(\n",
        "            5,\n",
        "            old.out_channels,\n",
        "            kernel_size=old.kernel_size,\n",
        "            stride=old.stride,\n",
        "            padding=old.padding,\n",
        "            bias=(old.bias is not None),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            new.weight[:, :3, :, :] = old.weight\n",
        "            mean_w = old.weight.mean(dim=1, keepdim=True)\n",
        "            new.weight[:, 3:5, :, :] = mean_w.repeat(1, 2, 1, 1)\n",
        "            if old.bias is not None:\n",
        "                new.bias.copy_(old.bias)\n",
        "        base.features[0][0] = new\n",
        "\n",
        "        in_f = base.classifier[-1].in_features\n",
        "        base.classifier[-1] = nn.Identity()\n",
        "        self.backbone = base\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25), nn.Linear(512, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x5):\n",
        "        f = self.backbone(x5)\n",
        "        return self.regressor(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:14.956708Z",
          "iopub.status.busy": "2025-11-10T05:46:14.955948Z",
          "iopub.status.idle": "2025-11-10T05:46:14.963048Z",
          "shell.execute_reply": "2025-11-10T05:46:14.962411Z",
          "shell.execute_reply.started": "2025-11-10T05:46:14.956686Z"
        },
        "id": "QY3UCLynFbhs"
      },
      "outputs": [],
      "source": [
        "# ===================== METRICS + EPOCH LOOP =====================\n",
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:18.407433Z",
          "iopub.status.busy": "2025-11-10T05:46:18.406736Z",
          "iopub.status.idle": "2025-11-10T05:52:34.919034Z",
          "shell.execute_reply": "2025-11-10T05:52:34.918137Z",
          "shell.execute_reply.started": "2025-11-10T05:46:18.407410Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNJIL79sFbhs",
        "outputId": "f5987b83-3891-4d1c-ad85-d1237f99fb40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] 59 VF cols: ['AGE', 'CCT', 'IOP_y', 'Interval Years', 'MD'] ... ['VF50', 'VF51', 'VF52', 'VF53', 'VF54']\n",
            "Epoch 01 | train_loss=5436.3612  train_pMAE=28.561  train_MS=28.399 || val_loss=5399.5040  val_pMAE=27.394  val_MS=27.168\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC.pth (pMAE=27.394)\n",
            "\n",
            "Epoch 02 | train_loss=5079.8854  train_pMAE=25.301  train_MS=24.830 || val_loss=4714.5946  val_pMAE=20.211  val_MS=18.855\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC.pth (pMAE=20.211)\n",
            "\n",
            "Epoch 03 | train_loss=4389.6697  train_pMAE=17.832  train_MS=14.672 || val_loss=4305.3402  val_pMAE=15.405  val_MS=12.248\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC.pth (pMAE=15.405)\n",
            "\n",
            "Epoch 04 | train_loss=3718.8704  train_pMAE=14.496  train_MS=6.755 || val_loss=3443.5373  val_pMAE=13.136  val_MS=5.552\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC.pth (pMAE=13.136)\n",
            "\n",
            "Epoch 05 | train_loss=3019.3303  train_pMAE=13.190  train_MS=5.987 || val_loss=2456.0560  val_pMAE=12.122  val_MS=4.269\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC.pth (pMAE=12.122)\n",
            "\n",
            "Epoch 06 | train_loss=2253.8380  train_pMAE=12.167  train_MS=5.971 || val_loss=1731.0946  val_pMAE=10.636  val_MS=4.383\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC.pth (pMAE=10.636)\n",
            "\n",
            "Epoch 07 | train_loss=1557.8781  train_pMAE=11.552  train_MS=5.448 || val_loss=1133.1015  val_pMAE=9.563  val_MS=4.400\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC.pth (pMAE=9.563)\n",
            "\n",
            "Epoch 08 | train_loss=992.0178  train_pMAE=10.768  train_MS=4.934 || val_loss=836.1976  val_pMAE=9.113  val_MS=5.020\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC.pth (pMAE=9.113)\n",
            "\n",
            "Epoch 09 | train_loss=574.8994  train_pMAE=9.900  train_MS=4.280 || val_loss=329.2517  val_pMAE=7.352  val_MS=3.729\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC.pth (pMAE=7.352)\n",
            "\n",
            "Epoch 10 | train_loss=346.9001  train_pMAE=9.353  train_MS=3.940 || val_loss=220.6897  val_pMAE=7.086  val_MS=3.963\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC.pth (pMAE=7.086)\n",
            "\n",
            "Epoch 11 | train_loss=220.9936  train_pMAE=8.753  train_MS=3.614 || val_loss=122.7794  val_pMAE=6.126  val_MS=3.433\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC.pth (pMAE=6.126)\n",
            "\n",
            "Epoch 12 | train_loss=170.5649  train_pMAE=8.489  train_MS=3.640 || val_loss=129.5481  val_pMAE=6.540  val_MS=3.917\n",
            "Epoch 13 | train_loss=153.4964  train_pMAE=8.308  train_MS=3.631 || val_loss=115.2359  val_pMAE=6.236  val_MS=3.790\n",
            "Epoch 14 | train_loss=139.1641  train_pMAE=8.153  train_MS=3.436 || val_loss=101.0795  val_pMAE=5.890  val_MS=3.398\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC.pth (pMAE=5.890)\n",
            "\n",
            "Epoch 15 | train_loss=135.6327  train_pMAE=8.088  train_MS=3.432 || val_loss=115.7472  val_pMAE=6.282  val_MS=3.738\n",
            "Epoch 16 | train_loss=135.4013  train_pMAE=8.056  train_MS=3.492 || val_loss=98.5071  val_pMAE=5.984  val_MS=3.517\n",
            "Epoch 17 | train_loss=132.7665  train_pMAE=7.961  train_MS=3.435 || val_loss=105.2013  val_pMAE=6.178  val_MS=3.778\n",
            "Epoch 18 | train_loss=130.4484  train_pMAE=7.860  train_MS=3.302 || val_loss=98.3959  val_pMAE=6.163  val_MS=3.665\n",
            "Epoch 19 | train_loss=129.2972  train_pMAE=7.875  train_MS=3.511 || val_loss=99.0393  val_pMAE=6.038  val_MS=3.699\n",
            "Epoch 20 | train_loss=127.2299  train_pMAE=7.823  train_MS=3.479 || val_loss=96.7849  val_pMAE=6.128  val_MS=3.748\n",
            "Epoch 21 | train_loss=128.2210  train_pMAE=7.820  train_MS=3.491 || val_loss=101.7536  val_pMAE=6.005  val_MS=3.641\n",
            "Epoch 22 | train_loss=122.0162  train_pMAE=7.652  train_MS=3.216 || val_loss=96.9427  val_pMAE=5.967  val_MS=3.494\n",
            "Epoch 23 | train_loss=123.2779  train_pMAE=7.668  train_MS=3.348 || val_loss=98.8400  val_pMAE=6.052  val_MS=3.719\n",
            "Epoch 24 | train_loss=126.0203  train_pMAE=7.704  train_MS=3.354 || val_loss=97.0809  val_pMAE=5.933  val_MS=3.599\n",
            "Epoch 25 | train_loss=120.3058  train_pMAE=7.580  train_MS=3.327 || val_loss=97.3396  val_pMAE=5.975  val_MS=3.605\n",
            "Early stopping at epoch 25 (best val pMAE=5.890)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ---- (optional) reproducibility on small data\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---- Early Stopping helper\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best = float(\"inf\")\n",
        "        self.bad_epochs = 0\n",
        "\n",
        "    def step(self, val_metric, model, epoch_meta=None):\n",
        "        # returns True if we should stop\n",
        "        if val_metric < self.best - self.min_delta:\n",
        "            self.best = val_metric\n",
        "            self.bad_epochs = 0\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            return False\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "            return self.bad_epochs > self.patience\n",
        "\n",
        "\n",
        "# ===================== TRAIN =====================\n",
        "def train_resnet50_roi_odoc(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01):\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] {len(vf_cols)} VF cols: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    random.shuffle(rows)\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    train_ds = ROI_OD_OC_Dataset(train_rows, image_col, vf_cols, train=True)\n",
        "    val_ds = ROI_OD_OC_Dataset(val_rows, image_col, vf_cols, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = EfficientNetB3_5ch_VF(out_dim=NUM_POINTS, pretrained=True).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "    # ---- LR scheduler (plateau)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt = os.path.join(CHECK_DIR, \"best_efficientnetb3_ROI_ODOC.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        # step scheduler on validation pMAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # save best (and detect improvement for pretty print)\n",
        "        prev_best = stopper.best\n",
        "        should_stop = stopper.step(va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch})\n",
        "        if stopper.best < prev_best - MIN_DELTA:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt} (pMAE={stopper.best:.3f})\\n\")\n",
        "\n",
        "        if should_stop:\n",
        "            print(f\"Early stopping at epoch {epoch} (best val pMAE={stopper.best:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best weights before returning\n",
        "    state = torch.load(ckpt, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt\n",
        "\n",
        "\n",
        "# run training and expose globals\n",
        "model_odoc, train_dl_odoc, val_dl_odoc, image_col_odoc, vf_cols_odoc, CKPT_ODOC = (\n",
        "    train_resnet50_roi_odoc(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:52:45.952603Z",
          "iopub.status.busy": "2025-11-10T05:52:45.951688Z",
          "iopub.status.idle": "2025-11-10T05:52:47.343625Z",
          "shell.execute_reply": "2025-11-10T05:52:47.342644Z",
          "shell.execute_reply.started": "2025-11-10T05:52:45.952569Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXsYGVmMFbht",
        "outputId": "00c04c11-068b-4b54-f4df-81a3f6c74dbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n",
            "\n",
            "== ROI+OD/OC: POINTWISE ==\n",
            "RMSE: 10.0538\n",
            "MAE : 5.8898\n",
            "R²  : 0.9788\n",
            "\n",
            "== ROI+OD/OC: POINTWISE-MEAN / MS ==\n",
            "RMSE: 4.6389\n",
            "MAE : 3.3981\n",
            "R²  : 0.2070\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ===================== EVALUATE BEST + PAPER METRICS =====================\n",
        "# reload best\n",
        "best_odoc = EfficientNetB3_5ch_VF(out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "state = torch.load(CKPT_ODOC, map_location=DEVICE)\n",
        "best_odoc.load_state_dict(state[\"model\"])\n",
        "best_odoc.eval()\n",
        "\n",
        "# predictions\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in val_dl_odoc:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        p = best_odoc(x)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)\n",
        "\n",
        "\n",
        "# metrics (safe names to avoid clobbering mae/ms_mae)\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_value(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "print(\"\\n== ROI+OD/OC: POINTWISE ==\")\n",
        "print(f\"RMSE: {rmse(pw_true, pw_pred):.4f}\")\n",
        "print(f\"MAE : {mae_value(pw_true, pw_pred):.4f}\")\n",
        "print(f\"R²  : {r2(pw_true, pw_pred):.4f}\")\n",
        "\n",
        "t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "print(\"\\n== ROI+OD/OC: POINTWISE-MEAN / MS ==\")\n",
        "print(f\"RMSE: {rmse(t_mean, p_mean):.4f}\")\n",
        "print(f\"MAE : {mae_value(t_mean, p_mean):.4f}\")\n",
        "print(f\"R²  : {r2(t_mean, p_mean):.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T10:04:17.910859Z",
          "iopub.status.busy": "2025-11-08T10:04:17.910580Z",
          "iopub.status.idle": "2025-11-08T10:04:17.914574Z",
          "shell.execute_reply": "2025-11-08T10:04:17.913924Z",
          "shell.execute_reply.started": "2025-11-08T10:04:17.910833Z"
        },
        "id": "dRdlhm_0Fbht"
      },
      "source": [
        "# 4. ROI + Clinical Features EfficientNetB3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:04:44.866421Z",
          "iopub.status.busy": "2025-11-10T06:04:44.865721Z",
          "iopub.status.idle": "2025-11-10T06:04:44.873765Z",
          "shell.execute_reply": "2025-11-10T06:04:44.873117Z",
          "shell.execute_reply.started": "2025-11-10T06:04:44.866399Z"
        },
        "id": "8T-N5XqvFbht"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "# ---- training ----\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59  # VF1..VF59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# clinical columns present in your CSV (plus computed ones)\n",
        "# from your columns: AGE, GENDER, IOP_y, MD exist; we’ll add computed CDR & PSD\n",
        "CLIN_NUM_COLS = [\"AGE\", \"IOP_y\", \"CDR\"]  # numeric\n",
        "CLIN_CAT_COLS = [\"GENDER\"]  # categorical (mapped to 0/1)\n",
        "IMAGE_COLS_CANDIDATES = [\n",
        "    \"Corresponding CFP\",\n",
        "    \"image\",\n",
        "    \"image_name\",\n",
        "    \"img\",\n",
        "    \"image_path\",\n",
        "    \"filename\",\n",
        "    \"file\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:04:52.367164Z",
          "iopub.status.busy": "2025-11-10T06:04:52.366737Z",
          "iopub.status.idle": "2025-11-10T06:04:52.378136Z",
          "shell.execute_reply": "2025-11-10T06:04:52.377479Z",
          "shell.execute_reply.started": "2025-11-10T06:04:52.367140Z"
        },
        "id": "wEQyKiW2Fbht"
      },
      "outputs": [],
      "source": [
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]) -> Tuple[str, List[str]]:\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # image column: prefer \"Corresponding CFP\" if present\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # prefer explicit VF1..VF59 (ignore VF0, VF60)\n",
        "    vf_cols_pref = [f\"VF{i}\" for i in range(1, 60)]\n",
        "    if all(c in cols for c in vf_cols_pref):\n",
        "        return image_col, vf_cols_pref\n",
        "\n",
        "    # fallback: numeric detection (exclude clinical & image)\n",
        "    excluded = set([image_col] + CLIN_NUM_COLS + CLIN_CAT_COLS + [\"VF0\", \"VF60\"])\n",
        "    candidates = []\n",
        "    for c in cols:\n",
        "        if c in excluded:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            candidates.append(c)\n",
        "\n",
        "    if len(candidates) < NUM_POINTS:\n",
        "        raise ValueError(\n",
        "            f\"Not enough numeric VF columns; found {len(candidates)}, need {NUM_POINTS}.\"\n",
        "        )\n",
        "\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    candidates_sorted = sorted(candidates, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, candidates_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:01.646023Z",
          "iopub.status.busy": "2025-11-10T06:05:01.645439Z",
          "iopub.status.idle": "2025-11-10T06:05:01.654860Z",
          "shell.execute_reply": "2025-11-10T06:05:01.654051Z",
          "shell.execute_reply.started": "2025-11-10T06:05:01.646002Z"
        },
        "id": "FGfecST8Fbht"
      },
      "outputs": [],
      "source": [
        "# ---- CDR from LabelMe polygons (vertical cup/disc ratio) ----\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _read_labelme_polys(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        lab = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = sh.get(\"points\", [])\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if lab in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        if lab in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "\n",
        "    # keep polygon with max vertical height if multiple\n",
        "    def vheight(poly):\n",
        "        ys = [p[1] for p in poly]\n",
        "        return (max(ys) - min(ys)) if ys else 0.0\n",
        "\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=vheight)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=vheight)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        p = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def compute_cdr_from_json(img_name: str):\n",
        "    \"\"\"\n",
        "    CDR = vertical height of OC / vertical height of OD.\n",
        "    Returns None if JSON missing or polygons absent.\n",
        "    \"\"\"\n",
        "    jpath = _guess_json(img_name)\n",
        "    if not jpath:\n",
        "        return None\n",
        "    try:\n",
        "        od_polys, oc_polys = _read_labelme_polys(jpath)\n",
        "        if not od_polys or not oc_polys:\n",
        "            return None\n",
        "\n",
        "        def vheight(poly):\n",
        "            ys = [float(y) for _, y in poly]\n",
        "            return max(ys) - min(ys) if ys else 0.0\n",
        "\n",
        "        h_od = vheight(od_polys[0])\n",
        "        h_oc = vheight(oc_polys[0])\n",
        "        if h_od <= 0:\n",
        "            return None\n",
        "        return float(h_oc / h_od)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] CDR parse failed for {img_name}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:08.135660Z",
          "iopub.status.busy": "2025-11-10T06:05:08.134961Z",
          "iopub.status.idle": "2025-11-10T06:05:08.139853Z",
          "shell.execute_reply": "2025-11-10T06:05:08.139232Z",
          "shell.execute_reply.started": "2025-11-10T06:05:08.135637Z"
        },
        "id": "NB4AALXmFbhu"
      },
      "outputs": [],
      "source": [
        "def augment_rows_with_cdr(rows, image_col, vf_cols):\n",
        "    augmented = []\n",
        "    miss_cdr = miss_psd = 0\n",
        "    for r in rows:\n",
        "        r2 = dict(r)\n",
        "        # compute CDR from JSON polygons\n",
        "        cdr = compute_cdr_from_json(r2[image_col])\n",
        "        if cdr is None:\n",
        "            miss_cdr += 1\n",
        "        r2[\"CDR\"] = cdr\n",
        "\n",
        "        augmented.append(r2)\n",
        "    print(f\"✅ Augmented rows: CDR missing={miss_cdr}, PSD missing={miss_psd}\")\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nbJpTzsyFbhu"
      },
      "outputs": [],
      "source": [
        "# ----------------- AUGMENT ROWS WITH CDR  -----------------\n",
        "def augment_rows_with_cdr_psd(rows, image_col, vf_cols):\n",
        "    augmented = []\n",
        "    miss_cdr = miss_psd = 0\n",
        "    for r in rows:\n",
        "        r2 = dict(r)\n",
        "        cdr = compute_cdr_from_json(r2[image_col])\n",
        "        if cdr is None:\n",
        "            miss_cdr += 1\n",
        "        r2[\"CDR\"] = cdr\n",
        "\n",
        "        augmented.append(r2)\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:11.650178Z",
          "iopub.status.busy": "2025-11-10T06:05:11.649913Z",
          "iopub.status.idle": "2025-11-10T06:05:11.657823Z",
          "shell.execute_reply": "2025-11-10T06:05:11.657129Z",
          "shell.execute_reply.started": "2025-11-10T06:05:11.650160Z"
        },
        "id": "xEScaVtVFbhu"
      },
      "outputs": [],
      "source": [
        "def to_float(x):\n",
        "    x = str(x).strip()\n",
        "    if x == \"\":\n",
        "        return None\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def fit_clinical_stats(rows, clin_num_cols):\n",
        "    stats = {}\n",
        "    for c in clin_num_cols:\n",
        "        vals = [to_float(r.get(c, \"\")) for r in rows]\n",
        "        vals = [v for v in vals if v is not None]\n",
        "        mean = np.mean(vals) if vals else 0.0\n",
        "        std = np.std(vals) if vals else 1.0\n",
        "        if std == 0:\n",
        "            std = 1.0\n",
        "        stats[c] = (float(mean), float(std))\n",
        "    return stats\n",
        "\n",
        "\n",
        "def encode_gender(x):\n",
        "    s = str(x).strip().lower()\n",
        "    if s in (\"m\", \"male\", \"man\"):\n",
        "        return 1.0\n",
        "    if s in (\"f\", \"female\", \"woman\"):\n",
        "        return 0.0\n",
        "    return 0.5  # unknown/other\n",
        "\n",
        "\n",
        "def build_clinical_vector(r, stats):\n",
        "    vec = []\n",
        "    for c in CLIN_NUM_COLS:\n",
        "        v = to_float(r.get(c, \"\"))\n",
        "        mean, std = stats[c]\n",
        "        v = mean if v is None else v\n",
        "        v = (v - mean) / std\n",
        "        vec.append(v)\n",
        "    for c in CLIN_CAT_COLS:\n",
        "        if c == \"GENDER\":\n",
        "            vec.append(encode_gender(r.get(c, \"\")))\n",
        "        else:\n",
        "            vec.append(0.0)\n",
        "    return torch.tensor(vec, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:17.437004Z",
          "iopub.status.busy": "2025-11-10T06:05:17.436286Z",
          "iopub.status.idle": "2025-11-10T06:05:17.443298Z",
          "shell.execute_reply": "2025-11-10T06:05:17.442595Z",
          "shell.execute_reply.started": "2025-11-10T06:05:17.436978Z"
        },
        "id": "yooJMl5RFbhu"
      },
      "outputs": [],
      "source": [
        "class ROIClinicalDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows, image_col, vf_cols, clin_stats, train=True, img_root=ROI_DIR, img_size=IMG_SIZE\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.clin_stats = clin_stats\n",
        "        self.train = train\n",
        "        self.img_root = img_root\n",
        "\n",
        "        normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.tf = transforms.Compose(\n",
        "            [transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(), *aug, normalize]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.rows[idx]\n",
        "        name = r[self.image_col]\n",
        "        path = name if os.path.isabs(name) else os.path.join(self.img_root, name)\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        x_img = self.tf(img)\n",
        "\n",
        "        x_clin = build_clinical_vector(r, self.clin_stats)  # (clin_dim,)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)  # (59,)\n",
        "\n",
        "        return x_img, x_clin, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "gqpwF0BjFbhu"
      },
      "outputs": [],
      "source": [
        "# ----------------- DATASET: 5-CH ROI + CLINICAL -----------------\n",
        "class ROI_ODOC_Clinical_Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows, image_col, vf_cols, clin_stats, train=True, img_root=ROI_DIR, img_size=IMG_SIZE\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.clin_stats = clin_stats\n",
        "        self.train = train\n",
        "        self.img_root = img_root\n",
        "        self.img_size = img_size\n",
        "\n",
        "        norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.rgb_tf = transforms.Compose(\n",
        "            [transforms.Resize((img_size, img_size)), transforms.ToTensor(), *aug, norm]\n",
        "        )\n",
        "        self.mask_tf = transforms.ToTensor()  # L→(1,H,W) float {0,1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.rows[idx]\n",
        "        fn = r[self.image_col]\n",
        "        path = fn if os.path.isabs(fn) else os.path.join(self.img_root, fn)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        od_img, oc_img = build_masks_from_labelme(img, fn, self.img_size)\n",
        "\n",
        "        x_rgb = self.rgb_tf(img)  # (3,H,W)\n",
        "        x_od = self.mask_tf(od_img)  # (1,H,W)\n",
        "        x_oc = self.mask_tf(oc_img)  # (1,H,W)\n",
        "        x5 = torch.cat([x_rgb, x_od, x_oc], dim=0)  # (5,H,W)\n",
        "\n",
        "        x_clin = build_clinical_vector(r, self.clin_stats)  # (clin_dim,)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)  # (59,)\n",
        "\n",
        "        return x5, x_clin, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:23.335737Z",
          "iopub.status.busy": "2025-11-10T06:05:23.335473Z",
          "iopub.status.idle": "2025-11-10T06:05:23.342292Z",
          "shell.execute_reply": "2025-11-10T06:05:23.341478Z",
          "shell.execute_reply.started": "2025-11-10T06:05:23.335717Z"
        },
        "id": "RXPGMh4PFbhv"
      },
      "outputs": [],
      "source": [
        "class EfficientNetB3_ROI_Clinical(nn.Module):\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.efficientnet_b3(\n",
        "            weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        in_f = self.backbone.classifier[-1].in_features\n",
        "        self.backbone.classifier[-1] = nn.Identity()\n",
        "\n",
        "        self.img_head = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.10),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(512 + 64, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_img, x_clin):\n",
        "        f = self.backbone(x_img)  # (B, 2048)\n",
        "        f = self.img_head(f)  # (B, 512)\n",
        "        g = self.clin_head(x_clin)  # (B, 64)\n",
        "        z = torch.cat([f, g], dim=1)  # (B, 576)\n",
        "        out = self.fuse(z)  # (B, 59)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FxCv69mLFbhv"
      },
      "outputs": [],
      "source": [
        "# ----------------- MODEL: 5-CH RESNET50 + CLINICAL MLP (FUSION) -----------------\n",
        "class EfficientNetB3_5ch_Clinical(nn.Module):\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        base = models.efficientnet_b3(\n",
        "            weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        # adapt Conv2dNormActivation: 3→5 channels (init extra channels with mean RGB weights)\n",
        "        old = base.features[0][0]\n",
        "        new = nn.Conv2d(\n",
        "            5,\n",
        "            old.out_channels,\n",
        "            kernel_size=old.kernel_size,\n",
        "            stride=old.stride,\n",
        "            padding=old.padding,\n",
        "            bias=(old.bias is not None),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            new.weight[:, :3, :, :] = old.weight\n",
        "            mean_w = old.weight.mean(dim=1, keepdim=True)\n",
        "            new.weight[:, 3:5, :, :] = mean_w.repeat(1, 2, 1, 1)\n",
        "            if old.bias is not None:\n",
        "                new.bias.copy_(old.bias)\n",
        "        base.features[0][0] = new\n",
        "\n",
        "        in_f = base.classifier[-1].in_features\n",
        "        base.classifier[-1] = nn.Identity()\n",
        "        self.backbone = base\n",
        "\n",
        "        self.img_head = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25)\n",
        "        )\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.10),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(512 + 64, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x5, xclin):\n",
        "        f = self.backbone(x5)  # (B, 2048)\n",
        "        f = self.img_head(f)  # (B, 512)\n",
        "        g = self.clin_head(xclin)  # (B, 64)\n",
        "        z = torch.cat([f, g], dim=1)\n",
        "        out = self.fuse(z)  # (B, 59)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:28.785280Z",
          "iopub.status.busy": "2025-11-10T06:05:28.785012Z",
          "iopub.status.idle": "2025-11-10T06:05:28.792074Z",
          "shell.execute_reply": "2025-11-10T06:05:28.791195Z",
          "shell.execute_reply.started": "2025-11-10T06:05:28.785262Z"
        },
        "id": "7QmlWT66Fbhv"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "    for x_img, x_clin, y in loader:\n",
        "        x_img = x_img.to(DEVICE)\n",
        "        x_clin = x_clin.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "        pred = model(x_img, x_clin)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x_img.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:52.866937Z",
          "iopub.status.busy": "2025-11-10T06:05:52.866250Z",
          "iopub.status.idle": "2025-11-10T06:09:44.041224Z",
          "shell.execute_reply": "2025-11-10T06:09:44.040337Z",
          "shell.execute_reply.started": "2025-11-10T06:05:52.866915Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4gsEdcPFbhv",
        "outputId": "7940dea1-2751-4b0b-a654-87625d5f626f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] 59 VF cols: ['VF1', 'VF2', 'VF3', 'VF4', 'VF5'] ... ['VF55', 'VF56', 'VF57', 'VF58', 'VF59']\n",
            "\n",
            "Epoch 01 | train_loss=484.9972  train_pMAE=20.390  train_MS=20.243 || val_loss=438.0090  val_pMAE=19.137  val_MS=18.928\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC_CDR.pth (pMAE=19.137)\n",
            "\n",
            "Epoch 02 | train_loss=306.0965  train_pMAE=15.576  train_MS=14.913 || val_loss=116.4437  val_pMAE=9.195  val_MS=7.411\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC_CDR.pth (pMAE=9.195)\n",
            "\n",
            "Epoch 03 | train_loss=70.4604  train_pMAE=6.623  train_MS=4.228 || val_loss=65.7970  val_pMAE=6.896  val_MS=5.382\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC_CDR.pth (pMAE=6.896)\n",
            "\n",
            "Epoch 04 | train_loss=49.2092  train_pMAE=5.557  train_MS=3.335 || val_loss=44.8308  val_pMAE=5.152  val_MS=3.564\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC_CDR.pth (pMAE=5.152)\n",
            "\n",
            "Epoch 05 | train_loss=48.3400  train_pMAE=5.491  train_MS=3.161 || val_loss=44.5198  val_pMAE=5.222  val_MS=3.466\n",
            "\n",
            "Epoch 06 | train_loss=44.9606  train_pMAE=5.286  train_MS=2.971 || val_loss=44.6312  val_pMAE=5.262  val_MS=3.483\n",
            "\n",
            "Epoch 07 | train_loss=43.6279  train_pMAE=5.212  train_MS=2.893 || val_loss=43.9633  val_pMAE=5.216  val_MS=3.428\n",
            "\n",
            "Epoch 08 | train_loss=41.7430  train_pMAE=5.052  train_MS=2.737 || val_loss=43.3900  val_pMAE=5.128  val_MS=3.354\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC_CDR.pth (pMAE=5.128)\n",
            "\n",
            "Epoch 09 | train_loss=39.5849  train_pMAE=4.940  train_MS=2.475 || val_loss=41.4182  val_pMAE=4.907  val_MS=3.217\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC_CDR.pth (pMAE=4.907)\n",
            "\n",
            "Epoch 10 | train_loss=38.9338  train_pMAE=4.873  train_MS=2.492 || val_loss=41.2728  val_pMAE=5.006  val_MS=3.211\n",
            "\n",
            "Epoch 11 | train_loss=38.9699  train_pMAE=4.882  train_MS=2.552 || val_loss=41.4886  val_pMAE=5.062  val_MS=3.202\n",
            "\n",
            "Epoch 12 | train_loss=37.7473  train_pMAE=4.795  train_MS=2.346 || val_loss=37.8972  val_pMAE=4.563  val_MS=2.668\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC_CDR.pth (pMAE=4.563)\n",
            "\n",
            "Epoch 13 | train_loss=37.0088  train_pMAE=4.737  train_MS=2.336 || val_loss=40.0721  val_pMAE=4.915  val_MS=3.128\n",
            "\n",
            "Epoch 14 | train_loss=36.3986  train_pMAE=4.699  train_MS=2.251 || val_loss=40.4296  val_pMAE=4.988  val_MS=3.171\n",
            "\n",
            "Epoch 15 | train_loss=38.0158  train_pMAE=4.793  train_MS=2.478 || val_loss=39.1306  val_pMAE=4.716  val_MS=2.820\n",
            "\n",
            "Epoch 16 | train_loss=35.7125  train_pMAE=4.640  train_MS=2.183 || val_loss=38.9505  val_pMAE=4.685  val_MS=2.716\n",
            "\n",
            "Epoch 17 | train_loss=35.8451  train_pMAE=4.608  train_MS=2.226 || val_loss=39.0285  val_pMAE=4.807  val_MS=2.919\n",
            "\n",
            "Epoch 18 | train_loss=35.0432  train_pMAE=4.564  train_MS=2.175 || val_loss=37.5248  val_pMAE=4.661  val_MS=2.740\n",
            "\n",
            "Epoch 19 | train_loss=34.8331  train_pMAE=4.560  train_MS=2.209 || val_loss=36.8513  val_pMAE=4.547  val_MS=2.540\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC_CDR.pth (pMAE=4.547)\n",
            "\n",
            "Epoch 20 | train_loss=34.3057  train_pMAE=4.517  train_MS=2.091 || val_loss=37.7913  val_pMAE=4.707  val_MS=2.760\n",
            "\n",
            "Epoch 21 | train_loss=33.8407  train_pMAE=4.500  train_MS=2.071 || val_loss=39.1254  val_pMAE=4.849  val_MS=3.029\n",
            "\n",
            "Epoch 22 | train_loss=33.9422  train_pMAE=4.475  train_MS=2.021 || val_loss=38.0954  val_pMAE=4.639  val_MS=2.791\n",
            "\n",
            "Epoch 23 | train_loss=33.2177  train_pMAE=4.426  train_MS=1.947 || val_loss=37.9501  val_pMAE=4.702  val_MS=2.813\n",
            "\n",
            "Epoch 24 | train_loss=33.1945  train_pMAE=4.450  train_MS=2.014 || val_loss=36.9089  val_pMAE=4.558  val_MS=2.593\n",
            "\n",
            "Epoch 25 | train_loss=33.2525  train_pMAE=4.419  train_MS=1.989 || val_loss=37.5109  val_pMAE=4.655  val_MS=2.735\n",
            "\n",
            "Epoch 26 | train_loss=31.8261  train_pMAE=4.319  train_MS=1.781 || val_loss=35.9292  val_pMAE=4.472  val_MS=2.435\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC_CDR.pth (pMAE=4.472)\n",
            "\n",
            "Epoch 27 | train_loss=32.3099  train_pMAE=4.368  train_MS=1.912 || val_loss=36.3226  val_pMAE=4.458  val_MS=2.476\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC_CDR.pth (pMAE=4.458)\n",
            "\n",
            "Epoch 28 | train_loss=32.7181  train_pMAE=4.430  train_MS=1.948 || val_loss=37.2097  val_pMAE=4.591  val_MS=2.610\n",
            "\n",
            "Epoch 29 | train_loss=32.3066  train_pMAE=4.375  train_MS=1.830 || val_loss=36.5003  val_pMAE=4.435  val_MS=2.450\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC_CDR.pth (pMAE=4.435)\n",
            "\n",
            "Epoch 30 | train_loss=32.9891  train_pMAE=4.451  train_MS=1.968 || val_loss=36.3717  val_pMAE=4.506  val_MS=2.467\n",
            "\n",
            "Epoch 31 | train_loss=32.7978  train_pMAE=4.412  train_MS=2.009 || val_loss=37.4249  val_pMAE=4.614  val_MS=2.664\n",
            "\n",
            "Epoch 32 | train_loss=32.8558  train_pMAE=4.401  train_MS=1.934 || val_loss=37.0330  val_pMAE=4.576  val_MS=2.593\n",
            "\n",
            "Epoch 33 | train_loss=31.8373  train_pMAE=4.331  train_MS=1.863 || val_loss=37.3576  val_pMAE=4.637  val_MS=2.661\n",
            "\n",
            "Epoch 34 | train_loss=32.9344  train_pMAE=4.433  train_MS=2.032 || val_loss=37.4317  val_pMAE=4.656  val_MS=2.687\n",
            "\n",
            "Epoch 35 | train_loss=32.6959  train_pMAE=4.369  train_MS=1.941 || val_loss=36.1273  val_pMAE=4.456  val_MS=2.468\n",
            "\n",
            "Epoch 36 | train_loss=32.3615  train_pMAE=4.377  train_MS=1.914 || val_loss=35.9766  val_pMAE=4.421  val_MS=2.432\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_efficientnetb3_ROI_ODOC_CDR.pth (pMAE=4.421)\n",
            "\n",
            "Epoch 37 | train_loss=30.9367  train_pMAE=4.270  train_MS=1.680 || val_loss=36.1812  val_pMAE=4.441  val_MS=2.440\n",
            "\n",
            "Epoch 38 | train_loss=32.3974  train_pMAE=4.393  train_MS=1.923 || val_loss=37.0049  val_pMAE=4.573  val_MS=2.564\n",
            "\n",
            "Epoch 39 | train_loss=32.3526  train_pMAE=4.337  train_MS=1.902 || val_loss=36.4534  val_pMAE=4.473  val_MS=2.500\n",
            "\n",
            "Epoch 40 | train_loss=32.1545  train_pMAE=4.354  train_MS=1.917 || val_loss=36.1906  val_pMAE=4.493  val_MS=2.511\n",
            "\n",
            "Epoch 41 | train_loss=31.7786  train_pMAE=4.325  train_MS=1.788 || val_loss=36.3703  val_pMAE=4.516  val_MS=2.524\n",
            "\n",
            "Epoch 42 | train_loss=31.4082  train_pMAE=4.297  train_MS=1.806 || val_loss=36.3801  val_pMAE=4.473  val_MS=2.494\n",
            "\n",
            "Epoch 43 | train_loss=31.8565  train_pMAE=4.358  train_MS=1.849 || val_loss=37.4347  val_pMAE=4.648  val_MS=2.663\n",
            "\n",
            "Epoch 44 | train_loss=31.6670  train_pMAE=4.301  train_MS=1.774 || val_loss=36.6286  val_pMAE=4.497  val_MS=2.515\n",
            "\n",
            "Epoch 45 | train_loss=31.4809  train_pMAE=4.304  train_MS=1.782 || val_loss=36.8988  val_pMAE=4.556  val_MS=2.553\n",
            "\n",
            "Epoch 46 | train_loss=31.9230  train_pMAE=4.337  train_MS=1.834 || val_loss=36.8181  val_pMAE=4.559  val_MS=2.569\n",
            "\n",
            "Epoch 47 | train_loss=30.7865  train_pMAE=4.247  train_MS=1.706 || val_loss=36.6400  val_pMAE=4.540  val_MS=2.557\n",
            "\n",
            "Early stopping at epoch 47 (best val pMAE=4.421)\n"
          ]
        }
      ],
      "source": [
        "def train_resnet50_roi_odoc_with_cdr(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01):\n",
        "    import os\n",
        "    import random\n",
        "\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    # --- Early stopper that saves best on val pMAE ---\n",
        "    class EarlyStopper:\n",
        "        def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "            self.patience = int(patience)\n",
        "            self.min_delta = float(min_delta)\n",
        "            self.ckpt_path = ckpt_path\n",
        "            self.best = float(\"inf\")\n",
        "            self.bad_epochs = 0\n",
        "\n",
        "        def step(self, val_pmae, model, epoch_meta=None):\n",
        "            if val_pmae < self.best - self.min_delta:\n",
        "                self.best = val_pmae\n",
        "                self.bad_epochs = 0\n",
        "                if self.ckpt_path:\n",
        "                    torch.save(\n",
        "                        {\n",
        "                            \"model\": model.state_dict(),\n",
        "                            \"val_pointwise_mae\": self.best,\n",
        "                            **(epoch_meta or {}),\n",
        "                        },\n",
        "                        self.ckpt_path,\n",
        "                    )\n",
        "                return False\n",
        "            else:\n",
        "                self.bad_epochs += 1\n",
        "                return self.bad_epochs > self.patience\n",
        "\n",
        "    # --- read & detect columns ---\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] {len(vf_cols)} VF cols: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # --- ADD CDR BEFORE SPLIT ---\n",
        "    rows = augment_rows_with_cdr_psd(rows, image_col, vf_cols)  # <-- adds CDR fields per row\n",
        "\n",
        "    random.shuffle(rows)\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    # --- fit clinical stats on TRAIN only (handles imputation/encoding) ---\n",
        "    clin_stats = fit_clinical_stats(train_rows, CLIN_NUM_COLS)\n",
        "    clin_dim = len(CLIN_NUM_COLS) + len(CLIN_CAT_COLS)\n",
        "\n",
        "    # --- datasets / loaders: use the dataset that returns (x5, x_clin, y) ---\n",
        "    train_ds = ROI_ODOC_Clinical_Dataset(train_rows, image_col, vf_cols, clin_stats, train=True)\n",
        "    val_ds = ROI_ODOC_Clinical_Dataset(val_rows, image_col, vf_cols, clin_stats, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # --- model that accepts 5ch image + clinical vector ---\n",
        "    model = EfficientNetB3_5ch_Clinical(clin_dim=clin_dim, out_dim=NUM_POINTS, pretrained=True).to(\n",
        "        DEVICE\n",
        "    )\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt = os.path.join(CHECK_DIR, \"best_efficientnetb3_ROI_ODOC_CDR.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)  # should read (x5, x_clin, y) inside\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        print(\n",
        "            f\"\\nEpoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # step LR on validation pMAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        # save best & decide stopping\n",
        "        improved = va[\"pointwise_mae\"] < stopper.best - MIN_DELTA\n",
        "        should_stop = stopper.step(va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch})\n",
        "        if improved:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt} (pMAE={stopper.best:.3f})\")\n",
        "\n",
        "        if should_stop:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (best val pMAE={stopper.best:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best before returning\n",
        "    state = torch.load(ckpt, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt, clin_dim\n",
        "\n",
        "\n",
        "# run training and expose globals\n",
        "model_odoc, train_dl_odoc, val_dl_odoc, image_col_odoc, vf_cols_odoc, CKPT_ODOC, CLIN_DIM_OD = (\n",
        "    train_resnet50_roi_odoc_with_cdr(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:31:38.896271Z",
          "iopub.status.busy": "2025-11-10T06:31:38.895650Z",
          "iopub.status.idle": "2025-11-10T06:31:39.930251Z",
          "shell.execute_reply": "2025-11-10T06:31:39.929374Z",
          "shell.execute_reply.started": "2025-11-10T06:31:38.896246Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ampqWTNvFbhv",
        "outputId": "52eb22d1-6287-4a5e-e8c0-54b5bae9b256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n",
            "\n",
            "== ROI+Clinical (with CDR): POINTWISE ==\n",
            "RMSE: 5.9980 | MAE: 4.4209 | R²: 0.5687\n",
            "\n",
            "== ROI+Clinical (with CDR): MEAN SENSITIVITY ==\n",
            "RMSE: 3.3592 | MAE: 2.4318 | R²: 0.6852\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------\n",
        "# ✅ Corrected Cell 10: Reload best model & evaluate\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Use the model already trained in Cell 9\n",
        "# model_odoc → best model returned by train_resnet50_roi_odoc_with_cdr\n",
        "# val_dl_odoc → validation dataloader\n",
        "# CKPT_ODOC → checkpoint path\n",
        "# CLIN_DIM_OD → clinical feature dimension\n",
        "# image_col_odoc, vf_cols_odoc already created\n",
        "\n",
        "best_roi_clin = model_odoc  # model already returned from training\n",
        "best_roi_clin.eval()\n",
        "\n",
        "# Load the best checkpoint\n",
        "state = torch.load(CKPT_ODOC, map_location=DEVICE)\n",
        "best_roi_clin.load_state_dict(state[\"model\"])\n",
        "best_roi_clin.eval()\n",
        "\n",
        "# Collect predictions\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x_img, x_clin, y in val_dl_odoc:\n",
        "        x_img = x_img.to(DEVICE)\n",
        "        x_clin = x_clin.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        p = best_roi_clin(x_img, x_clin)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)\n",
        "\n",
        "\n",
        "# --- Metrics ---\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_value(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# Pointwise metrics (VF1..VF59 flattened)\n",
        "pw_true = y_true.reshape(-1)\n",
        "pw_pred = y_pred.reshape(-1)\n",
        "\n",
        "print(\"\\n== ROI+Clinical (with CDR): POINTWISE ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(pw_true, pw_pred):.4f} | \"\n",
        "    f\"MAE: {mae_value(pw_true, pw_pred):.4f} | \"\n",
        "    f\"R²: {r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "\n",
        "# Mean Sensitivity metrics\n",
        "t_mean = y_true.mean(dim=1)\n",
        "p_mean = y_pred.mean(dim=1)\n",
        "\n",
        "print(\"\\n== ROI+Clinical (with CDR): MEAN SENSITIVITY ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | \"\n",
        "    f\"MAE: {mae_value(t_mean, p_mean):.4f} | \"\n",
        "    f\"R²: {r2(t_mean, p_mean):.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T10:08:07.655701Z",
          "iopub.status.busy": "2025-11-08T10:08:07.655425Z",
          "iopub.status.idle": "2025-11-08T10:08:07.659428Z"
        },
        "id": "4TKn3MZ1Fbhw"
      },
      "source": [
        "# 5. ROI+ OD/OC + Clinical EfficientNetB3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:33.300575Z",
          "iopub.status.busy": "2025-11-10T04:06:33.299912Z",
          "iopub.status.idle": "2025-11-10T04:06:33.432177Z",
          "shell.execute_reply": "2025-11-10T04:06:33.431611Z",
          "shell.execute_reply.started": "2025-11-10T04:06:33.300551Z"
        },
        "lines_to_next_cell": 2,
        "id": "ASed192fFbhw"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# FULL FUSION: ROI + OD/OC + Clinical   →  VF (59)\n",
        "# ==========================\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# ----------------- PATHS & CONFIG -----------------\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59  # VF1..VF59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:36.730860Z",
          "iopub.status.busy": "2025-11-10T04:06:36.730557Z",
          "iopub.status.idle": "2025-11-10T04:06:36.740736Z",
          "shell.execute_reply": "2025-11-10T04:06:36.740178Z",
          "shell.execute_reply.started": "2025-11-10T04:06:36.730834Z"
        },
        "id": "ySLUntkGFbhw"
      },
      "outputs": [],
      "source": [
        "# Clinical features for fusion (present or computed)\n",
        "CLIN_NUM_COLS = [\"AGE\", \"IOP_y\", \"CDR\"]\n",
        "CLIN_CAT_COLS = [\"GENDER\"]\n",
        "IMAGE_COLS_CANDIDATES = [\n",
        "    \"Corresponding CFP\",\n",
        "    \"image\",\n",
        "    \"image_name\",\n",
        "    \"img\",\n",
        "    \"image_path\",\n",
        "    \"filename\",\n",
        "    \"file\",\n",
        "]\n",
        "\n",
        "\n",
        "# ----------------- CSV UTILS -----------------\n",
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]) -> Tuple[str, List[str]]:\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # image column (prefer Corresponding CFP)\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # prefer explicit VF1..VF59\n",
        "    vf_pref = [f\"VF{i}\" for i in range(1, 60)]\n",
        "    if all(c in cols for c in vf_pref):\n",
        "        return image_col, vf_pref\n",
        "\n",
        "    # fallback: detect numeric columns (exclude clinical & image & VF0/VF60)\n",
        "    excluded = set([image_col] + CLIN_NUM_COLS + CLIN_CAT_COLS + [\"VF0\", \"VF60\"])\n",
        "    cand = []\n",
        "    for c in cols:\n",
        "        if c in excluded:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            cand.append(c)\n",
        "    if len(cand) < NUM_POINTS:\n",
        "        raise ValueError(f\"Not enough numeric VF columns; found {len(cand)}, need {NUM_POINTS}.\")\n",
        "\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    cand = sorted(cand, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, cand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:40.269978Z",
          "iopub.status.busy": "2025-11-10T04:06:40.269252Z",
          "iopub.status.idle": "2025-11-10T04:06:40.280558Z",
          "shell.execute_reply": "2025-11-10T04:06:40.279748Z",
          "shell.execute_reply.started": "2025-11-10T04:06:40.269944Z"
        },
        "id": "mTGdvr5gFbhw"
      },
      "outputs": [],
      "source": [
        "# ----------------- OD/OC POLYGONS → CDR & MASKS -----------------\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _read_labelme_polys(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        lab = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = sh.get(\"points\", [])\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if lab in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        if lab in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "\n",
        "    # keep polygon with max vertical height\n",
        "    def vheight(poly):\n",
        "        ys = [p[1] for p in poly]\n",
        "        return (max(ys) - min(ys)) if ys else 0.0\n",
        "\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=vheight)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=vheight)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        p = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def compute_cdr_from_json(img_name: str):\n",
        "    \"\"\"CDR = vertical cup height / vertical disc height (from polygons).\"\"\"\n",
        "    jpath = _guess_json(img_name)\n",
        "    if not jpath:\n",
        "        return None\n",
        "    try:\n",
        "        od_polys, oc_polys = _read_labelme_polys(jpath)\n",
        "        if not od_polys or not oc_polys:\n",
        "            return None\n",
        "\n",
        "        def vheight(poly):\n",
        "            ys = [float(y) for _, y in poly]\n",
        "            return max(ys) - min(ys) if ys else 0.0\n",
        "\n",
        "        h_od = vheight(od_polys[0])\n",
        "        h_oc = vheight(oc_polys[0])\n",
        "        if h_od <= 0:\n",
        "            return None\n",
        "        return float(h_oc / h_od)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] CDR parse failed for {img_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def build_masks_from_labelme(img_pil: Image.Image, img_name: str, out_size: int):\n",
        "    \"\"\"Binary OD/OC masks (L mode), resized to out_size.\"\"\"\n",
        "    W, H = img_pil.size\n",
        "    od_mask = Image.new(\"L\", (W, H), 0)\n",
        "    oc_mask = Image.new(\"L\", (W, H), 0)\n",
        "    jpath = _guess_json(img_name)\n",
        "    if jpath:\n",
        "        try:\n",
        "            od_polys, oc_polys = _read_labelme_polys(jpath)\n",
        "            d_od = ImageDraw.Draw(od_mask)\n",
        "            d_oc = ImageDraw.Draw(oc_mask)\n",
        "            for poly in od_polys:\n",
        "                d_od.polygon(poly, outline=1, fill=1)\n",
        "            for poly in oc_polys:\n",
        "                d_oc.polygon(poly, outline=1, fill=1)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] mask parse {jpath}: {e}\")\n",
        "    od_mask = od_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    oc_mask = oc_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    return od_mask, oc_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:43.829520Z",
          "iopub.status.busy": "2025-11-10T04:06:43.828915Z",
          "iopub.status.idle": "2025-11-10T04:06:43.833579Z",
          "shell.execute_reply": "2025-11-10T04:06:43.832931Z",
          "shell.execute_reply.started": "2025-11-10T04:06:43.829498Z"
        },
        "id": "Nb9dhiNGFbhx"
      },
      "outputs": [],
      "source": [
        "# ----------------- AUGMENT ROWS WITH CDR  -----------------\n",
        "def augment_rows_with_cdr_psd(rows, image_col, vf_cols):\n",
        "    augmented = []\n",
        "    miss_cdr = miss_psd = 0\n",
        "    for r in rows:\n",
        "        r2 = dict(r)\n",
        "        cdr = compute_cdr_from_json(r2[image_col])\n",
        "        if cdr is None:\n",
        "            miss_cdr += 1\n",
        "        r2[\"CDR\"] = cdr\n",
        "\n",
        "        augmented.append(r2)\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:47.264094Z",
          "iopub.status.busy": "2025-11-10T04:06:47.263774Z",
          "iopub.status.idle": "2025-11-10T04:06:47.271273Z",
          "shell.execute_reply": "2025-11-10T04:06:47.270696Z",
          "shell.execute_reply.started": "2025-11-10T04:06:47.264074Z"
        },
        "id": "xjDMsGO6Fbhx"
      },
      "outputs": [],
      "source": [
        "# ----------------- CLINICAL PREPROCESS -----------------\n",
        "def to_float(x):\n",
        "    x = str(x).strip()\n",
        "    if x == \"\":\n",
        "        return None\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def fit_clinical_stats(rows, clin_num_cols):\n",
        "    stats = {}\n",
        "    for c in clin_num_cols:\n",
        "        vals = [to_float(r.get(c, \"\")) for r in rows]\n",
        "        vals = [v for v in vals if v is not None]\n",
        "        mean = np.mean(vals) if vals else 0.0\n",
        "        std = np.std(vals) if vals else 1.0\n",
        "        if std == 0:\n",
        "            std = 1.0\n",
        "        stats[c] = (float(mean), float(std))\n",
        "    return stats\n",
        "\n",
        "\n",
        "def encode_gender(x):\n",
        "    s = str(x).strip().lower()\n",
        "    if s in (\"m\", \"male\", \"man\"):\n",
        "        return 1.0\n",
        "    if s in (\"f\", \"female\", \"woman\"):\n",
        "        return 0.0\n",
        "    return 0.5  # unknown/other\n",
        "\n",
        "\n",
        "def build_clinical_vector(r, stats):\n",
        "    vec = []\n",
        "    for c in CLIN_NUM_COLS:\n",
        "        v = to_float(r.get(c, \"\"))\n",
        "        mean, std = stats[c]\n",
        "        v = mean if v is None else v\n",
        "        v = (v - mean) / std\n",
        "        vec.append(v)\n",
        "    for c in CLIN_CAT_COLS:\n",
        "        if c == \"GENDER\":\n",
        "            vec.append(encode_gender(r.get(c, \"\")))\n",
        "        else:\n",
        "            vec.append(0.0)\n",
        "    return torch.tensor(vec, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:50.779769Z",
          "iopub.status.busy": "2025-11-10T04:06:50.779086Z",
          "iopub.status.idle": "2025-11-10T04:06:50.787036Z",
          "shell.execute_reply": "2025-11-10T04:06:50.786326Z",
          "shell.execute_reply.started": "2025-11-10T04:06:50.779743Z"
        },
        "id": "xeIsa0ViFbhx"
      },
      "outputs": [],
      "source": [
        "# ----------------- DATASET: 5-CH ROI + CLINICAL -----------------\n",
        "class ROI_ODOC_Clinical_Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows, image_col, vf_cols, clin_stats, train=True, img_root=ROI_DIR, img_size=IMG_SIZE\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.clin_stats = clin_stats\n",
        "        self.train = train\n",
        "        self.img_root = img_root\n",
        "        self.img_size = img_size\n",
        "\n",
        "        norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.rgb_tf = transforms.Compose(\n",
        "            [transforms.Resize((img_size, img_size)), transforms.ToTensor(), *aug, norm]\n",
        "        )\n",
        "        self.mask_tf = transforms.ToTensor()  # L→(1,H,W) float {0,1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.rows[idx]\n",
        "        fn = r[self.image_col]\n",
        "        path = fn if os.path.isabs(fn) else os.path.join(self.img_root, fn)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        od_img, oc_img = build_masks_from_labelme(img, fn, self.img_size)\n",
        "\n",
        "        x_rgb = self.rgb_tf(img)  # (3,H,W)\n",
        "        x_od = self.mask_tf(od_img)  # (1,H,W)\n",
        "        x_oc = self.mask_tf(oc_img)  # (1,H,W)\n",
        "        x5 = torch.cat([x_rgb, x_od, x_oc], dim=0)  # (5,H,W)\n",
        "\n",
        "        x_clin = build_clinical_vector(r, self.clin_stats)  # (clin_dim,)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)  # (59,)\n",
        "\n",
        "        return x5, x_clin, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:54.064124Z",
          "iopub.status.busy": "2025-11-10T04:06:54.063332Z",
          "iopub.status.idle": "2025-11-10T04:06:54.071929Z",
          "shell.execute_reply": "2025-11-10T04:06:54.071421Z",
          "shell.execute_reply.started": "2025-11-10T04:06:54.064093Z"
        },
        "id": "Mly7-3OwFbhx"
      },
      "outputs": [],
      "source": [
        "# ----------------- MODEL: 5-CH RESNET50 + CLINICAL MLP (FUSION) -----------------\n",
        "class EfficientNetB3_5ch_Clinical(nn.Module):\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        base = models.efficientnet_b3(\n",
        "            weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        # adapt Conv2dNormActivation: 3→5 channels (init extra channels with mean RGB weights)\n",
        "        old = base.features[0][0]\n",
        "        new = nn.Conv2d(\n",
        "            5,\n",
        "            old.out_channels,\n",
        "            kernel_size=old.kernel_size,\n",
        "            stride=old.stride,\n",
        "            padding=old.padding,\n",
        "            bias=(old.bias is not None),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            new.weight[:, :3, :, :] = old.weight\n",
        "            mean_w = old.weight.mean(dim=1, keepdim=True)\n",
        "            new.weight[:, 3:5, :, :] = mean_w.repeat(1, 2, 1, 1)\n",
        "            if old.bias is not None:\n",
        "                new.bias.copy_(old.bias)\n",
        "        base.features[0][0] = new\n",
        "\n",
        "        in_f = base.classifier[-1].in_features\n",
        "        base.classifier[-1] = nn.Identity()\n",
        "        self.backbone = base\n",
        "\n",
        "        self.img_head = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25)\n",
        "        )\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.10),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(512 + 64, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x5, xclin):\n",
        "        f = self.backbone(x5)  # (B, 2048)\n",
        "        f = self.img_head(f)  # (B, 512)\n",
        "        g = self.clin_head(xclin)  # (B, 64)\n",
        "        z = torch.cat([f, g], dim=1)\n",
        "        out = self.fuse(z)  # (B, 59)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:57.415483Z",
          "iopub.status.busy": "2025-11-10T04:06:57.414915Z",
          "iopub.status.idle": "2025-11-10T04:06:57.422155Z",
          "shell.execute_reply": "2025-11-10T04:06:57.421329Z",
          "shell.execute_reply.started": "2025-11-10T04:06:57.415462Z"
        },
        "id": "pQSYuqZoFbhy"
      },
      "outputs": [],
      "source": [
        "# ----------------- METRICS & EPOCH LOOP (same logic as your earlier code) -----------------\n",
        "@torch.no_grad()\n",
        "def mae(pred, true):  # pointwise MAE\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):  # MS per-sample then MAE\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "    for x5, xclin, y in loader:\n",
        "        x5, xclin, y = x5.to(DEVICE), xclin.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "        pred = model(x5, xclin)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x5.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:08:38.414507Z",
          "iopub.status.busy": "2025-11-10T04:08:38.414201Z",
          "iopub.status.idle": "2025-11-10T04:17:55.812102Z",
          "shell.execute_reply": "2025-11-10T04:17:55.810937Z",
          "shell.execute_reply.started": "2025-11-10T04:08:38.414480Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfCk7ylqFbhy",
        "outputId": "4e7e8394-0712-4041-81a2-67aad109d4b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] 59 VF cols detected: ['VF1', 'VF2', 'VF3', 'VF4', 'VF5'] ... ['VF55', 'VF56', 'VF57', 'VF58', 'VF59']\n",
            "Epoch 01 | train_loss=484.9248 train_pMAE=20.389 train_MS=20.242 || val_loss=436.5099 val_pMAE=19.099 val_MS=18.889\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=19.099)\n",
            "Epoch 02 | train_loss=302.3349 train_pMAE=15.459 train_MS=14.835 || val_loss=127.7193 val_pMAE=9.720 val_MS=8.172\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=9.720)\n",
            "Epoch 03 | train_loss=69.6885 train_pMAE=6.594 train_MS=4.123 || val_loss=66.6311 val_pMAE=6.936 val_MS=5.323\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=6.936)\n",
            "Epoch 04 | train_loss=49.8389 train_pMAE=5.622 train_MS=3.410 || val_loss=45.8848 val_pMAE=5.309 val_MS=3.705\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=5.309)\n",
            "Epoch 05 | train_loss=48.1899 train_pMAE=5.469 train_MS=3.226 || val_loss=48.9570 val_pMAE=5.603 val_MS=4.024\n",
            "Epoch 06 | train_loss=45.5800 train_pMAE=5.335 train_MS=3.095 || val_loss=47.3520 val_pMAE=5.500 val_MS=3.696\n",
            "Epoch 07 | train_loss=41.9274 train_pMAE=5.080 train_MS=2.741 || val_loss=42.1688 val_pMAE=5.004 val_MS=3.236\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=5.004)\n",
            "Epoch 08 | train_loss=41.3651 train_pMAE=5.047 train_MS=2.708 || val_loss=42.5906 val_pMAE=5.069 val_MS=3.297\n",
            "Epoch 09 | train_loss=40.5340 train_pMAE=4.977 train_MS=2.615 || val_loss=42.5916 val_pMAE=5.052 val_MS=3.145\n",
            "Epoch 10 | train_loss=38.3436 train_pMAE=4.849 train_MS=2.464 || val_loss=44.7673 val_pMAE=5.221 val_MS=3.497\n",
            "Epoch 11 | train_loss=38.1715 train_pMAE=4.807 train_MS=2.368 || val_loss=44.4578 val_pMAE=5.309 val_MS=3.629\n",
            "Epoch 12 | train_loss=37.4761 train_pMAE=4.770 train_MS=2.317 || val_loss=40.1752 val_pMAE=4.775 val_MS=2.953\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.775)\n",
            "Epoch 13 | train_loss=37.6067 train_pMAE=4.741 train_MS=2.343 || val_loss=39.8129 val_pMAE=4.679 val_MS=2.850\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.679)\n",
            "Epoch 14 | train_loss=36.2248 train_pMAE=4.707 train_MS=2.267 || val_loss=38.8762 val_pMAE=4.671 val_MS=2.772\n",
            "Epoch 15 | train_loss=36.9458 train_pMAE=4.722 train_MS=2.361 || val_loss=40.0182 val_pMAE=4.850 val_MS=3.030\n",
            "Epoch 16 | train_loss=36.5709 train_pMAE=4.709 train_MS=2.347 || val_loss=39.2648 val_pMAE=4.763 val_MS=2.851\n",
            "Epoch 17 | train_loss=36.6814 train_pMAE=4.672 train_MS=2.221 || val_loss=40.4404 val_pMAE=4.943 val_MS=3.164\n",
            "Epoch 18 | train_loss=35.6618 train_pMAE=4.646 train_MS=2.225 || val_loss=39.9498 val_pMAE=4.886 val_MS=3.043\n",
            "Epoch 19 | train_loss=34.4675 train_pMAE=4.540 train_MS=2.021 || val_loss=38.2548 val_pMAE=4.646 val_MS=2.729\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.646)\n",
            "Epoch 20 | train_loss=35.3617 train_pMAE=4.604 train_MS=2.229 || val_loss=38.9020 val_pMAE=4.761 val_MS=2.890\n",
            "Epoch 21 | train_loss=35.6507 train_pMAE=4.624 train_MS=2.232 || val_loss=39.5254 val_pMAE=4.766 val_MS=2.962\n",
            "Epoch 22 | train_loss=33.9673 train_pMAE=4.485 train_MS=1.957 || val_loss=38.5129 val_pMAE=4.655 val_MS=2.841\n",
            "Epoch 23 | train_loss=34.8622 train_pMAE=4.564 train_MS=2.112 || val_loss=38.6611 val_pMAE=4.676 val_MS=2.876\n",
            "Epoch 24 | train_loss=33.6239 train_pMAE=4.463 train_MS=2.008 || val_loss=38.7208 val_pMAE=4.608 val_MS=2.829\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.608)\n",
            "Epoch 25 | train_loss=34.4971 train_pMAE=4.520 train_MS=2.071 || val_loss=38.0396 val_pMAE=4.630 val_MS=2.775\n",
            "Epoch 26 | train_loss=35.7635 train_pMAE=4.647 train_MS=2.312 || val_loss=38.4574 val_pMAE=4.679 val_MS=2.826\n",
            "Epoch 27 | train_loss=33.7189 train_pMAE=4.471 train_MS=1.955 || val_loss=38.0471 val_pMAE=4.636 val_MS=2.747\n",
            "Epoch 28 | train_loss=34.3488 train_pMAE=4.535 train_MS=2.077 || val_loss=38.3251 val_pMAE=4.631 val_MS=2.752\n",
            "Epoch 29 | train_loss=34.3455 train_pMAE=4.510 train_MS=2.018 || val_loss=38.0016 val_pMAE=4.587 val_MS=2.721\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.587)\n",
            "Epoch 30 | train_loss=33.0706 train_pMAE=4.430 train_MS=1.880 || val_loss=38.2705 val_pMAE=4.653 val_MS=2.799\n",
            "Epoch 31 | train_loss=33.6237 train_pMAE=4.432 train_MS=1.986 || val_loss=38.8833 val_pMAE=4.702 val_MS=2.863\n",
            "Epoch 32 | train_loss=33.7301 train_pMAE=4.496 train_MS=2.051 || val_loss=38.1348 val_pMAE=4.591 val_MS=2.751\n",
            "Epoch 33 | train_loss=33.6387 train_pMAE=4.455 train_MS=1.962 || val_loss=38.3373 val_pMAE=4.636 val_MS=2.782\n",
            "Epoch 34 | train_loss=34.5238 train_pMAE=4.526 train_MS=2.152 || val_loss=38.4529 val_pMAE=4.657 val_MS=2.825\n",
            "Epoch 35 | train_loss=34.2446 train_pMAE=4.511 train_MS=2.098 || val_loss=38.4410 val_pMAE=4.683 val_MS=2.853\n",
            "Epoch 36 | train_loss=32.8698 train_pMAE=4.398 train_MS=1.844 || val_loss=37.9205 val_pMAE=4.533 val_MS=2.676\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.533)\n",
            "Epoch 37 | train_loss=33.3356 train_pMAE=4.423 train_MS=1.966 || val_loss=39.6090 val_pMAE=4.806 val_MS=3.009\n",
            "Epoch 38 | train_loss=33.4841 train_pMAE=4.460 train_MS=1.983 || val_loss=38.3616 val_pMAE=4.654 val_MS=2.806\n",
            "Epoch 39 | train_loss=34.6042 train_pMAE=4.553 train_MS=2.152 || val_loss=38.5116 val_pMAE=4.689 val_MS=2.839\n",
            "Epoch 40 | train_loss=33.0548 train_pMAE=4.438 train_MS=1.923 || val_loss=38.3339 val_pMAE=4.626 val_MS=2.766\n",
            "Epoch 41 | train_loss=33.9159 train_pMAE=4.486 train_MS=2.034 || val_loss=38.1629 val_pMAE=4.600 val_MS=2.744\n",
            "Epoch 42 | train_loss=33.1942 train_pMAE=4.431 train_MS=1.915 || val_loss=37.8162 val_pMAE=4.602 val_MS=2.739\n",
            "Epoch 43 | train_loss=33.7319 train_pMAE=4.505 train_MS=2.044 || val_loss=38.6697 val_pMAE=4.692 val_MS=2.854\n",
            "Epoch 44 | train_loss=33.5680 train_pMAE=4.468 train_MS=1.950 || val_loss=38.5779 val_pMAE=4.685 val_MS=2.832\n",
            "Epoch 45 | train_loss=33.1235 train_pMAE=4.446 train_MS=1.883 || val_loss=38.3613 val_pMAE=4.701 val_MS=2.854\n",
            "Epoch 46 | train_loss=33.1822 train_pMAE=4.438 train_MS=1.859 || val_loss=37.9442 val_pMAE=4.607 val_MS=2.756\n",
            "Epoch 47 | train_loss=33.1747 train_pMAE=4.399 train_MS=1.932 || val_loss=38.3261 val_pMAE=4.674 val_MS=2.846\n",
            "Early stopping at epoch 47 (best val pMAE=4.533)\n"
          ]
        }
      ],
      "source": [
        "# ----------------- TRAIN -----------------\n",
        "def train_full_fusion(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01):\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] {len(vf_cols)} VF cols detected: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # augment with CDR BEFORE splitting\n",
        "    rows = augment_rows_with_cdr_psd(rows, image_col, vf_cols)\n",
        "\n",
        "    import random\n",
        "\n",
        "    random.shuffle(rows)\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    # fit clinical stats on train (handles None via mean imputation)\n",
        "    clin_stats = fit_clinical_stats(train_rows, CLIN_NUM_COLS)\n",
        "    clin_dim = len(CLIN_NUM_COLS) + len(CLIN_CAT_COLS)\n",
        "\n",
        "    # datasets / loaders\n",
        "    train_ds = ROI_ODOC_Clinical_Dataset(train_rows, image_col, vf_cols, clin_stats, train=True)\n",
        "    val_ds = ROI_ODOC_Clinical_Dataset(val_rows, image_col, vf_cols, clin_stats, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # model / optimizer\n",
        "    model = EfficientNetB3_5ch_Clinical(clin_dim=clin_dim, out_dim=NUM_POINTS, pretrained=True).to(\n",
        "        DEVICE\n",
        "    )\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    best = float(\"inf\")\n",
        "    no_improve = 0\n",
        "    ckpt = os.path.join(CHECK_DIR, \"best_full_fusion_ROI_ODOC_CLIN1.pth\")\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f} train_pMAE={tr['pointwise_mae']:.3f} train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f} val_pMAE={va['pointwise_mae']:.3f} val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # early stopping on val pointwise MAE\n",
        "        if va[\"pointwise_mae\"] < best - MIN_DELTA:\n",
        "            best = va[\"pointwise_mae\"]\n",
        "            no_improve = 0\n",
        "            torch.save(\n",
        "                {\"epoch\": epoch, \"model\": model.state_dict(), \"val_pointwise_mae\": best}, ckpt\n",
        "            )\n",
        "            print(f\"  -> saved new best to {ckpt} (val pMAE={best:.3f})\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve > PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch} (best val pMAE={best:.3f})\")\n",
        "                break\n",
        "\n",
        "    # expose objects / paths like before\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt, clin_dim\n",
        "\n",
        "\n",
        "# run training and expose globals\n",
        "model_full, train_dl_full, val_dl_full, image_col_full, vf_cols_full, CKPT_FULL, CLIN_DIM = (\n",
        "    train_full_fusion(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:18:02.710520Z",
          "iopub.status.busy": "2025-11-10T04:18:02.709945Z",
          "iopub.status.idle": "2025-11-10T04:18:04.452619Z",
          "shell.execute_reply": "2025-11-10T04:18:04.451838Z",
          "shell.execute_reply.started": "2025-11-10T04:18:02.710498Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Utg2LddXFbhy",
        "outputId": "56ec0577-00cd-45d7-f1fa-1279bc94beae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n",
            "\n",
            "== FULL FUSION (ROI + OD/OC + Clinical ): POINTWISE ==\n",
            "RMSE: 6.1580 | MAE: 4.5332 | R²: 0.5454\n",
            "== FULL FUSION: POINTWISE-MEAN / MS ==\n",
            "RMSE: 3.6238 | MAE: 2.6760 | R²: 0.6336\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -------EVALUATE BEST + PRINT METRICS -----------------\n",
        "# reload best\n",
        "best_full = EfficientNetB3_5ch_Clinical(\n",
        "    clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False\n",
        ").to(DEVICE)\n",
        "state = torch.load(CKPT_FULL, map_location=DEVICE)\n",
        "best_full.load_state_dict(state[\"model\"])\n",
        "best_full.eval()\n",
        "\n",
        "# collect preds\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x5, xclin, y in val_dl_full:\n",
        "        x5, xclin, y = x5.to(DEVICE), xclin.to(DEVICE), y.to(DEVICE)\n",
        "        p = best_full(x5, xclin)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)\n",
        "\n",
        "\n",
        "# paper-style metrics\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_value(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "print(\"\\n== FULL FUSION (ROI + OD/OC + Clinical ): POINTWISE ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(pw_true, pw_pred):.4f} | MAE: {mae_value(pw_true, pw_pred):.4f} | R²: {r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "\n",
        "t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "print(\"== FULL FUSION: POINTWISE-MEAN / MS ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | MAE: {mae_value(t_mean, p_mean):.4f} | R²: {r2(t_mean, p_mean):.4f}\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLbzqEfPFbhy"
      },
      "source": [
        "# 6. ROI + OD/OC + Clinical SWIN-T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:20:07.695024Z",
          "iopub.status.busy": "2025-11-10T04:20:07.694736Z",
          "iopub.status.idle": "2025-11-10T04:20:07.713069Z",
          "shell.execute_reply": "2025-11-10T04:20:07.712345Z",
          "shell.execute_reply.started": "2025-11-10T04:20:07.695004Z"
        },
        "lines_to_next_cell": 2,
        "id": "PZ7yc3fqFbhy"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# SWIN-T for FULL FUSION (ROI + OD/OC + Clinical) → VF (59)\n",
        "# Reuses train_dl_full, val_dl_full, and CLIN_DIM from your existing setup\n",
        "# ==========================\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "# ---- metrics used inside the loop\n",
        "@torch.no_grad()\n",
        "def _mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def _run_epoch_ff(model, loader, device, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "\n",
        "    for batch in loader:\n",
        "        # x5: (B,5,H,W) ; x_clin: (B, CLIN_DIM) ; y: (B,59)\n",
        "        x5, x_clin, y = batch\n",
        "        x5 = x5.to(device, non_blocking=True)\n",
        "        x_clin = x_clin.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "        pred = model(x5, x_clin)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x5.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += _mae(pred, y) * bs\n",
        "        msmae_sum += _ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss_sum / max(1, n),\n",
        "        \"pointwise_mae\": pmae_sum / max(1, n),\n",
        "        \"ms_mae\": msmae_sum / max(1, n),\n",
        "    }\n",
        "\n",
        "\n",
        "# ---- Model: Swin-T (RGB) + small CNN for OD/OC masks + clinical MLP → fused regressor\n",
        "class SwinT_5ch_Clinical(nn.Module):\n",
        "    \"\"\"\n",
        "    Uses Swin-T on RGB (x5[:, :3]),\n",
        "    Encodes OD/OC masks (x5[:, 3:5]) with a lightweight CNN,\n",
        "    Encodes clinical features with an MLP,\n",
        "    Concats [swin_feat, mask_feat, clin_feat] → MLP → 59-D VF regression.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True, dropout=0.25):\n",
        "        super().__init__()\n",
        "        # Swin-T backbone (ImageNet weights) → (B, feat_dim)\n",
        "        self.backbone = models.swin_t(\n",
        "            weights=models.Swin_T_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        feat_dim = self.backbone.head.in_features\n",
        "        self.backbone.head = nn.Identity()  # keep pooled features\n",
        "\n",
        "        # OD/OC mask encoder (2xHxW → vector)\n",
        "        self.mask_enc = nn.Sequential(\n",
        "            nn.Conv2d(2, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),  # (B,128)\n",
        "        )\n",
        "        mask_dim = 128\n",
        "\n",
        "        # clinical encoder\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        clin_out = 64\n",
        "\n",
        "        # fusion & regressor\n",
        "        fused_in = feat_dim + mask_dim + clin_out\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(fused_in, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x5, xclin):\n",
        "        x_rgb = x5[:, :3, :, :]\n",
        "        x_msk = x5[:, 3:, :, :]\n",
        "        f_rgb = self.backbone(x_rgb)\n",
        "        f_msk = self.mask_enc(x_msk)\n",
        "        f_cln = self.clin_head(xclin)\n",
        "        z = torch.cat([f_rgb, f_msk, f_cln], dim=1)\n",
        "        return self.regressor(z)\n",
        "\n",
        "\n",
        "# ---- Early Stopping helper\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = patience\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best = float(\"inf\")\n",
        "        self.bad_epochs = 0\n",
        "\n",
        "    def step(self, current, model, epoch_meta=None):\n",
        "        if current < self.best - self.min_delta:\n",
        "            self.best = current\n",
        "            self.bad_epochs = 0\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            return False  # do not stop\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "            return self.bad_epochs > self.patience  # stop if exceeded\n",
        "\n",
        "\n",
        "# ---- Train\n",
        "def train_full_fusion_swinT(\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    clin_dim,\n",
        "    out_dim=59,\n",
        "    epochs=80,\n",
        "    lr=1e-4,\n",
        "    wd=1e-4,\n",
        "    device=\"cuda\",\n",
        "    pretrained=True,\n",
        "    patience=10,\n",
        "    min_delta=0.01,\n",
        "    ckpt_dir=\"./checkpoints\",\n",
        "):\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    ckpt = os.path.join(ckpt_dir, \"best_full_fusion_swint.pth\")\n",
        "\n",
        "    model = SwinT_5ch_Clinical(clin_dim=clin_dim, out_dim=out_dim, pretrained=pretrained).to(\n",
        "        device\n",
        "    )\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    # reduce LR when val pMAE plateaus\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    stopper = EarlyStopper(patience=patience, min_delta=min_delta, ckpt_path=ckpt)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        tr = _run_epoch_ff(model, train_loader, device, opt=opt)\n",
        "        va = _run_epoch_ff(model, val_loader, device, opt=None)\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f} train_pMAE={tr['pointwise_mae']:.3f} train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f} val_pMAE={va['pointwise_mae']:.3f} val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        should_stop = stopper.step(va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch})\n",
        "        if should_stop:\n",
        "            print(f\"Early stopping at epoch {epoch} (best val pMAE={stopper.best:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best\n",
        "    state = torch.load(ckpt, map_location=device)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:22:55.770223Z",
          "iopub.status.busy": "2025-11-10T04:22:55.769445Z",
          "iopub.status.idle": "2025-11-10T04:35:02.461704Z",
          "shell.execute_reply": "2025-11-10T04:35:02.461076Z",
          "shell.execute_reply.started": "2025-11-10T04:22:55.770197Z"
        },
        "lines_to_next_cell": 2,
        "id": "evX_QdHVFbhz"
      },
      "outputs": [],
      "source": [
        "# # ===== RUN: SWIN-T FULL-FUSION TRAIN + EVAL =====\n",
        "# from math import sqrt\n",
        "# import os\n",
        "\n",
        "# import torch\n",
        "\n",
        "# # ---- config (tweak if you like)\n",
        "# EPOCHS = 80\n",
        "# LR = 1e-4\n",
        "# WD = 1e-4\n",
        "# PATIENCE = 10\n",
        "# MIN_DELTA = 0.01\n",
        "# CHECK_DIR = \"./checkpoints\"\n",
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "# # ---- quick checks\n",
        "# assert \"train_dl_full\" in globals() and \"val_dl_full\" in globals(), (\n",
        "#     \"Missing loaders. Make sure you created train_dl_full and val_dl_full.\"\n",
        "# )\n",
        "# assert \"CLIN_DIM\" in globals(), \"Missing CLIN_DIM.\"\n",
        "# OUT_DIM = 59 if \"NUM_POINTS\" not in globals() else int(NUM_POINTS)\n",
        "\n",
        "# # ---- train\n",
        "# swinT_model, SWINT_CKPT = train_full_fusion_swinT(\n",
        "#     train_loader=train_dl_full,\n",
        "#     val_loader=val_dl_full,\n",
        "#     clin_dim=CLIN_DIM,\n",
        "#     out_dim=OUT_DIM,\n",
        "#     epochs=EPOCHS,\n",
        "#     lr=LR,\n",
        "#     wd=WD,\n",
        "#     device=DEVICE,\n",
        "#     patience=PATIENCE,\n",
        "#     min_delta=MIN_DELTA,\n",
        "#     ckpt_dir=CHECK_DIR,\n",
        "#     pretrained=True,\n",
        "# )\n",
        "\n",
        "# print(f\"\\n[OK] Training finished. Best checkpoint: {SWINT_CKPT}\")\n",
        "\n",
        "\n",
        "# # ---- evaluation helpers (pointwise + mean-of-points/“MS”)\n",
        "# @torch.no_grad()\n",
        "# def _rmse(a, b):\n",
        "#     return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def _mae_scalar(a, b):\n",
        "#     return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def _r2(a, b):\n",
        "#     ss_res = torch.sum((a - b) ** 2)\n",
        "#     ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "#     return float(1.0 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# # ---- load best and evaluate on val\n",
        "# best_swinT = SwinT_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=OUT_DIM, pretrained=False).to(DEVICE)\n",
        "# state = torch.load(SWINT_CKPT, map_location=DEVICE)\n",
        "# best_swinT.load_state_dict(state[\"model\"])\n",
        "# best_swinT.eval()\n",
        "\n",
        "# y_true, y_pred = [], []\n",
        "# with torch.no_grad():\n",
        "#     for x5, xclin, y in val_dl_full:\n",
        "#         x5 = x5.to(DEVICE)\n",
        "#         xclin = xclin.to(DEVICE)\n",
        "#         y = y.to(DEVICE)\n",
        "#         p = best_swinT(x5, xclin)\n",
        "#         y_true.append(y.cpu())\n",
        "#         y_pred.append(p.cpu())\n",
        "\n",
        "# y_true = torch.cat(y_true, dim=0)\n",
        "# y_pred = torch.cat(y_pred, dim=0)\n",
        "\n",
        "# # pointwise metrics\n",
        "# pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "# print(\"\\n== SWIN-T FULL FUSION: POINTWISE ==\")\n",
        "# print(\n",
        "#     f\"RMSE: {_rmse(pw_true, pw_pred):.4f} | MAE: {_mae_scalar(pw_true, pw_pred):.4f} | R²: {_r2(pw_true, pw_pred):.4f}\"\n",
        "# )\n",
        "\n",
        "# # mean-of-points (“MS”) metrics\n",
        "# t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "# print(\"== SWIN-T FULL FUSION: MEAN (MS) ==\")\n",
        "# print(\n",
        "#     f\"RMSE: {_rmse(t_mean, p_mean):.4f} | MAE: {_mae_scalar(t_mean, p_mean):.4f} | R²: {_r2(t_mean, p_mean):.4f}\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T11:59:52.375643Z",
          "iopub.status.busy": "2025-11-08T11:59:52.375380Z",
          "iopub.status.idle": "2025-11-08T11:59:52.379449Z",
          "shell.execute_reply": "2025-11-08T11:59:52.378654Z",
          "shell.execute_reply.started": "2025-11-08T11:59:52.375625Z"
        },
        "id": "3YE2prohFbhz"
      },
      "source": [
        "# 7. Weighted Averaging Ensemble Technique (EfficientNetB3 + SWIN-T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:36:47.133505Z",
          "iopub.status.busy": "2025-11-10T04:36:47.132776Z",
          "iopub.status.idle": "2025-11-10T04:36:50.037569Z",
          "shell.execute_reply": "2025-11-10T04:36:50.036982Z",
          "shell.execute_reply.started": "2025-11-10T04:36:47.133480Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z6BoYDxFbh0",
        "outputId": "c21e886d-beec-4ced-85e2-7e252974b65a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== INDIVIDUAL MODELS (pointwise) ==\n",
            "EfficientNet  → RMSE: 6.1580 | MAE: 4.5332 | R²: 0.5454\n",
            "Swin-B  → RMSE: 5.8988 | MAE: 4.1052 | R²: 0.5829\n",
            "\n",
            "== ENSEMBLE (pointwise) ==\n",
            "Avg (α=0.50) → RMSE: 5.8186 | MAE: 4.1577 | R²: 0.5941\n",
            "\n",
            "== INDIVIDUAL MODELS (MS) ==\n",
            "EfficientNet  → RMSE: 3.6238 | MAE: 2.6760 | R²: 0.6336\n",
            "Swin-B  → RMSE: 3.4719 | MAE: 2.4142 | R²: 0.6637\n",
            "\n",
            "== ENSEMBLE (MS) ==\n",
            "Avg (α=0.50) → RMSE: 3.2852 | MAE: 2.2601 | R²: 0.6989\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# Simple Ensemble: EfficientNetB3_5ch_Clinical + Swint_5ch_Clinical\n",
        "# Uses the SAME loaders (val_dl_full / test_dl_full) as your full-fusion setup\n",
        "# ==========================\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ---- your checkpoint paths\n",
        "RESNET_CKPT_PATH = CHECK_DIR + \"/best_full_fusion_ROI_ODOC_CLIN1.pth\"\n",
        "SWIN_CKPT_PATH = CHECK_DIR + \"/best_full_fusion_swint.pth\"\n",
        "\n",
        "\n",
        "# ---- small metric helpers (names won't clash with your existing ones)\n",
        "@torch.no_grad()\n",
        "def _rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _mae(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "def _load_model_states():\n",
        "    # Build the exact architectures (no pretrain needed when loading checkpoints)\n",
        "    resnet = EfficientNetB3_5ch_Clinical(\n",
        "        clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False\n",
        "    ).to(DEVICE)\n",
        "    swin = SwinT_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "\n",
        "    r_state = torch.load(RESNET_CKPT_PATH, map_location=DEVICE)\n",
        "    s_state = torch.load(SWIN_CKPT_PATH, map_location=DEVICE)\n",
        "\n",
        "    resnet.load_state_dict(r_state[\"model\"])\n",
        "    resnet.eval()\n",
        "    swin.load_state_dict(s_state[\"model\"])\n",
        "    swin.eval()\n",
        "    return resnet, swin\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ensemble_eval(loader, alpha=0.5):\n",
        "    \"\"\"\n",
        "    alpha: weight for SWIN (0..1). 0.5 = simple average\n",
        "    pred = (1-alpha)*resnet + alpha*swin\n",
        "    \"\"\"\n",
        "    assert 0.0 <= alpha <= 1.0\n",
        "    resnet, swin = _load_model_states()\n",
        "\n",
        "    y_true_chunks, y_pred_res_chunks, y_pred_swin_chunks, y_pred_ens_chunks = [], [], [], []\n",
        "\n",
        "    for x5, xclin, y in loader:\n",
        "        x5 = x5.to(DEVICE, non_blocking=True)\n",
        "        xcli = xclin.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        p_r = resnet(x5, xcli)  # (B, 59)\n",
        "        p_s = swin(x5, xcli)  # (B, 59)\n",
        "        p_e = (1.0 - alpha) * p_r + alpha * p_s\n",
        "\n",
        "        y_true_chunks.append(y.cpu())\n",
        "        y_pred_res_chunks.append(p_r.cpu())\n",
        "        y_pred_swin_chunks.append(p_s.cpu())\n",
        "        y_pred_ens_chunks.append(p_e.cpu())\n",
        "\n",
        "    y_true = torch.cat(y_true_chunks, dim=0)\n",
        "    p_res = torch.cat(y_pred_res_chunks, dim=0)\n",
        "    p_swin = torch.cat(y_pred_swin_chunks, dim=0)\n",
        "    p_ens = torch.cat(y_pred_ens_chunks, dim=0)\n",
        "\n",
        "    # pointwise metrics (flatten all 59 points)\n",
        "    pw_true, pw_res, pw_swin, pw_ens = (\n",
        "        y_true.reshape(-1),\n",
        "        p_res.reshape(-1),\n",
        "        p_swin.reshape(-1),\n",
        "        p_ens.reshape(-1),\n",
        "    )\n",
        "\n",
        "    print(\"\\n== INDIVIDUAL MODELS (pointwise) ==\")\n",
        "    print(\n",
        "        f\"EfficientNet  → RMSE: {_rmse(pw_true, pw_res):.4f} | MAE: {_mae(pw_true, pw_res):.4f} | R²: {_r2(pw_true, pw_res):.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Swin-B  → RMSE: {_rmse(pw_true, pw_swin):.4f} | MAE: {_mae(pw_true, pw_swin):.4f} | R²: {_r2(pw_true, pw_swin):.4f}\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n== ENSEMBLE (pointwise) ==\")\n",
        "    print(\n",
        "        f\"Avg (α={alpha:.2f}) → RMSE: {_rmse(pw_true, pw_ens):.4f} | MAE: {_mae(pw_true, pw_ens):.4f} | R²: {_r2(pw_true, pw_ens):.4f}\"\n",
        "    )\n",
        "\n",
        "    # MS metrics = mean of 59 points per sample\n",
        "    t_mean, r_mean, s_mean, e_mean = (\n",
        "        y_true.mean(dim=1),\n",
        "        p_res.mean(dim=1),\n",
        "        p_swin.mean(dim=1),\n",
        "        p_ens.mean(dim=1),\n",
        "    )\n",
        "\n",
        "    print(\"\\n== INDIVIDUAL MODELS (MS) ==\")\n",
        "    print(\n",
        "        f\"EfficientNet  → RMSE: {_rmse(t_mean, r_mean):.4f} | MAE: {_mae(t_mean, r_mean):.4f} | R²: {_r2(t_mean, r_mean):.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Swin-B  → RMSE: {_rmse(t_mean, s_mean):.4f} | MAE: {_mae(t_mean, s_mean):.4f} | R²: {_r2(t_mean, s_mean):.4f}\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n== ENSEMBLE (MS) ==\")\n",
        "    print(\n",
        "        f\"Avg (α={alpha:.2f}) → RMSE: {_rmse(t_mean, e_mean):.4f} | MAE: {_mae(t_mean, e_mean):.4f} | R²: {_r2(t_mean, e_mean):.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"y_true\": y_true,\n",
        "        \"pred_resnet\": p_res,\n",
        "        \"pred_swin\": p_swin,\n",
        "        \"pred_ensemble\": p_ens,\n",
        "    }\n",
        "\n",
        "\n",
        "# ===== RUN on your existing loaders =====\n",
        "# Use val set:\n",
        "assert \"val_dl_full\" in globals(), \"val_dl_full not found. Run your full-fusion data cell first.\"\n",
        "_ = ensemble_eval(val_dl_full, alpha=0.5)  # try alpha=0.3, 0.7, etc.\n",
        "\n",
        "# If you also have test_dl_full:\n",
        "# assert 'test_dl_full' in globals()\n",
        "# _ = ensemble_eval(test_dl_full, alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:37:07.068710Z",
          "iopub.status.busy": "2025-11-10T04:37:07.068455Z",
          "iopub.status.idle": "2025-11-10T04:37:09.933957Z",
          "shell.execute_reply": "2025-11-10T04:37:09.933312Z",
          "shell.execute_reply.started": "2025-11-10T04:37:07.068691Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH1dNR7EFbh0",
        "outputId": "b0ba3ce5-4418-4c61-83a5-6b355f501127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Ensemble] best alpha=0.80 (weights: SWIN=0.80, EFFICIENTNET=0.20)\n",
            "POINTWISE:\n",
            "  RMSE=5.8154 | MAE=4.0837 | R²=0.5946\n",
            "MS (mean sensitivity):\n",
            "  RMSE=3.3343 | MAE=2.2817 | R²=0.6898\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# SIMPLE ENSEMBLE: ResNet (full fusion) + Swin-B (full fusion)\n",
        "# Grid-search alpha on VAL to weight SWIN higher/lower\n",
        "# ==========================\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# --- Metrics (same style you used)\n",
        "@torch.no_grad()\n",
        "def _rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _mae(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# --- Load both models\n",
        "RESNET_CKPT = CHECK_DIR + \"/best_full_fusion_ROI_ODOC_CLIN1.pth\"\n",
        "SWIN_CKPT = CHECK_DIR + \"/best_full_fusion_swint.pth\"\n",
        "\n",
        "resnet = EfficientNetB3_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(\n",
        "    DEVICE\n",
        ")\n",
        "swin = SwinT_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "\n",
        "resnet.load_state_dict(torch.load(RESNET_CKPT, map_location=DEVICE)[\"model\"])\n",
        "swin.load_state_dict(torch.load(SWIN_CKPT, map_location=DEVICE)[\"model\"])\n",
        "resnet.eval()\n",
        "swin.eval()\n",
        "\n",
        "# --- Collect full VAL predictions once for speed\n",
        "y_true_list, pred_resnet_list, pred_swin_list = [], [], []\n",
        "with torch.no_grad():\n",
        "    for x5, xclin, y in val_dl_full:\n",
        "        x5 = x5.to(DEVICE)\n",
        "        xclin = xclin.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        pr = resnet(x5, xclin)\n",
        "        ps = swin(x5, xclin)\n",
        "        y_true_list.append(y.cpu())\n",
        "        pred_resnet_list.append(pr.cpu())\n",
        "        pred_swin_list.append(ps.cpu())\n",
        "\n",
        "y_true = torch.cat(y_true_list, dim=0)  # (N, 59)\n",
        "pred_r = torch.cat(pred_resnet_list, dim=0)  # (N, 59)\n",
        "pred_s = torch.cat(pred_swin_list, dim=0)  # (N, 59)\n",
        "\n",
        "# --- Grid-search alpha in [0,1] to minimize pointwise MAE (you can switch to MS if preferred)\n",
        "best_alpha, best_mae = None, float(\"inf\")\n",
        "for a in [i / 20 for i in range(21)]:  # 0.00, 0.05, ..., 1.00\n",
        "    ens = a * pred_s + (1 - a) * pred_r\n",
        "    mae_pw = _mae(ens.reshape(-1), y_true.reshape(-1))\n",
        "    if mae_pw < best_mae:\n",
        "        best_mae = mae_pw\n",
        "        best_alpha = a\n",
        "\n",
        "# --- Final ensemble metrics with the chosen alpha\n",
        "ens = best_alpha * pred_s + (1 - best_alpha) * pred_r\n",
        "pw_true, pw_pred = y_true.reshape(-1), ens.reshape(-1)\n",
        "print(\n",
        "    f\"\\n[Ensemble] best alpha={best_alpha:.2f} (weights: SWIN={best_alpha:.2f}, EFFICIENTNET={(1 - best_alpha):.2f})\"\n",
        ")\n",
        "print(\"POINTWISE:\")\n",
        "print(\n",
        "    f\"  RMSE={_rmse(pw_true, pw_pred):.4f} | MAE={_mae(pw_true, pw_pred):.4f} | R²={_r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "t_mean, p_mean = y_true.mean(dim=1), ens.mean(dim=1)\n",
        "print(\"MS (mean sensitivity):\")\n",
        "print(\n",
        "    f\"  RMSE={_rmse(t_mean, p_mean):.4f} | MAE={_mae(t_mean, p_mean):.4f} | R²={_r2(t_mean, p_mean):.4f}\"\n",
        ")\n",
        "\n",
        "# Save alpha if you want to reuse for test-time ensembling\n",
        "BEST_ALPHA = best_alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:35:33.870233Z",
          "iopub.status.busy": "2025-11-10T06:35:33.869481Z",
          "iopub.status.idle": "2025-11-10T06:35:34.053693Z",
          "shell.execute_reply": "2025-11-10T06:35:34.052990Z",
          "shell.execute_reply.started": "2025-11-10T06:35:33.870203Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6VKR4neFbh0",
        "outputId": "e8da7f4b-cc69-4208-b070-e0a9bf34b0af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_efficientnetb3_original_cfp.pth  best_efficientnetb3_ROI_ODOC.pth\n",
            "best_efficientnetb3_original_roi.pth  best_full_fusion_ROI_ODOC_CLIN1.pth\n",
            "best_efficientnetb3_ROI_ODOC_CDR.pth  best_full_fusion_swint.pth\n"
          ]
        }
      ],
      "source": [
        "!ls $CHECK_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:37:19.823311Z",
          "iopub.status.busy": "2025-11-10T06:37:19.823011Z",
          "iopub.status.idle": "2025-11-10T06:37:49.186221Z",
          "shell.execute_reply": "2025-11-10T06:37:49.185475Z",
          "shell.execute_reply.started": "2025-11-10T06:37:19.823284Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Ce6qxJhbFbh0",
        "outputId": "2c19690a-5487-45a1-8610-05991bbe314d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/final_checkpoints_archive.zip"
            ],
            "text/html": [
              "<a href='final_checkpoints_archive.zip' target='_blank'>final_checkpoints_archive.zip</a><br>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "from IPython.display import FileLink\n",
        "\n",
        "shutil.make_archive(\"final_checkpoints_archive\", \"zip\", CHECK_DIR)\n",
        "FileLink(\"final_checkpoints_archive.zip\")"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "formats": "ipynb,py:percent"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 8671492,
          "sourceId": 13641781,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}