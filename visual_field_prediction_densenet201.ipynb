{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:47:26.641075Z",
          "iopub.status.busy": "2025-11-08T09:47:26.640756Z",
          "iopub.status.idle": "2025-11-08T09:47:26.659110Z",
          "shell.execute_reply": "2025-11-08T09:47:26.658596Z",
          "shell.execute_reply.started": "2025-11-08T09:47:26.641051Z"
        },
        "id": "mwkqSmzY0EBQ"
      },
      "source": [
        "# 1. CFP Images DenseNet201"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:12.542505Z",
          "iopub.status.busy": "2025-11-10T05:17:12.542179Z",
          "iopub.status.idle": "2025-11-10T05:17:12.550563Z",
          "shell.execute_reply": "2025-11-10T05:17:12.550006Z",
          "shell.execute_reply.started": "2025-11-10T05:17:12.542470Z"
        },
        "id": "twrHvE2K0EBT"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "CSV_PATH = \"./filtered_glaucoma.csv\"\n",
        "IMG_ROOT = \"./glaucoma_data/CFPs\"  # <-- CFP images folder\n",
        "CHECK_DIR = \"./checkpoints\"\n",
        "CFP_DIR = \"./glaucoma_data/ROI images\"  # <-- ROI images folder\n",
        "ROI_DIR = \"./glaucoma_data/ROI images\"  # ROI images folder\n",
        "JSON_DIR = \"./glaucoma_data/json\"  # LabelMe JSON files matching image names\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59\n",
        "PATIENCE = 10\n",
        "MIN_DELTA = 0.01\n",
        "\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fE1jACmT0EBX"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(\"./glaucoma_data.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"./\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:15.582725Z",
          "iopub.status.busy": "2025-11-10T05:17:15.582037Z",
          "iopub.status.idle": "2025-11-10T05:17:15.592790Z",
          "shell.execute_reply": "2025-11-10T05:17:15.592080Z",
          "shell.execute_reply.started": "2025-11-10T05:17:15.582701Z"
        },
        "id": "Tu5DQA7v0EBY"
      },
      "outputs": [],
      "source": [
        "# ---- tiny CSV reader (no pandas) ------------------------------------------------\n",
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]  # ensure equal length\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ---- detect columns --------------------------------------------------------------\n",
        "IMAGE_COLS_CANDIDATES = [\"image\", \"image_name\", \"img\", \"image_path\", \"filename\", \"file\"]\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]):\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # find image column\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # pick 59 VF columns: prefer v1..v59\n",
        "    vf_cols = [f\"v{i}\" for i in range(1, NUM_POINTS + 1)]\n",
        "    if all(c in cols for c in vf_cols):\n",
        "        return image_col, vf_cols\n",
        "\n",
        "    # fallback: numeric columns\n",
        "    candidates = []\n",
        "    for c in cols:\n",
        "        if c == image_col:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            candidates.append(c)\n",
        "\n",
        "    if len(candidates) < NUM_POINTS:\n",
        "        raise ValueError(\"Not enough numeric VF columns detected.\")\n",
        "\n",
        "    # sort by trailing number if exists\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    candidates_sorted = sorted(candidates, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, candidates_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:18.937424Z",
          "iopub.status.busy": "2025-11-10T05:17:18.936885Z",
          "iopub.status.idle": "2025-11-10T05:17:18.944011Z",
          "shell.execute_reply": "2025-11-10T05:17:18.943261Z",
          "shell.execute_reply.started": "2025-11-10T05:17:18.937404Z"
        },
        "id": "s9R_A95m0EBa"
      },
      "outputs": [],
      "source": [
        "class CFPDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows: List[Dict[str, str]], image_col: str, vf_cols: List[str], train: bool\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.train = train\n",
        "\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "\n",
        "        self.tf = transforms.Compose(\n",
        "            [transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(), *aug, normalize]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        name = r[self.image_col]\n",
        "\n",
        "        path = name\n",
        "        if not os.path.isabs(path):\n",
        "            if os.path.basename(path) == path:\n",
        "                path = os.path.join(IMG_ROOT, path)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        x = self.tf(img)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:22.158208Z",
          "iopub.status.busy": "2025-11-10T05:17:22.157482Z",
          "iopub.status.idle": "2025-11-10T05:17:22.163262Z",
          "shell.execute_reply": "2025-11-10T05:17:22.162535Z",
          "shell.execute_reply.started": "2025-11-10T05:17:22.158181Z"
        },
        "id": "CGqM-TcJ0EBb"
      },
      "outputs": [],
      "source": [
        "class DenseNet201VF(nn.Module):\n",
        "    def __init__(self, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.densenet201(\n",
        "            weights=models.DenseNet201_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        in_f = self.backbone.classifier.in_features\n",
        "        self.backbone.classifier = nn.Identity()\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25), nn.Linear(512, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x)\n",
        "        return self.regressor(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:25.577607Z",
          "iopub.status.busy": "2025-11-10T05:17:25.577128Z",
          "iopub.status.idle": "2025-11-10T05:17:25.584146Z",
          "shell.execute_reply": "2025-11-10T05:17:25.583424Z",
          "shell.execute_reply.started": "2025-11-10T05:17:25.577575Z"
        },
        "id": "ul9FXRGi0EBc"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = crit(pred, y)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        n += bs\n",
        "\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:29.978088Z",
          "iopub.status.busy": "2025-11-10T05:17:29.977820Z",
          "iopub.status.idle": "2025-11-10T05:28:21.822456Z",
          "shell.execute_reply": "2025-11-10T05:28:21.821528Z",
          "shell.execute_reply.started": "2025-11-10T05:17:29.978068Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1I0uEvZ0EBc",
        "outputId": "68356daf-6e4c-4f5a-e63c-735f80c4cdfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] using 59 VF columns: ['AGE', 'CCT', 'IOP_y', 'Interval Years', 'MD'] ... ['VF50', 'VF51', 'VF52', 'VF53', 'VF54']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77.4M/77.4M [00:00<00:00, 119MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 01 | train_loss=5358.6245  train_pMAE=28.256  train_msMAE=28.089 || val_loss=5118.2879  val_pMAE=25.933  val_msMAE=25.552\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=25.933)\n",
            "\n",
            "Epoch 02 | train_loss=4821.2684  train_pMAE=23.697  train_msMAE=22.992 || val_loss=4381.2883  val_pMAE=18.276  val_msMAE=16.004\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=18.276)\n",
            "\n",
            "Epoch 03 | train_loss=4084.5757  train_pMAE=15.818  train_msMAE=12.446 || val_loss=3755.1085  val_pMAE=13.890  val_msMAE=5.775\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=13.890)\n",
            "\n",
            "Epoch 04 | train_loss=3466.2184  train_pMAE=13.501  train_msMAE=5.512 || val_loss=3194.1249  val_pMAE=13.026  val_msMAE=5.134\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=13.026)\n",
            "\n",
            "Epoch 05 | train_loss=2856.3070  train_pMAE=12.492  train_msMAE=5.590 || val_loss=2582.7294  val_pMAE=11.677  val_msMAE=6.041\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=11.677)\n",
            "\n",
            "Epoch 06 | train_loss=2218.3164  train_pMAE=11.895  train_msMAE=5.615 || val_loss=1888.5956  val_pMAE=10.717  val_msMAE=5.106\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=10.717)\n",
            "\n",
            "Epoch 07 | train_loss=1603.2886  train_pMAE=11.301  train_msMAE=5.022 || val_loss=1328.7101  val_pMAE=10.047  val_msMAE=5.404\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=10.047)\n",
            "\n",
            "Epoch 08 | train_loss=1072.5740  train_pMAE=10.737  train_msMAE=4.732 || val_loss=818.3152  val_pMAE=8.974  val_msMAE=4.749\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=8.974)\n",
            "\n",
            "Epoch 09 | train_loss=677.8454  train_pMAE=10.036  train_msMAE=4.175 || val_loss=532.8961  val_pMAE=8.406  val_msMAE=4.762\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=8.406)\n",
            "\n",
            "Epoch 10 | train_loss=402.0371  train_pMAE=9.421  train_msMAE=3.893 || val_loss=232.6357  val_pMAE=6.839  val_msMAE=3.518\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=6.839)\n",
            "\n",
            "Epoch 11 | train_loss=244.5982  train_pMAE=8.862  train_msMAE=3.656 || val_loss=253.0705  val_pMAE=7.183  val_msMAE=4.071\n",
            "\n",
            "Epoch 12 | train_loss=173.2676  train_pMAE=8.440  train_msMAE=3.505 || val_loss=128.0871  val_pMAE=6.334  val_msMAE=3.631\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=6.334)\n",
            "\n",
            "Epoch 13 | train_loss=143.9874  train_pMAE=8.149  train_msMAE=3.287 || val_loss=109.8815  val_pMAE=6.150  val_msMAE=3.698\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=6.150)\n",
            "\n",
            "Epoch 14 | train_loss=131.9759  train_pMAE=8.012  train_msMAE=3.285 || val_loss=113.7499  val_pMAE=6.334  val_msMAE=3.845\n",
            "\n",
            "Epoch 15 | train_loss=126.5196  train_pMAE=7.904  train_msMAE=3.301 || val_loss=108.3843  val_pMAE=6.105  val_msMAE=3.640\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=6.105)\n",
            "\n",
            "Epoch 16 | train_loss=124.0292  train_pMAE=7.810  train_msMAE=3.254 || val_loss=96.1256  val_pMAE=5.920  val_msMAE=3.536\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=5.920)\n",
            "\n",
            "Epoch 17 | train_loss=123.6944  train_pMAE=7.752  train_msMAE=3.262 || val_loss=103.0086  val_pMAE=6.238  val_msMAE=3.852\n",
            "\n",
            "Epoch 18 | train_loss=122.7401  train_pMAE=7.737  train_msMAE=3.276 || val_loss=106.8798  val_pMAE=6.012  val_msMAE=3.655\n",
            "\n",
            "Epoch 19 | train_loss=122.3470  train_pMAE=7.721  train_msMAE=3.272 || val_loss=97.1376  val_pMAE=6.173  val_msMAE=3.788\n",
            "\n",
            "Epoch 20 | train_loss=120.3132  train_pMAE=7.583  train_msMAE=3.188 || val_loss=96.4986  val_pMAE=6.003  val_msMAE=3.697\n",
            "\n",
            "Epoch 21 | train_loss=118.9189  train_pMAE=7.575  train_msMAE=3.284 || val_loss=95.8327  val_pMAE=5.859  val_msMAE=3.406\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=5.859)\n",
            "\n",
            "Epoch 22 | train_loss=118.8693  train_pMAE=7.518  train_msMAE=3.168 || val_loss=93.1542  val_pMAE=5.919  val_msMAE=3.615\n",
            "\n",
            "Epoch 23 | train_loss=115.2532  train_pMAE=7.400  train_msMAE=3.232 || val_loss=93.9740  val_pMAE=5.909  val_msMAE=3.581\n",
            "\n",
            "Epoch 24 | train_loss=116.6391  train_pMAE=7.501  train_msMAE=3.330 || val_loss=93.7733  val_pMAE=5.875  val_msMAE=3.572\n",
            "\n",
            "Epoch 25 | train_loss=115.6087  train_pMAE=7.444  train_msMAE=3.286 || val_loss=93.6647  val_pMAE=6.016  val_msMAE=3.660\n",
            "\n",
            "Epoch 26 | train_loss=114.3173  train_pMAE=7.374  train_msMAE=3.199 || val_loss=93.7354  val_pMAE=5.939  val_msMAE=3.585\n",
            "\n",
            "Epoch 27 | train_loss=115.1870  train_pMAE=7.408  train_msMAE=3.185 || val_loss=92.9430  val_pMAE=5.880  val_msMAE=3.503\n",
            "\n",
            "Epoch 28 | train_loss=115.7011  train_pMAE=7.403  train_msMAE=3.243 || val_loss=93.3421  val_pMAE=5.865  val_msMAE=3.528\n",
            "\n",
            "Epoch 29 | train_loss=112.8363  train_pMAE=7.318  train_msMAE=3.210 || val_loss=93.2790  val_pMAE=5.830  val_msMAE=3.496\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=5.830)\n",
            "\n",
            "Epoch 30 | train_loss=112.2970  train_pMAE=7.306  train_msMAE=3.243 || val_loss=93.9308  val_pMAE=5.902  val_msMAE=3.573\n",
            "\n",
            "Epoch 31 | train_loss=113.1799  train_pMAE=7.331  train_msMAE=3.244 || val_loss=94.3266  val_pMAE=5.863  val_msMAE=3.529\n",
            "\n",
            "Epoch 32 | train_loss=112.0546  train_pMAE=7.309  train_msMAE=3.088 || val_loss=93.8762  val_pMAE=5.879  val_msMAE=3.555\n",
            "\n",
            "Epoch 33 | train_loss=111.8304  train_pMAE=7.249  train_msMAE=3.208 || val_loss=93.1818  val_pMAE=5.936  val_msMAE=3.615\n",
            "\n",
            "Epoch 34 | train_loss=113.3294  train_pMAE=7.300  train_msMAE=3.164 || val_loss=92.5383  val_pMAE=5.860  val_msMAE=3.546\n",
            "\n",
            "Epoch 35 | train_loss=112.5761  train_pMAE=7.313  train_msMAE=3.254 || val_loss=91.8646  val_pMAE=5.806  val_msMAE=3.483\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=5.806)\n",
            "\n",
            "Epoch 36 | train_loss=111.5327  train_pMAE=7.275  train_msMAE=3.262 || val_loss=93.6281  val_pMAE=5.981  val_msMAE=3.683\n",
            "\n",
            "Epoch 37 | train_loss=112.6503  train_pMAE=7.285  train_msMAE=3.220 || val_loss=93.2621  val_pMAE=5.867  val_msMAE=3.532\n",
            "\n",
            "Epoch 38 | train_loss=111.7099  train_pMAE=7.273  train_msMAE=3.244 || val_loss=94.8276  val_pMAE=5.934  val_msMAE=3.603\n",
            "\n",
            "Epoch 39 | train_loss=112.4546  train_pMAE=7.286  train_msMAE=3.210 || val_loss=92.8290  val_pMAE=5.825  val_msMAE=3.505\n",
            "\n",
            "Epoch 40 | train_loss=111.1356  train_pMAE=7.221  train_msMAE=3.187 || val_loss=92.8870  val_pMAE=5.834  val_msMAE=3.510\n",
            "\n",
            "Epoch 41 | train_loss=112.3468  train_pMAE=7.254  train_msMAE=3.185 || val_loss=92.3318  val_pMAE=5.807  val_msMAE=3.474\n",
            "\n",
            "Epoch 42 | train_loss=110.6626  train_pMAE=7.221  train_msMAE=3.190 || val_loss=92.2024  val_pMAE=5.803  val_msMAE=3.488\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_cfp.pth (pMAE=5.803)\n",
            "\n",
            "Epoch 43 | train_loss=110.6342  train_pMAE=7.222  train_msMAE=3.128 || val_loss=92.1488  val_pMAE=5.837  val_msMAE=3.524\n",
            "\n",
            "Epoch 44 | train_loss=112.2367  train_pMAE=7.267  train_msMAE=3.191 || val_loss=92.4864  val_pMAE=5.841  val_msMAE=3.536\n",
            "\n",
            "Epoch 45 | train_loss=109.9502  train_pMAE=7.176  train_msMAE=3.092 || val_loss=92.9060  val_pMAE=5.866  val_msMAE=3.572\n",
            "\n",
            "Epoch 46 | train_loss=110.8095  train_pMAE=7.249  train_msMAE=3.233 || val_loss=92.4653  val_pMAE=5.849  val_msMAE=3.551\n",
            "\n",
            "Early stopping at epoch 46 (best val pMAE=5.803)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- reproducibility (same as before)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class EarlyStopper:\n",
        "    \"\"\"\n",
        "    - Saves whenever val_metric strictly improves over 'best' (tolerance=1e-12).\n",
        "    - Uses 'min_delta' only to decide whether to reset patience (ref metric).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best_save = float(\"inf\")  # for checkpoint saving (any improvement)\n",
        "        self.best_ref = float(\"inf\")  # for patience (needs >= min_delta improvement)\n",
        "        self.bad_epochs = 0\n",
        "        if self.ckpt_path:\n",
        "            os.makedirs(os.path.dirname(self.ckpt_path), exist_ok=True)\n",
        "\n",
        "    def update(self, val_metric, model, epoch_meta=None):\n",
        "        saved = False\n",
        "        # --- Save on ANY strict improvement\n",
        "        if val_metric < self.best_save - 1e-12:\n",
        "            self.best_save = val_metric\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best_save,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            saved = True\n",
        "\n",
        "        # --- Early-stopping patience uses min_delta\n",
        "        if val_metric < self.best_ref - self.min_delta:\n",
        "            self.best_ref = val_metric\n",
        "            self.bad_epochs = 0\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "\n",
        "        should_stop = self.bad_epochs > self.patience\n",
        "        return should_stop, saved\n",
        "\n",
        "\n",
        "def main():\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] using {len(vf_cols)} VF columns: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # split\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    random.shuffle(rows)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    train_ds = CFPDataset(train_rows, image_col, vf_cols, train=True)\n",
        "    val_ds = CFPDataset(val_rows, image_col, vf_cols, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = DenseNet201VF(out_dim=NUM_POINTS, pretrained=True).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt_path = os.path.join(CHECK_DIR, \"best_densenet201_original_cfp.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt_path)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        print(\n",
        "            f\"\\nEpoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_msMAE={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_msMAE={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # scheduler on validation MAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        # early stopping + save best\n",
        "        should_stop, saved = stopper.update(\n",
        "            va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch}\n",
        "        )\n",
        "        if saved:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt_path} (pMAE={stopper.best_save:.3f})\")\n",
        "        if should_stop:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (best val pMAE={stopper.best_save:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best before returning\n",
        "    state = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt_path\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, train_dl, val_dl, image_col, vf_cols, CKPT = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:29:03.478902Z",
          "iopub.status.busy": "2025-11-10T05:29:03.478577Z",
          "iopub.status.idle": "2025-11-10T05:29:07.443091Z",
          "shell.execute_reply": "2025-11-10T05:29:07.442164Z",
          "shell.execute_reply.started": "2025-11-10T05:29:03.478871Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4obpIFs0EBe",
        "outputId": "5348c74a-836f-4755-f658-11c05d0376b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using BEST checkpoint: ./checkpoints/best_densenet201_original_cfp.pth\n",
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n"
          ]
        }
      ],
      "source": [
        "# --- reload BEST checkpoint and evaluate ---\n",
        "assert \"CKPT\" in globals(), (\n",
        "    \"CKPT not found. Make sure you ran the training cell that returns CKPT.\"\n",
        ")\n",
        "assert \"val_dl\" in globals(), (\n",
        "    \"val_dl not found. Make sure you ran the training cell that defines val_dl.\"\n",
        ")\n",
        "\n",
        "# rebuild the exact architecture\n",
        "best_model = DenseNet201VF(out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "\n",
        "state = torch.load(CKPT, map_location=DEVICE)\n",
        "best_model.load_state_dict(state[\"model\"])\n",
        "best_model.eval()\n",
        "\n",
        "# collect predictions on the validation set\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in val_dl:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        p = best_model(x)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "\n",
        "print(\"✅ Using BEST checkpoint:\", CKPT)\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:29:10.997372Z",
          "iopub.status.busy": "2025-11-10T05:29:10.997079Z",
          "iopub.status.idle": "2025-11-10T05:29:11.006552Z",
          "shell.execute_reply": "2025-11-10T05:29:11.005959Z",
          "shell.execute_reply.started": "2025-11-10T05:29:10.997345Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eukd-rFq0EBf",
        "outputId": "90c0c0da-77bd-4368-b9bb-e883b9cca153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== POINTWISE ==\n",
            "RMSE: 9.6022 | MAE: 5.8032 | R²: 0.9807\n",
            "== POINTWISE-MEAN ==\n",
            "RMSE: 4.5299 | MAE: 3.4884 | R²: 0.2438\n",
            "== MS (same as pointwise-mean) ==\n",
            "RMSE: 4.5299 | MAE: 3.4884 | R²: 0.2438\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_val(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "print(\"\\n== POINTWISE ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(pw_true, pw_pred):.4f} | MAE: {mae_val(pw_true, pw_pred):.4f} | R²: {r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "\n",
        "t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "print(\"== POINTWISE-MEAN ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | MAE: {mae_val(t_mean, p_mean):.4f} | R²: {r2(t_mean, p_mean):.4f}\"\n",
        ")\n",
        "\n",
        "print(\"== MS (same as pointwise-mean) ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | MAE: {mae_val(t_mean, p_mean):.4f} | R²: {r2(t_mean, p_mean):.4f}\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.117107Z",
          "iopub.status.busy": "2025-11-08T09:56:44.116818Z",
          "iopub.status.idle": "2025-11-08T09:56:44.137012Z",
          "shell.execute_reply": "2025-11-08T09:56:44.136317Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.117090Z"
        },
        "id": "VdISVo-E0EBg"
      },
      "source": [
        "# 2. ROI Images DenseNet201"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.138029Z",
          "iopub.status.busy": "2025-11-08T09:56:44.137733Z",
          "iopub.status.idle": "2025-11-08T09:56:44.156020Z",
          "shell.execute_reply": "2025-11-08T09:56:44.155523Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.138001Z"
        },
        "lines_to_next_cell": 2,
        "id": "sk266FIb0EBh"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.156862Z",
          "iopub.status.busy": "2025-11-08T09:56:44.156704Z",
          "iopub.status.idle": "2025-11-08T09:56:44.195467Z",
          "shell.execute_reply": "2025-11-08T09:56:44.194795Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.156849Z"
        },
        "id": "lVzTFfnf0EBh"
      },
      "outputs": [],
      "source": [
        "# ===================== LABELME JSON → OD/OC MASKS =====================\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _poly_area(pts):\n",
        "    x = [p[0] for p in pts]\n",
        "    y = [p[1] for p in pts]\n",
        "    return 0.5 * abs(\n",
        "        sum(x[i] * y[(i + 1) % len(pts)] - x[(i + 1) % len(pts)] * y[i] for i in range(len(pts)))\n",
        "    )\n",
        "\n",
        "\n",
        "def _read_labelme(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        label = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = [(float(x), float(y)) for x, y in sh.get(\"points\", [])]\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if label in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        elif label in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=_poly_area)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=_poly_area)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json_path(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        cand = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(cand):\n",
        "            return cand\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def build_masks_from_labelme(img_pil: Image.Image, img_name: str, out_size: int):\n",
        "    W, H = img_pil.size\n",
        "    od_mask = Image.new(\"L\", (W, H), 0)\n",
        "    oc_mask = Image.new(\"L\", (W, H), 0)\n",
        "\n",
        "    jpath = _guess_json_path(img_name)\n",
        "    if jpath:\n",
        "        try:\n",
        "            od_polys, oc_polys = _read_labelme(jpath)\n",
        "            d_od = ImageDraw.Draw(od_mask)\n",
        "            d_oc = ImageDraw.Draw(oc_mask)\n",
        "            for poly in od_polys:\n",
        "                d_od.polygon(poly, outline=1, fill=1)\n",
        "            for poly in oc_polys:\n",
        "                d_oc.polygon(poly, outline=1, fill=1)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] parsing {jpath}: {e}\")\n",
        "\n",
        "    od_mask = od_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    oc_mask = oc_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    return od_mask, oc_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.196439Z",
          "iopub.status.busy": "2025-11-08T09:56:44.196246Z",
          "iopub.status.idle": "2025-11-08T09:56:44.218107Z",
          "shell.execute_reply": "2025-11-08T09:56:44.217506Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.196424Z"
        },
        "id": "46kKrNKt0EBi"
      },
      "outputs": [],
      "source": [
        "class CFPDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows: List[Dict[str, str]], image_col: str, vf_cols: List[str], train: bool\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.train = train\n",
        "\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "\n",
        "        self.tf = transforms.Compose(\n",
        "            [transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(), *aug, normalize]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        name = r[self.image_col]\n",
        "\n",
        "        path = name\n",
        "        if not os.path.isabs(path):\n",
        "            if os.path.basename(path) == path:\n",
        "                path = os.path.join(CFP_DIR, path)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        x = self.tf(img)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.219231Z",
          "iopub.status.busy": "2025-11-08T09:56:44.218972Z",
          "iopub.status.idle": "2025-11-08T09:56:44.240915Z",
          "shell.execute_reply": "2025-11-08T09:56:44.240375Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.219208Z"
        },
        "id": "0bMTRm-Z0EBi"
      },
      "outputs": [],
      "source": [
        "class DenseNet201VF(nn.Module):\n",
        "    def __init__(self, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.densenet201(\n",
        "            weights=models.DenseNet201_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        in_f = self.backbone.classifier.in_features\n",
        "        self.backbone.classifier = nn.Identity()\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25), nn.Linear(512, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x)\n",
        "        return self.regressor(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.243731Z",
          "iopub.status.busy": "2025-11-08T09:56:44.243485Z",
          "iopub.status.idle": "2025-11-08T09:56:44.256968Z",
          "shell.execute_reply": "2025-11-08T09:56:44.256437Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.243714Z"
        },
        "id": "odxc4mrn0EBj"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = crit(pred, y)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        n += bs\n",
        "\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:29:55.027957Z",
          "iopub.status.busy": "2025-11-10T05:29:55.027457Z",
          "iopub.status.idle": "2025-11-10T05:40:46.560388Z",
          "shell.execute_reply": "2025-11-10T05:40:46.559609Z",
          "shell.execute_reply.started": "2025-11-10T05:29:55.027932Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ItNc8wk0EBj",
        "outputId": "bd74e69c-d9e1-45eb-b653-f0afb5fa0691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] using 59 VF columns: ['AGE', 'CCT', 'IOP_y', 'Interval Years', 'MD'] ... ['VF50', 'VF51', 'VF52', 'VF53', 'VF54']\n",
            "\n",
            "Epoch 01 | train_loss=5367.0008  train_pMAE=28.297  train_msMAE=28.135 || val_loss=5201.0622  val_pMAE=26.399  val_msMAE=26.109\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=26.399)\n",
            "\n",
            "Epoch 02 | train_loss=4877.6038  train_pMAE=24.096  train_msMAE=23.537 || val_loss=4407.6314  val_pMAE=18.385  val_msMAE=16.348\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=18.385)\n",
            "\n",
            "Epoch 03 | train_loss=4114.0299  train_pMAE=15.938  train_msMAE=12.688 || val_loss=3750.1176  val_pMAE=14.031  val_msMAE=5.293\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=14.031)\n",
            "\n",
            "Epoch 04 | train_loss=3456.9848  train_pMAE=13.586  train_msMAE=5.176 || val_loss=3127.6316  val_pMAE=13.272  val_msMAE=4.373\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=13.272)\n",
            "\n",
            "Epoch 05 | train_loss=2813.7278  train_pMAE=12.459  train_msMAE=5.504 || val_loss=2460.0857  val_pMAE=11.526  val_msMAE=5.532\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=11.526)\n",
            "\n",
            "Epoch 06 | train_loss=2154.5296  train_pMAE=11.824  train_msMAE=5.534 || val_loss=1765.4693  val_pMAE=10.527  val_msMAE=4.640\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=10.527)\n",
            "\n",
            "Epoch 07 | train_loss=1530.0316  train_pMAE=11.205  train_msMAE=4.936 || val_loss=1231.4021  val_pMAE=9.811  val_msMAE=5.047\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=9.811)\n",
            "\n",
            "Epoch 08 | train_loss=1003.1065  train_pMAE=10.638  train_msMAE=4.619 || val_loss=749.9099  val_pMAE=8.724  val_msMAE=4.469\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=8.724)\n",
            "\n",
            "Epoch 09 | train_loss=623.2700  train_pMAE=9.941  train_msMAE=4.126 || val_loss=416.8954  val_pMAE=7.900  val_msMAE=4.248\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=7.900)\n",
            "\n",
            "Epoch 10 | train_loss=364.1173  train_pMAE=9.309  train_msMAE=3.805 || val_loss=250.4579  val_pMAE=6.961  val_msMAE=3.603\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=6.961)\n",
            "\n",
            "Epoch 11 | train_loss=224.6746  train_pMAE=8.770  train_msMAE=3.614 || val_loss=211.0897  val_pMAE=6.878  val_msMAE=3.738\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=6.878)\n",
            "\n",
            "Epoch 12 | train_loss=163.5068  train_pMAE=8.361  train_msMAE=3.455 || val_loss=122.8299  val_pMAE=6.368  val_msMAE=3.825\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=6.368)\n",
            "\n",
            "Epoch 13 | train_loss=141.4682  train_pMAE=8.135  train_msMAE=3.332 || val_loss=101.4962  val_pMAE=5.959  val_msMAE=3.513\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=5.959)\n",
            "\n",
            "Epoch 14 | train_loss=131.1555  train_pMAE=8.010  train_msMAE=3.288 || val_loss=121.9770  val_pMAE=6.535  val_msMAE=3.954\n",
            "\n",
            "Epoch 15 | train_loss=126.1072  train_pMAE=7.903  train_msMAE=3.336 || val_loss=102.7671  val_pMAE=6.006  val_msMAE=3.530\n",
            "\n",
            "Epoch 16 | train_loss=123.6771  train_pMAE=7.806  train_msMAE=3.256 || val_loss=94.5603  val_pMAE=5.844  val_msMAE=3.368\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=5.844)\n",
            "\n",
            "Epoch 17 | train_loss=123.0939  train_pMAE=7.750  train_msMAE=3.277 || val_loss=99.4039  val_pMAE=6.141  val_msMAE=3.765\n",
            "\n",
            "Epoch 18 | train_loss=121.8544  train_pMAE=7.725  train_msMAE=3.280 || val_loss=101.5923  val_pMAE=5.899  val_msMAE=3.487\n",
            "\n",
            "Epoch 19 | train_loss=121.7092  train_pMAE=7.708  train_msMAE=3.264 || val_loss=95.6729  val_pMAE=6.125  val_msMAE=3.722\n",
            "\n",
            "Epoch 20 | train_loss=119.5437  train_pMAE=7.575  train_msMAE=3.185 || val_loss=94.9905  val_pMAE=5.932  val_msMAE=3.532\n",
            "\n",
            "Epoch 21 | train_loss=118.1672  train_pMAE=7.560  train_msMAE=3.236 || val_loss=95.4295  val_pMAE=5.901  val_msMAE=3.466\n",
            "\n",
            "Epoch 22 | train_loss=118.8316  train_pMAE=7.526  train_msMAE=3.180 || val_loss=95.9684  val_pMAE=6.058  val_msMAE=3.702\n",
            "\n",
            "Epoch 23 | train_loss=114.7565  train_pMAE=7.384  train_msMAE=3.196 || val_loss=97.0171  val_pMAE=6.026  val_msMAE=3.650\n",
            "\n",
            "Epoch 24 | train_loss=116.8204  train_pMAE=7.511  train_msMAE=3.359 || val_loss=93.2849  val_pMAE=5.853  val_msMAE=3.477\n",
            "\n",
            "Epoch 25 | train_loss=115.2403  train_pMAE=7.429  train_msMAE=3.267 || val_loss=93.6850  val_pMAE=5.932  val_msMAE=3.543\n",
            "\n",
            "Epoch 26 | train_loss=114.1459  train_pMAE=7.378  train_msMAE=3.193 || val_loss=93.2505  val_pMAE=5.928  val_msMAE=3.561\n",
            "\n",
            "Epoch 27 | train_loss=115.0488  train_pMAE=7.420  train_msMAE=3.218 || val_loss=91.9996  val_pMAE=5.813  val_msMAE=3.412\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=5.813)\n",
            "\n",
            "Epoch 28 | train_loss=115.6561  train_pMAE=7.418  train_msMAE=3.254 || val_loss=92.5928  val_pMAE=5.845  val_msMAE=3.499\n",
            "\n",
            "Epoch 29 | train_loss=112.9443  train_pMAE=7.341  train_msMAE=3.207 || val_loss=92.3193  val_pMAE=5.814  val_msMAE=3.461\n",
            "\n",
            "Epoch 30 | train_loss=112.6215  train_pMAE=7.331  train_msMAE=3.249 || val_loss=93.0642  val_pMAE=5.888  val_msMAE=3.553\n",
            "\n",
            "Epoch 31 | train_loss=113.3073  train_pMAE=7.341  train_msMAE=3.236 || val_loss=93.1186  val_pMAE=5.797  val_msMAE=3.440\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=5.797)\n",
            "\n",
            "Epoch 32 | train_loss=112.3431  train_pMAE=7.329  train_msMAE=3.095 || val_loss=92.7682  val_pMAE=5.843  val_msMAE=3.499\n",
            "\n",
            "Epoch 33 | train_loss=111.6761  train_pMAE=7.250  train_msMAE=3.204 || val_loss=92.8650  val_pMAE=5.924  val_msMAE=3.585\n",
            "\n",
            "Epoch 34 | train_loss=113.7112  train_pMAE=7.306  train_msMAE=3.122 || val_loss=91.9942  val_pMAE=5.829  val_msMAE=3.478\n",
            "\n",
            "Epoch 35 | train_loss=112.7127  train_pMAE=7.328  train_msMAE=3.251 || val_loss=91.6137  val_pMAE=5.777  val_msMAE=3.396\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=5.777)\n",
            "\n",
            "Epoch 36 | train_loss=111.4660  train_pMAE=7.288  train_msMAE=3.276 || val_loss=93.8944  val_pMAE=5.982  val_msMAE=3.630\n",
            "\n",
            "Epoch 37 | train_loss=112.6082  train_pMAE=7.260  train_msMAE=3.208 || val_loss=92.1307  val_pMAE=5.843  val_msMAE=3.456\n",
            "\n",
            "Epoch 38 | train_loss=111.4840  train_pMAE=7.275  train_msMAE=3.278 || val_loss=93.5627  val_pMAE=5.865  val_msMAE=3.461\n",
            "\n",
            "Epoch 39 | train_loss=112.3234  train_pMAE=7.279  train_msMAE=3.191 || val_loss=93.2082  val_pMAE=5.852  val_msMAE=3.479\n",
            "\n",
            "Epoch 40 | train_loss=111.4306  train_pMAE=7.232  train_msMAE=3.204 || val_loss=92.3948  val_pMAE=5.821  val_msMAE=3.456\n",
            "\n",
            "Epoch 41 | train_loss=111.7622  train_pMAE=7.225  train_msMAE=3.180 || val_loss=91.8538  val_pMAE=5.793  val_msMAE=3.419\n",
            "\n",
            "Epoch 42 | train_loss=110.6103  train_pMAE=7.217  train_msMAE=3.222 || val_loss=91.9031  val_pMAE=5.775  val_msMAE=3.416\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_original_roi.pth (pMAE=5.775)\n",
            "\n",
            "Epoch 43 | train_loss=110.2318  train_pMAE=7.201  train_msMAE=3.139 || val_loss=91.7348  val_pMAE=5.827  val_msMAE=3.459\n",
            "\n",
            "Epoch 44 | train_loss=111.5450  train_pMAE=7.229  train_msMAE=3.193 || val_loss=92.2475  val_pMAE=5.840  val_msMAE=3.497\n",
            "\n",
            "Epoch 45 | train_loss=109.3876  train_pMAE=7.151  train_msMAE=3.105 || val_loss=92.6508  val_pMAE=5.870  val_msMAE=3.566\n",
            "\n",
            "Epoch 46 | train_loss=110.2040  train_pMAE=7.220  train_msMAE=3.257 || val_loss=91.6391  val_pMAE=5.812  val_msMAE=3.483\n",
            "\n",
            "Early stopping at epoch 46 (best val pMAE=5.775)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- reproducibility (same as before)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class EarlyStopper:\n",
        "    \"\"\"\n",
        "    - Saves whenever val_metric strictly improves over 'best' (tolerance=1e-12).\n",
        "    - Uses 'min_delta' only to decide whether to reset patience (ref metric).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best_save = float(\"inf\")  # for checkpoint saving (any improvement)\n",
        "        self.best_ref = float(\"inf\")  # for patience (needs >= min_delta improvement)\n",
        "        self.bad_epochs = 0\n",
        "        if self.ckpt_path:\n",
        "            os.makedirs(os.path.dirname(self.ckpt_path), exist_ok=True)\n",
        "\n",
        "    def update(self, val_metric, model, epoch_meta=None):\n",
        "        saved = False\n",
        "        # --- Save on ANY strict improvement\n",
        "        if val_metric < self.best_save - 1e-12:\n",
        "            self.best_save = val_metric\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best_save,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            saved = True\n",
        "\n",
        "        # --- Early-stopping patience uses min_delta\n",
        "        if val_metric < self.best_ref - self.min_delta:\n",
        "            self.best_ref = val_metric\n",
        "            self.bad_epochs = 0\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "\n",
        "        should_stop = self.bad_epochs > self.patience\n",
        "        return should_stop, saved\n",
        "\n",
        "\n",
        "def main():\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] using {len(vf_cols)} VF columns: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # split\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    random.shuffle(rows)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    train_ds = CFPDataset(train_rows, image_col, vf_cols, train=True)\n",
        "    val_ds = CFPDataset(val_rows, image_col, vf_cols, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = DenseNet201VF(out_dim=NUM_POINTS, pretrained=True).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt_path = os.path.join(CHECK_DIR, \"best_densenet201_original_roi.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt_path)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        print(\n",
        "            f\"\\nEpoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_msMAE={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_msMAE={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # scheduler on validation MAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        # early stopping + save best\n",
        "        should_stop, saved = stopper.update(\n",
        "            va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch}\n",
        "        )\n",
        "        if saved:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt_path} (pMAE={stopper.best_save:.3f})\")\n",
        "        if should_stop:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (best val pMAE={stopper.best_save:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best before returning\n",
        "    state = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt_path\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, train_dl, val_dl, image_col, vf_cols, CKPT = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:41:26.754434Z",
          "iopub.status.busy": "2025-11-10T05:41:26.753893Z",
          "iopub.status.idle": "2025-11-10T05:41:30.190236Z",
          "shell.execute_reply": "2025-11-10T05:41:30.189451Z",
          "shell.execute_reply.started": "2025-11-10T05:41:26.754411Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFSvs1La0EBk",
        "outputId": "ac781fd5-e3ac-4401-e867-58683597057a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ROI predictions collected: torch.Size([127, 59]) torch.Size([127, 59])\n"
          ]
        }
      ],
      "source": [
        "# reload best ROI checkpoint\n",
        "best_roi = DenseNet201VF(out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "state = torch.load(CKPT, map_location=DEVICE)\n",
        "best_roi.load_state_dict(state[\"model\"])\n",
        "best_roi.eval()\n",
        "\n",
        "# collect predictions\n",
        "all_true = []\n",
        "all_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in val_dl:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        p = best_roi(x)\n",
        "\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "\n",
        "print(\"✅ ROI predictions collected:\", y_true.shape, y_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:41:33.799312Z",
          "iopub.status.busy": "2025-11-10T05:41:33.798656Z",
          "iopub.status.idle": "2025-11-10T05:41:33.808988Z",
          "shell.execute_reply": "2025-11-10T05:41:33.808148Z",
          "shell.execute_reply.started": "2025-11-10T05:41:33.799282Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZZ3DDdF0EBl",
        "outputId": "287dafa5-3d7d-4228-c1da-256b1faecd36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== ROI: POINTWISE ==\n",
            "RMSE: 9.5866\n",
            "MAE : 5.7747\n",
            "R²  : 0.9808\n",
            "\n",
            "== ROI: POINTWISE-MEAN / MS ==\n",
            "RMSE: 4.5056\n",
            "MAE : 3.4158\n",
            "R²  : 0.2520\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# helpers\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_val(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# POINTWISE\n",
        "pw_true = y_true.reshape(-1)\n",
        "pw_pred = y_pred.reshape(-1)\n",
        "\n",
        "print(\"\\n== ROI: POINTWISE ==\")\n",
        "print(f\"RMSE: {rmse(pw_true, pw_pred):.4f}\")\n",
        "print(f\"MAE : {mae_val(pw_true, pw_pred):.4f}\")\n",
        "print(f\"R²  : {r2(pw_true, pw_pred):.4f}\")\n",
        "\n",
        "# POINTWISE-MEAN / MS\n",
        "t_mean = y_true.mean(dim=1)\n",
        "p_mean = y_pred.mean(dim=1)\n",
        "\n",
        "print(\"\\n== ROI: POINTWISE-MEAN / MS ==\")\n",
        "print(f\"RMSE: {rmse(t_mean, p_mean):.4f}\")\n",
        "print(f\"MAE : {mae_val(t_mean, p_mean):.4f}\")\n",
        "print(f\"R²  : {r2(t_mean, p_mean):.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T10:00:28.759901Z",
          "iopub.status.busy": "2025-11-08T10:00:28.759605Z",
          "iopub.status.idle": "2025-11-08T10:00:28.775895Z",
          "shell.execute_reply": "2025-11-08T10:00:28.775339Z",
          "shell.execute_reply.started": "2025-11-08T10:00:28.759884Z"
        },
        "id": "X-KlSJ180EBl"
      },
      "source": [
        "# 3. ROI + OD/OD Segmentation DenseNet201"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:45:55.379375Z",
          "iopub.status.busy": "2025-11-10T05:45:55.378706Z",
          "iopub.status.idle": "2025-11-10T05:45:55.386060Z",
          "shell.execute_reply": "2025-11-10T05:45:55.385342Z",
          "shell.execute_reply.started": "2025-11-10T05:45:55.379352Z"
        },
        "id": "pIJ6bQpS0EBm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# ---- paths ----\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "# ---- training ----\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:45:58.655021Z",
          "iopub.status.busy": "2025-11-10T05:45:58.654715Z",
          "iopub.status.idle": "2025-11-10T05:45:58.665138Z",
          "shell.execute_reply": "2025-11-10T05:45:58.664425Z",
          "shell.execute_reply.started": "2025-11-10T05:45:58.655000Z"
        },
        "id": "QWztJOOV0EBm"
      },
      "outputs": [],
      "source": [
        "# ===================== CSV UTILS =====================\n",
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "IMAGE_COLS_CANDIDATES = [\"image\", \"image_name\", \"img\", \"image_path\", \"filename\", \"file\"]\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]) -> Tuple[str, List[str]]:\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # image col\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # VF cols prefer v1..v59 else numeric fallback\n",
        "    vf = [f\"v{i}\" for i in range(1, NUM_POINTS + 1)]\n",
        "    if all(c in cols for c in vf):\n",
        "        return image_col, vf\n",
        "\n",
        "    cand = []\n",
        "    for c in cols:\n",
        "        if c == image_col:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            cand.append(c)\n",
        "    if len(cand) < NUM_POINTS:\n",
        "        raise ValueError(f\"Need {NUM_POINTS} VF cols, found {len(cand)}.\")\n",
        "\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    cand = sorted(cand, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, cand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:02.177236Z",
          "iopub.status.busy": "2025-11-10T05:46:02.176683Z",
          "iopub.status.idle": "2025-11-10T05:46:02.187291Z",
          "shell.execute_reply": "2025-11-10T05:46:02.186682Z",
          "shell.execute_reply.started": "2025-11-10T05:46:02.177213Z"
        },
        "id": "7SMLRnSD0EBm"
      },
      "outputs": [],
      "source": [
        "# ===================== LABELME JSON → OD/OC MASKS =====================\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _poly_area(pts):\n",
        "    x = [p[0] for p in pts]\n",
        "    y = [p[1] for p in pts]\n",
        "    return 0.5 * abs(\n",
        "        sum(x[i] * y[(i + 1) % len(pts)] - x[(i + 1) % len(pts)] * y[i] for i in range(len(pts)))\n",
        "    )\n",
        "\n",
        "\n",
        "def _read_labelme(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        label = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = [(float(x), float(y)) for x, y in sh.get(\"points\", [])]\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if label in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        elif label in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=_poly_area)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=_poly_area)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json_path(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        cand = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(cand):\n",
        "            return cand\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def build_masks_from_labelme(img_pil: Image.Image, img_name: str, out_size: int):\n",
        "    W, H = img_pil.size\n",
        "    od_mask = Image.new(\"L\", (W, H), 0)\n",
        "    oc_mask = Image.new(\"L\", (W, H), 0)\n",
        "\n",
        "    jpath = _guess_json_path(img_name)\n",
        "    if jpath:\n",
        "        try:\n",
        "            od_polys, oc_polys = _read_labelme(jpath)\n",
        "            d_od = ImageDraw.Draw(od_mask)\n",
        "            d_oc = ImageDraw.Draw(oc_mask)\n",
        "            for poly in od_polys:\n",
        "                d_od.polygon(poly, outline=1, fill=1)\n",
        "            for poly in oc_polys:\n",
        "                d_oc.polygon(poly, outline=1, fill=1)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] parsing {jpath}: {e}\")\n",
        "\n",
        "    od_mask = od_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    oc_mask = oc_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    return od_mask, oc_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:05.752315Z",
          "iopub.status.busy": "2025-11-10T05:46:05.751834Z",
          "iopub.status.idle": "2025-11-10T05:46:05.759405Z",
          "shell.execute_reply": "2025-11-10T05:46:05.758759Z",
          "shell.execute_reply.started": "2025-11-10T05:46:05.752295Z"
        },
        "id": "s4EXBoZb0EBn"
      },
      "outputs": [],
      "source": [
        "# ===================== DATASET (5-channel RGB+OD+OC) =====================\n",
        "class ROI_OD_OC_Dataset(Dataset):\n",
        "    def __init__(self, rows, image_col, vf_cols, train=True, img_root=ROI_DIR, img_size=IMG_SIZE):\n",
        "        self.rows, self.image_col, self.vf_cols = rows, image_col, vf_cols\n",
        "        self.train, self.img_root, self.img_size = train, img_root, img_size\n",
        "\n",
        "        norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.rgb_tf = transforms.Compose(\n",
        "            [transforms.Resize((img_size, img_size)), transforms.ToTensor(), *aug, norm]\n",
        "        )\n",
        "        self.mask_tf = transforms.ToTensor()  # L → (1,H,W) float {0,1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        fn = r[self.image_col]\n",
        "        path = fn if os.path.isabs(fn) else os.path.join(self.img_root, fn)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        od_img, oc_img = build_masks_from_labelme(img, fn, self.img_size)\n",
        "\n",
        "        x_rgb = self.rgb_tf(img)  # (3,H,W)\n",
        "        x_od = self.mask_tf(od_img)  # (1,H,W)\n",
        "        x_oc = self.mask_tf(oc_img)  # (1,H,W)\n",
        "        x = torch.cat([x_rgb, x_od, x_oc], dim=0)  # (5,H,W)\n",
        "\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:11.357022Z",
          "iopub.status.busy": "2025-11-10T05:46:11.356328Z",
          "iopub.status.idle": "2025-11-10T05:46:11.363275Z",
          "shell.execute_reply": "2025-11-10T05:46:11.362574Z",
          "shell.execute_reply.started": "2025-11-10T05:46:11.356999Z"
        },
        "id": "P5jWMe1-0EBn"
      },
      "outputs": [],
      "source": [
        "# ===================== MODEL (ResNet-50 with 5-ch input) =====================\n",
        "class DenseNet201_5ch_VF(nn.Module):\n",
        "    def __init__(self, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        base = models.densenet201(\n",
        "            weights=models.DenseNet201_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        # adapt conv0: 3→5 channels (init extra channels with mean RGB weights)\n",
        "        old = base.features[0]\n",
        "        new = nn.Conv2d(\n",
        "            5,\n",
        "            old.out_channels,\n",
        "            kernel_size=old.kernel_size,\n",
        "            stride=old.stride,\n",
        "            padding=old.padding,\n",
        "            bias=(old.bias is not None),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            new.weight[:, :3, :, :] = old.weight\n",
        "            mean_w = old.weight.mean(dim=1, keepdim=True)\n",
        "            new.weight[:, 3:5, :, :] = mean_w.repeat(1, 2, 1, 1)\n",
        "            if old.bias is not None:\n",
        "                new.bias.copy_(old.bias)\n",
        "        base.features[0] = new\n",
        "\n",
        "        in_f = base.classifier.in_features\n",
        "        base.classifier = nn.Identity()\n",
        "        self.backbone = base\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25), nn.Linear(512, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x5):\n",
        "        f = self.backbone(x5)\n",
        "        return self.regressor(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:14.956708Z",
          "iopub.status.busy": "2025-11-10T05:46:14.955948Z",
          "iopub.status.idle": "2025-11-10T05:46:14.963048Z",
          "shell.execute_reply": "2025-11-10T05:46:14.962411Z",
          "shell.execute_reply.started": "2025-11-10T05:46:14.956686Z"
        },
        "id": "rYMJm7-h0EBn"
      },
      "outputs": [],
      "source": [
        "# ===================== METRICS + EPOCH LOOP =====================\n",
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:18.407433Z",
          "iopub.status.busy": "2025-11-10T05:46:18.406736Z",
          "iopub.status.idle": "2025-11-10T05:52:34.919034Z",
          "shell.execute_reply": "2025-11-10T05:52:34.918137Z",
          "shell.execute_reply.started": "2025-11-10T05:46:18.407410Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8FSDfHT0EBo",
        "outputId": "b57f9e01-a369-4574-e00a-bb1e8966635d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] 59 VF cols: ['AGE', 'CCT', 'IOP_y', 'Interval Years', 'MD'] ... ['VF50', 'VF51', 'VF52', 'VF53', 'VF54']\n",
            "Epoch 01 | train_loss=5365.3317  train_pMAE=28.253  train_MS=28.098 || val_loss=5187.6195  val_pMAE=26.280  val_MS=25.980\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC.pth (pMAE=26.280)\n",
            "\n",
            "Epoch 02 | train_loss=4869.0627  train_pMAE=24.017  train_MS=23.399 || val_loss=4197.4303  val_pMAE=16.380  val_MS=13.585\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC.pth (pMAE=16.380)\n",
            "\n",
            "Epoch 03 | train_loss=4086.5978  train_pMAE=15.577  train_MS=12.354 || val_loss=3715.0415  val_pMAE=14.236  val_MS=4.981\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC.pth (pMAE=14.236)\n",
            "\n",
            "Epoch 04 | train_loss=3415.9284  train_pMAE=13.477  train_MS=5.019 || val_loss=3137.9916  val_pMAE=12.748  val_MS=4.868\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC.pth (pMAE=12.748)\n",
            "\n",
            "Epoch 05 | train_loss=2746.2331  train_pMAE=12.357  train_MS=5.319 || val_loss=2408.0597  val_pMAE=11.449  val_MS=5.587\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC.pth (pMAE=11.449)\n",
            "\n",
            "Epoch 06 | train_loss=2070.0683  train_pMAE=11.697  train_MS=5.470 || val_loss=1795.7381  val_pMAE=10.631  val_MS=5.234\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC.pth (pMAE=10.631)\n",
            "\n",
            "Epoch 07 | train_loss=1457.2433  train_pMAE=11.160  train_MS=5.041 || val_loss=1159.7144  val_pMAE=9.521  val_MS=4.365\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC.pth (pMAE=9.521)\n",
            "\n",
            "Epoch 08 | train_loss=928.4738  train_pMAE=10.422  train_MS=4.411 || val_loss=679.1265  val_pMAE=8.514  val_MS=4.229\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC.pth (pMAE=8.514)\n",
            "\n",
            "Epoch 09 | train_loss=566.7626  train_pMAE=9.812  train_MS=4.026 || val_loss=380.3856  val_pMAE=7.625  val_MS=4.024\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC.pth (pMAE=7.625)\n",
            "\n",
            "Epoch 10 | train_loss=326.4294  train_pMAE=9.149  train_MS=3.669 || val_loss=237.0333  val_pMAE=7.049  val_MS=3.821\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC.pth (pMAE=7.049)\n",
            "\n",
            "Epoch 11 | train_loss=206.2525  train_pMAE=8.706  train_MS=3.622 || val_loss=159.5938  val_pMAE=6.613  val_MS=3.787\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC.pth (pMAE=6.613)\n",
            "\n",
            "Epoch 12 | train_loss=154.6554  train_pMAE=8.287  train_MS=3.330 || val_loss=115.1083  val_pMAE=6.222  val_MS=3.662\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC.pth (pMAE=6.222)\n",
            "\n",
            "Epoch 13 | train_loss=135.0100  train_pMAE=8.098  train_MS=3.380 || val_loss=102.3541  val_pMAE=5.822  val_MS=3.449\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC.pth (pMAE=5.822)\n",
            "\n",
            "Epoch 14 | train_loss=129.2286  train_pMAE=7.963  train_MS=3.237 || val_loss=99.8379  val_pMAE=6.032  val_MS=3.555\n",
            "Epoch 15 | train_loss=125.1215  train_pMAE=7.896  train_MS=3.284 || val_loss=97.7342  val_pMAE=6.158  val_MS=3.770\n",
            "Epoch 16 | train_loss=124.4377  train_pMAE=7.821  train_MS=3.282 || val_loss=100.4151  val_pMAE=6.272  val_MS=3.873\n",
            "Epoch 17 | train_loss=122.8943  train_pMAE=7.786  train_MS=3.282 || val_loss=99.7953  val_pMAE=6.212  val_MS=3.864\n",
            "Epoch 18 | train_loss=121.4363  train_pMAE=7.711  train_MS=3.300 || val_loss=97.3361  val_pMAE=5.953  val_MS=3.537\n",
            "Epoch 19 | train_loss=121.4348  train_pMAE=7.680  train_MS=3.220 || val_loss=96.1958  val_pMAE=5.940  val_MS=3.535\n",
            "Epoch 20 | train_loss=121.6377  train_pMAE=7.717  train_MS=3.331 || val_loss=96.5753  val_pMAE=5.953  val_MS=3.540\n",
            "Epoch 21 | train_loss=118.2996  train_pMAE=7.570  train_MS=3.176 || val_loss=93.1832  val_pMAE=5.843  val_MS=3.399\n",
            "Epoch 22 | train_loss=119.6232  train_pMAE=7.600  train_MS=3.282 || val_loss=93.3896  val_pMAE=5.867  val_MS=3.465\n",
            "Epoch 23 | train_loss=118.6228  train_pMAE=7.629  train_MS=3.311 || val_loss=93.3710  val_pMAE=5.861  val_MS=3.523\n",
            "Epoch 24 | train_loss=117.6488  train_pMAE=7.533  train_MS=3.275 || val_loss=93.6567  val_pMAE=5.904  val_MS=3.555\n",
            "Early stopping at epoch 24 (best val pMAE=5.822)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ---- (optional) reproducibility on small data\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---- Early Stopping helper\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best = float(\"inf\")\n",
        "        self.bad_epochs = 0\n",
        "\n",
        "    def step(self, val_metric, model, epoch_meta=None):\n",
        "        # returns True if we should stop\n",
        "        if val_metric < self.best - self.min_delta:\n",
        "            self.best = val_metric\n",
        "            self.bad_epochs = 0\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            return False\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "            return self.bad_epochs > self.patience\n",
        "\n",
        "\n",
        "# ===================== TRAIN =====================\n",
        "def train_resnet50_roi_odoc(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01):\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] {len(vf_cols)} VF cols: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    random.shuffle(rows)\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    train_ds = ROI_OD_OC_Dataset(train_rows, image_col, vf_cols, train=True)\n",
        "    val_ds = ROI_OD_OC_Dataset(val_rows, image_col, vf_cols, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = DenseNet201_5ch_VF(out_dim=NUM_POINTS, pretrained=True).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "    # ---- LR scheduler (plateau)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt = os.path.join(CHECK_DIR, \"best_densenet201_ROI_ODOC.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        # step scheduler on validation pMAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # save best (and detect improvement for pretty print)\n",
        "        prev_best = stopper.best\n",
        "        should_stop = stopper.step(va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch})\n",
        "        if stopper.best < prev_best - MIN_DELTA:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt} (pMAE={stopper.best:.3f})\\n\")\n",
        "\n",
        "        if should_stop:\n",
        "            print(f\"Early stopping at epoch {epoch} (best val pMAE={stopper.best:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best weights before returning\n",
        "    state = torch.load(ckpt, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt\n",
        "\n",
        "\n",
        "# run training and expose globals\n",
        "model_odoc, train_dl_odoc, val_dl_odoc, image_col_odoc, vf_cols_odoc, CKPT_ODOC = (\n",
        "    train_resnet50_roi_odoc(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:52:45.952603Z",
          "iopub.status.busy": "2025-11-10T05:52:45.951688Z",
          "iopub.status.idle": "2025-11-10T05:52:47.343625Z",
          "shell.execute_reply": "2025-11-10T05:52:47.342644Z",
          "shell.execute_reply.started": "2025-11-10T05:52:45.952569Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kntSJOPF0EBo",
        "outputId": "5e4837ed-f354-4b76-f69e-41b470233e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n",
            "\n",
            "== ROI+OD/OC: POINTWISE ==\n",
            "RMSE: 10.1170\n",
            "MAE : 5.8223\n",
            "R²  : 0.9786\n",
            "\n",
            "== ROI+OD/OC: POINTWISE-MEAN / MS ==\n",
            "RMSE: 5.0390\n",
            "MAE : 3.4491\n",
            "R²  : 0.0644\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ===================== EVALUATE BEST + PAPER METRICS =====================\n",
        "# reload best\n",
        "best_odoc = DenseNet201_5ch_VF(out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "state = torch.load(CKPT_ODOC, map_location=DEVICE)\n",
        "best_odoc.load_state_dict(state[\"model\"])\n",
        "best_odoc.eval()\n",
        "\n",
        "# predictions\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in val_dl_odoc:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        p = best_odoc(x)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)\n",
        "\n",
        "\n",
        "# metrics (safe names to avoid clobbering mae/ms_mae)\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_value(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "print(\"\\n== ROI+OD/OC: POINTWISE ==\")\n",
        "print(f\"RMSE: {rmse(pw_true, pw_pred):.4f}\")\n",
        "print(f\"MAE : {mae_value(pw_true, pw_pred):.4f}\")\n",
        "print(f\"R²  : {r2(pw_true, pw_pred):.4f}\")\n",
        "\n",
        "t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "print(\"\\n== ROI+OD/OC: POINTWISE-MEAN / MS ==\")\n",
        "print(f\"RMSE: {rmse(t_mean, p_mean):.4f}\")\n",
        "print(f\"MAE : {mae_value(t_mean, p_mean):.4f}\")\n",
        "print(f\"R²  : {r2(t_mean, p_mean):.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T10:04:17.910859Z",
          "iopub.status.busy": "2025-11-08T10:04:17.910580Z",
          "iopub.status.idle": "2025-11-08T10:04:17.914574Z",
          "shell.execute_reply": "2025-11-08T10:04:17.913924Z",
          "shell.execute_reply.started": "2025-11-08T10:04:17.910833Z"
        },
        "id": "kl2LIu5T0EBp"
      },
      "source": [
        "# 4. ROI + Clinical Features DenseNet201"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:04:44.866421Z",
          "iopub.status.busy": "2025-11-10T06:04:44.865721Z",
          "iopub.status.idle": "2025-11-10T06:04:44.873765Z",
          "shell.execute_reply": "2025-11-10T06:04:44.873117Z",
          "shell.execute_reply.started": "2025-11-10T06:04:44.866399Z"
        },
        "id": "CanpjEGn0EBp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "# ---- training ----\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59  # VF1..VF59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# clinical columns present in your CSV (plus computed ones)\n",
        "# from your columns: AGE, GENDER, IOP_y, MD exist; we’ll add computed CDR & PSD\n",
        "CLIN_NUM_COLS = [\"AGE\", \"IOP_y\", \"CDR\"]  # numeric\n",
        "CLIN_CAT_COLS = [\"GENDER\"]  # categorical (mapped to 0/1)\n",
        "IMAGE_COLS_CANDIDATES = [\n",
        "    \"Corresponding CFP\",\n",
        "    \"image\",\n",
        "    \"image_name\",\n",
        "    \"img\",\n",
        "    \"image_path\",\n",
        "    \"filename\",\n",
        "    \"file\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:04:52.367164Z",
          "iopub.status.busy": "2025-11-10T06:04:52.366737Z",
          "iopub.status.idle": "2025-11-10T06:04:52.378136Z",
          "shell.execute_reply": "2025-11-10T06:04:52.377479Z",
          "shell.execute_reply.started": "2025-11-10T06:04:52.367140Z"
        },
        "id": "2-_wDeCt0EBp"
      },
      "outputs": [],
      "source": [
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]) -> Tuple[str, List[str]]:\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # image column: prefer \"Corresponding CFP\" if present\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # prefer explicit VF1..VF59 (ignore VF0, VF60)\n",
        "    vf_cols_pref = [f\"VF{i}\" for i in range(1, 60)]\n",
        "    if all(c in cols for c in vf_cols_pref):\n",
        "        return image_col, vf_cols_pref\n",
        "\n",
        "    # fallback: numeric detection (exclude clinical & image)\n",
        "    excluded = set([image_col] + CLIN_NUM_COLS + CLIN_CAT_COLS + [\"VF0\", \"VF60\"])\n",
        "    candidates = []\n",
        "    for c in cols:\n",
        "        if c in excluded:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            candidates.append(c)\n",
        "\n",
        "    if len(candidates) < NUM_POINTS:\n",
        "        raise ValueError(\n",
        "            f\"Not enough numeric VF columns; found {len(candidates)}, need {NUM_POINTS}.\"\n",
        "        )\n",
        "\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    candidates_sorted = sorted(candidates, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, candidates_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:01.646023Z",
          "iopub.status.busy": "2025-11-10T06:05:01.645439Z",
          "iopub.status.idle": "2025-11-10T06:05:01.654860Z",
          "shell.execute_reply": "2025-11-10T06:05:01.654051Z",
          "shell.execute_reply.started": "2025-11-10T06:05:01.646002Z"
        },
        "id": "mQNdb4QH0EBp"
      },
      "outputs": [],
      "source": [
        "# ---- CDR from LabelMe polygons (vertical cup/disc ratio) ----\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _read_labelme_polys(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        lab = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = sh.get(\"points\", [])\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if lab in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        if lab in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "\n",
        "    # keep polygon with max vertical height if multiple\n",
        "    def vheight(poly):\n",
        "        ys = [p[1] for p in poly]\n",
        "        return (max(ys) - min(ys)) if ys else 0.0\n",
        "\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=vheight)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=vheight)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        p = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def compute_cdr_from_json(img_name: str):\n",
        "    \"\"\"\n",
        "    CDR = vertical height of OC / vertical height of OD.\n",
        "    Returns None if JSON missing or polygons absent.\n",
        "    \"\"\"\n",
        "    jpath = _guess_json(img_name)\n",
        "    if not jpath:\n",
        "        return None\n",
        "    try:\n",
        "        od_polys, oc_polys = _read_labelme_polys(jpath)\n",
        "        if not od_polys or not oc_polys:\n",
        "            return None\n",
        "\n",
        "        def vheight(poly):\n",
        "            ys = [float(y) for _, y in poly]\n",
        "            return max(ys) - min(ys) if ys else 0.0\n",
        "\n",
        "        h_od = vheight(od_polys[0])\n",
        "        h_oc = vheight(oc_polys[0])\n",
        "        if h_od <= 0:\n",
        "            return None\n",
        "        return float(h_oc / h_od)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] CDR parse failed for {img_name}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:08.135660Z",
          "iopub.status.busy": "2025-11-10T06:05:08.134961Z",
          "iopub.status.idle": "2025-11-10T06:05:08.139853Z",
          "shell.execute_reply": "2025-11-10T06:05:08.139232Z",
          "shell.execute_reply.started": "2025-11-10T06:05:08.135637Z"
        },
        "id": "B0bXMwlv0EBq"
      },
      "outputs": [],
      "source": [
        "def augment_rows_with_cdr(rows, image_col, vf_cols):\n",
        "    augmented = []\n",
        "    miss_cdr = miss_psd = 0\n",
        "    for r in rows:\n",
        "        r2 = dict(r)\n",
        "        # compute CDR from JSON polygons\n",
        "        cdr = compute_cdr_from_json(r2[image_col])\n",
        "        if cdr is None:\n",
        "            miss_cdr += 1\n",
        "        r2[\"CDR\"] = cdr\n",
        "\n",
        "        augmented.append(r2)\n",
        "    print(f\"✅ Augmented rows: CDR missing={miss_cdr}, PSD missing={miss_psd}\")\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Xda3GP6-0EBq"
      },
      "outputs": [],
      "source": [
        "# ----------------- AUGMENT ROWS WITH CDR  -----------------\n",
        "def augment_rows_with_cdr_psd(rows, image_col, vf_cols):\n",
        "    augmented = []\n",
        "    miss_cdr = miss_psd = 0\n",
        "    for r in rows:\n",
        "        r2 = dict(r)\n",
        "        cdr = compute_cdr_from_json(r2[image_col])\n",
        "        if cdr is None:\n",
        "            miss_cdr += 1\n",
        "        r2[\"CDR\"] = cdr\n",
        "\n",
        "        augmented.append(r2)\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:11.650178Z",
          "iopub.status.busy": "2025-11-10T06:05:11.649913Z",
          "iopub.status.idle": "2025-11-10T06:05:11.657823Z",
          "shell.execute_reply": "2025-11-10T06:05:11.657129Z",
          "shell.execute_reply.started": "2025-11-10T06:05:11.650160Z"
        },
        "id": "AI7Ya0--0EBr"
      },
      "outputs": [],
      "source": [
        "def to_float(x):\n",
        "    x = str(x).strip()\n",
        "    if x == \"\":\n",
        "        return None\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def fit_clinical_stats(rows, clin_num_cols):\n",
        "    stats = {}\n",
        "    for c in clin_num_cols:\n",
        "        vals = [to_float(r.get(c, \"\")) for r in rows]\n",
        "        vals = [v for v in vals if v is not None]\n",
        "        mean = np.mean(vals) if vals else 0.0\n",
        "        std = np.std(vals) if vals else 1.0\n",
        "        if std == 0:\n",
        "            std = 1.0\n",
        "        stats[c] = (float(mean), float(std))\n",
        "    return stats\n",
        "\n",
        "\n",
        "def encode_gender(x):\n",
        "    s = str(x).strip().lower()\n",
        "    if s in (\"m\", \"male\", \"man\"):\n",
        "        return 1.0\n",
        "    if s in (\"f\", \"female\", \"woman\"):\n",
        "        return 0.0\n",
        "    return 0.5  # unknown/other\n",
        "\n",
        "\n",
        "def build_clinical_vector(r, stats):\n",
        "    vec = []\n",
        "    for c in CLIN_NUM_COLS:\n",
        "        v = to_float(r.get(c, \"\"))\n",
        "        mean, std = stats[c]\n",
        "        v = mean if v is None else v\n",
        "        v = (v - mean) / std\n",
        "        vec.append(v)\n",
        "    for c in CLIN_CAT_COLS:\n",
        "        if c == \"GENDER\":\n",
        "            vec.append(encode_gender(r.get(c, \"\")))\n",
        "        else:\n",
        "            vec.append(0.0)\n",
        "    return torch.tensor(vec, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:17.437004Z",
          "iopub.status.busy": "2025-11-10T06:05:17.436286Z",
          "iopub.status.idle": "2025-11-10T06:05:17.443298Z",
          "shell.execute_reply": "2025-11-10T06:05:17.442595Z",
          "shell.execute_reply.started": "2025-11-10T06:05:17.436978Z"
        },
        "id": "rBIu_WHc0EBr"
      },
      "outputs": [],
      "source": [
        "class ROIClinicalDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows, image_col, vf_cols, clin_stats, train=True, img_root=ROI_DIR, img_size=IMG_SIZE\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.clin_stats = clin_stats\n",
        "        self.train = train\n",
        "        self.img_root = img_root\n",
        "\n",
        "        normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.tf = transforms.Compose(\n",
        "            [transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(), *aug, normalize]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.rows[idx]\n",
        "        name = r[self.image_col]\n",
        "        path = name if os.path.isabs(name) else os.path.join(self.img_root, name)\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        x_img = self.tf(img)\n",
        "\n",
        "        x_clin = build_clinical_vector(r, self.clin_stats)  # (clin_dim,)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)  # (59,)\n",
        "\n",
        "        return x_img, x_clin, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ZlBqfybD0EBs"
      },
      "outputs": [],
      "source": [
        "# ----------------- DATASET: 5-CH ROI + CLINICAL -----------------\n",
        "class ROI_ODOC_Clinical_Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows, image_col, vf_cols, clin_stats, train=True, img_root=ROI_DIR, img_size=IMG_SIZE\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.clin_stats = clin_stats\n",
        "        self.train = train\n",
        "        self.img_root = img_root\n",
        "        self.img_size = img_size\n",
        "\n",
        "        norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.rgb_tf = transforms.Compose(\n",
        "            [transforms.Resize((img_size, img_size)), transforms.ToTensor(), *aug, norm]\n",
        "        )\n",
        "        self.mask_tf = transforms.ToTensor()  # L→(1,H,W) float {0,1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.rows[idx]\n",
        "        fn = r[self.image_col]\n",
        "        path = fn if os.path.isabs(fn) else os.path.join(self.img_root, fn)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        od_img, oc_img = build_masks_from_labelme(img, fn, self.img_size)\n",
        "\n",
        "        x_rgb = self.rgb_tf(img)  # (3,H,W)\n",
        "        x_od = self.mask_tf(od_img)  # (1,H,W)\n",
        "        x_oc = self.mask_tf(oc_img)  # (1,H,W)\n",
        "        x5 = torch.cat([x_rgb, x_od, x_oc], dim=0)  # (5,H,W)\n",
        "\n",
        "        x_clin = build_clinical_vector(r, self.clin_stats)  # (clin_dim,)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)  # (59,)\n",
        "\n",
        "        return x5, x_clin, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:23.335737Z",
          "iopub.status.busy": "2025-11-10T06:05:23.335473Z",
          "iopub.status.idle": "2025-11-10T06:05:23.342292Z",
          "shell.execute_reply": "2025-11-10T06:05:23.341478Z",
          "shell.execute_reply.started": "2025-11-10T06:05:23.335717Z"
        },
        "id": "kKIVHulX0EBs"
      },
      "outputs": [],
      "source": [
        "class DenseNet201_ROI_Clinical(nn.Module):\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.densenet201(\n",
        "            weights=models.DenseNet201_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        in_f = self.backbone.classifier.in_features\n",
        "        self.backbone.classifier = nn.Identity()\n",
        "\n",
        "        self.img_head = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.10),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(512 + 64, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_img, x_clin):\n",
        "        f = self.backbone(x_img)  # (B, 2048)\n",
        "        f = self.img_head(f)  # (B, 512)\n",
        "        g = self.clin_head(x_clin)  # (B, 64)\n",
        "        z = torch.cat([f, g], dim=1)  # (B, 576)\n",
        "        out = self.fuse(z)  # (B, 59)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "w9dXzaT60EBs"
      },
      "outputs": [],
      "source": [
        "# ----------------- MODEL: 5-CH RESNET50 + CLINICAL MLP (FUSION) -----------------\n",
        "class DenseNet201_5ch_Clinical(nn.Module):\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        base = models.densenet201(\n",
        "            weights=models.DenseNet201_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        # adapt conv0: 3→5 channels (init extra channels with mean RGB weights)\n",
        "        old = base.features[0]\n",
        "        new = nn.Conv2d(\n",
        "            5,\n",
        "            old.out_channels,\n",
        "            kernel_size=old.kernel_size,\n",
        "            stride=old.stride,\n",
        "            padding=old.padding,\n",
        "            bias=(old.bias is not None),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            new.weight[:, :3, :, :] = old.weight\n",
        "            mean_w = old.weight.mean(dim=1, keepdim=True)\n",
        "            new.weight[:, 3:5, :, :] = mean_w.repeat(1, 2, 1, 1)\n",
        "            if old.bias is not None:\n",
        "                new.bias.copy_(old.bias)\n",
        "        base.features[0] = new\n",
        "\n",
        "        in_f = base.classifier.in_features\n",
        "        base.classifier = nn.Identity()\n",
        "        self.backbone = base\n",
        "\n",
        "        self.img_head = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25)\n",
        "        )\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.10),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(512 + 64, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x5, xclin):\n",
        "        f = self.backbone(x5)  # (B, 2048)\n",
        "        f = self.img_head(f)  # (B, 512)\n",
        "        g = self.clin_head(xclin)  # (B, 64)\n",
        "        z = torch.cat([f, g], dim=1)\n",
        "        out = self.fuse(z)  # (B, 59)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:28.785280Z",
          "iopub.status.busy": "2025-11-10T06:05:28.785012Z",
          "iopub.status.idle": "2025-11-10T06:05:28.792074Z",
          "shell.execute_reply": "2025-11-10T06:05:28.791195Z",
          "shell.execute_reply.started": "2025-11-10T06:05:28.785262Z"
        },
        "id": "mfQ4kjNS0EBs"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "    for x_img, x_clin, y in loader:\n",
        "        x_img = x_img.to(DEVICE)\n",
        "        x_clin = x_clin.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "        pred = model(x_img, x_clin)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x_img.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:52.866937Z",
          "iopub.status.busy": "2025-11-10T06:05:52.866250Z",
          "iopub.status.idle": "2025-11-10T06:09:44.041224Z",
          "shell.execute_reply": "2025-11-10T06:09:44.040337Z",
          "shell.execute_reply.started": "2025-11-10T06:05:52.866915Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZtVgGaj0EBt",
        "outputId": "5b7b9be0-4f31-4b31-f69c-c694d65ad171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] 59 VF cols: ['VF1', 'VF2', 'VF3', 'VF4', 'VF5'] ... ['VF55', 'VF56', 'VF57', 'VF58', 'VF59']\n",
            "\n",
            "Epoch 01 | train_loss=475.6793  train_pMAE=20.186  train_MS=20.031 || val_loss=390.2274  val_pMAE=18.058  val_MS=17.825\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC_CDR.pth (pMAE=18.058)\n",
            "\n",
            "Epoch 02 | train_loss=254.4340  train_pMAE=14.048  train_MS=13.436 || val_loss=64.2691  val_pMAE=6.253  val_MS=3.172\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC_CDR.pth (pMAE=6.253)\n",
            "\n",
            "Epoch 03 | train_loss=56.9249  train_pMAE=5.970  train_MS=3.376 || val_loss=59.3772  val_pMAE=6.499  val_MS=4.838\n",
            "\n",
            "Epoch 04 | train_loss=44.5226  train_pMAE=5.286  train_MS=2.835 || val_loss=36.2186  val_pMAE=4.571  val_MS=2.712\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC_CDR.pth (pMAE=4.571)\n",
            "\n",
            "Epoch 05 | train_loss=41.9070  train_pMAE=5.081  train_MS=2.552 || val_loss=37.8693  val_pMAE=4.859  val_MS=2.890\n",
            "\n",
            "Epoch 06 | train_loss=41.0263  train_pMAE=5.014  train_MS=2.473 || val_loss=34.1009  val_pMAE=4.201  val_MS=2.143\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC_CDR.pth (pMAE=4.201)\n",
            "\n",
            "Epoch 07 | train_loss=38.4280  train_pMAE=4.855  train_MS=2.249 || val_loss=35.2309  val_pMAE=4.349  val_MS=2.450\n",
            "\n",
            "Epoch 08 | train_loss=36.2827  train_pMAE=4.703  train_MS=1.951 || val_loss=34.9951  val_pMAE=4.374  val_MS=2.411\n",
            "\n",
            "Epoch 09 | train_loss=36.2604  train_pMAE=4.697  train_MS=1.972 || val_loss=35.8244  val_pMAE=4.295  val_MS=2.370\n",
            "\n",
            "Epoch 10 | train_loss=37.0108  train_pMAE=4.742  train_MS=2.159 || val_loss=36.1867  val_pMAE=4.189  val_MS=2.318\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC_CDR.pth (pMAE=4.189)\n",
            "\n",
            "Epoch 11 | train_loss=35.1035  train_pMAE=4.619  train_MS=1.952 || val_loss=35.3497  val_pMAE=4.330  val_MS=2.413\n",
            "\n",
            "Epoch 12 | train_loss=33.4579  train_pMAE=4.485  train_MS=1.722 || val_loss=36.8129  val_pMAE=4.343  val_MS=2.533\n",
            "\n",
            "Epoch 13 | train_loss=35.1089  train_pMAE=4.591  train_MS=1.989 || val_loss=33.6774  val_pMAE=4.303  val_MS=2.280\n",
            "\n",
            "Epoch 14 | train_loss=33.4865  train_pMAE=4.479  train_MS=1.835 || val_loss=37.0490  val_pMAE=4.277  val_MS=2.488\n",
            "\n",
            "Epoch 15 | train_loss=33.0892  train_pMAE=4.459  train_MS=1.757 || val_loss=34.0368  val_pMAE=4.126  val_MS=2.015\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_densenet201_ROI_ODOC_CDR.pth (pMAE=4.126)\n",
            "\n",
            "Epoch 16 | train_loss=32.5401  train_pMAE=4.392  train_MS=1.744 || val_loss=33.7769  val_pMAE=4.192  val_MS=2.145\n",
            "\n",
            "Epoch 17 | train_loss=31.7249  train_pMAE=4.325  train_MS=1.605 || val_loss=34.9082  val_pMAE=4.285  val_MS=2.407\n",
            "\n",
            "Epoch 18 | train_loss=31.7237  train_pMAE=4.326  train_MS=1.612 || val_loss=34.7573  val_pMAE=4.253  val_MS=2.403\n",
            "\n",
            "Epoch 19 | train_loss=31.3272  train_pMAE=4.308  train_MS=1.570 || val_loss=33.9694  val_pMAE=4.229  val_MS=2.196\n",
            "\n",
            "Epoch 20 | train_loss=31.9988  train_pMAE=4.368  train_MS=1.653 || val_loss=33.5227  val_pMAE=4.144  val_MS=2.128\n",
            "\n",
            "Epoch 21 | train_loss=30.8784  train_pMAE=4.285  train_MS=1.459 || val_loss=34.2595  val_pMAE=4.194  val_MS=2.274\n",
            "\n",
            "Epoch 22 | train_loss=32.1310  train_pMAE=4.378  train_MS=1.714 || val_loss=34.3041  val_pMAE=4.209  val_MS=2.263\n",
            "\n",
            "Epoch 23 | train_loss=31.7695  train_pMAE=4.328  train_MS=1.684 || val_loss=33.7759  val_pMAE=4.222  val_MS=2.234\n",
            "\n",
            "Epoch 24 | train_loss=30.2722  train_pMAE=4.229  train_MS=1.419 || val_loss=33.8551  val_pMAE=4.218  val_MS=2.223\n",
            "\n",
            "Epoch 25 | train_loss=30.4795  train_pMAE=4.229  train_MS=1.361 || val_loss=34.4206  val_pMAE=4.195  val_MS=2.226\n",
            "\n",
            "Epoch 26 | train_loss=31.1947  train_pMAE=4.299  train_MS=1.594 || val_loss=34.0256  val_pMAE=4.218  val_MS=2.243\n",
            "\n",
            "Early stopping at epoch 26 (best val pMAE=4.126)\n"
          ]
        }
      ],
      "source": [
        "def train_resnet50_roi_odoc_with_cdr(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01):\n",
        "    import os\n",
        "    import random\n",
        "\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    # --- Early stopper that saves best on val pMAE ---\n",
        "    class EarlyStopper:\n",
        "        def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "            self.patience = int(patience)\n",
        "            self.min_delta = float(min_delta)\n",
        "            self.ckpt_path = ckpt_path\n",
        "            self.best = float(\"inf\")\n",
        "            self.bad_epochs = 0\n",
        "\n",
        "        def step(self, val_pmae, model, epoch_meta=None):\n",
        "            if val_pmae < self.best - self.min_delta:\n",
        "                self.best = val_pmae\n",
        "                self.bad_epochs = 0\n",
        "                if self.ckpt_path:\n",
        "                    torch.save(\n",
        "                        {\n",
        "                            \"model\": model.state_dict(),\n",
        "                            \"val_pointwise_mae\": self.best,\n",
        "                            **(epoch_meta or {}),\n",
        "                        },\n",
        "                        self.ckpt_path,\n",
        "                    )\n",
        "                return False\n",
        "            else:\n",
        "                self.bad_epochs += 1\n",
        "                return self.bad_epochs > self.patience\n",
        "\n",
        "    # --- read & detect columns ---\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] {len(vf_cols)} VF cols: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # --- ADD CDR BEFORE SPLIT ---\n",
        "    rows = augment_rows_with_cdr_psd(rows, image_col, vf_cols)  # <-- adds CDR fields per row\n",
        "\n",
        "    random.shuffle(rows)\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    # --- fit clinical stats on TRAIN only (handles imputation/encoding) ---\n",
        "    clin_stats = fit_clinical_stats(train_rows, CLIN_NUM_COLS)\n",
        "    clin_dim = len(CLIN_NUM_COLS) + len(CLIN_CAT_COLS)\n",
        "\n",
        "    # --- datasets / loaders: use the dataset that returns (x5, x_clin, y) ---\n",
        "    train_ds = ROI_ODOC_Clinical_Dataset(train_rows, image_col, vf_cols, clin_stats, train=True)\n",
        "    val_ds = ROI_ODOC_Clinical_Dataset(val_rows, image_col, vf_cols, clin_stats, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # --- model that accepts 5ch image + clinical vector ---\n",
        "    model = DenseNet201_5ch_Clinical(clin_dim=clin_dim, out_dim=NUM_POINTS, pretrained=True).to(\n",
        "        DEVICE\n",
        "    )\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt = os.path.join(CHECK_DIR, \"best_densenet201_ROI_ODOC_CDR.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)  # should read (x5, x_clin, y) inside\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        print(\n",
        "            f\"\\nEpoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # step LR on validation pMAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        # save best & decide stopping\n",
        "        improved = va[\"pointwise_mae\"] < stopper.best - MIN_DELTA\n",
        "        should_stop = stopper.step(va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch})\n",
        "        if improved:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt} (pMAE={stopper.best:.3f})\")\n",
        "\n",
        "        if should_stop:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (best val pMAE={stopper.best:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best before returning\n",
        "    state = torch.load(ckpt, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt, clin_dim\n",
        "\n",
        "\n",
        "# run training and expose globals\n",
        "model_odoc, train_dl_odoc, val_dl_odoc, image_col_odoc, vf_cols_odoc, CKPT_ODOC, CLIN_DIM_OD = (\n",
        "    train_resnet50_roi_odoc_with_cdr(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:31:38.896271Z",
          "iopub.status.busy": "2025-11-10T06:31:38.895650Z",
          "iopub.status.idle": "2025-11-10T06:31:39.930251Z",
          "shell.execute_reply": "2025-11-10T06:31:39.929374Z",
          "shell.execute_reply.started": "2025-11-10T06:31:38.896246Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW6fRQBT0EBt",
        "outputId": "3f1fa1b5-7671-42f6-c115-6f9eb4696778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n",
            "\n",
            "== ROI+Clinical (with CDR): POINTWISE ==\n",
            "RMSE: 5.8341 | MAE: 4.1260 | R²: 0.5920\n",
            "\n",
            "== ROI+Clinical (with CDR): MEAN SENSITIVITY ==\n",
            "RMSE: 3.0735 | MAE: 2.0148 | R²: 0.7365\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------\n",
        "# ✅ Corrected Cell 10: Reload best model & evaluate\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Use the model already trained in Cell 9\n",
        "# model_odoc → best model returned by train_resnet50_roi_odoc_with_cdr\n",
        "# val_dl_odoc → validation dataloader\n",
        "# CKPT_ODOC → checkpoint path\n",
        "# CLIN_DIM_OD → clinical feature dimension\n",
        "# image_col_odoc, vf_cols_odoc already created\n",
        "\n",
        "best_roi_clin = model_odoc  # model already returned from training\n",
        "best_roi_clin.eval()\n",
        "\n",
        "# Load the best checkpoint\n",
        "state = torch.load(CKPT_ODOC, map_location=DEVICE)\n",
        "best_roi_clin.load_state_dict(state[\"model\"])\n",
        "best_roi_clin.eval()\n",
        "\n",
        "# Collect predictions\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x_img, x_clin, y in val_dl_odoc:\n",
        "        x_img = x_img.to(DEVICE)\n",
        "        x_clin = x_clin.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        p = best_roi_clin(x_img, x_clin)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)\n",
        "\n",
        "\n",
        "# --- Metrics ---\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_value(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# Pointwise metrics (VF1..VF59 flattened)\n",
        "pw_true = y_true.reshape(-1)\n",
        "pw_pred = y_pred.reshape(-1)\n",
        "\n",
        "print(\"\\n== ROI+Clinical (with CDR): POINTWISE ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(pw_true, pw_pred):.4f} | \"\n",
        "    f\"MAE: {mae_value(pw_true, pw_pred):.4f} | \"\n",
        "    f\"R²: {r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "\n",
        "# Mean Sensitivity metrics\n",
        "t_mean = y_true.mean(dim=1)\n",
        "p_mean = y_pred.mean(dim=1)\n",
        "\n",
        "print(\"\\n== ROI+Clinical (with CDR): MEAN SENSITIVITY ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | \"\n",
        "    f\"MAE: {mae_value(t_mean, p_mean):.4f} | \"\n",
        "    f\"R²: {r2(t_mean, p_mean):.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T10:08:07.655701Z",
          "iopub.status.busy": "2025-11-08T10:08:07.655425Z",
          "iopub.status.idle": "2025-11-08T10:08:07.659428Z"
        },
        "id": "xTv11YSr0EBu"
      },
      "source": [
        "# 5. ROI+ OD/OC + Clinical Dense201"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:33.300575Z",
          "iopub.status.busy": "2025-11-10T04:06:33.299912Z",
          "iopub.status.idle": "2025-11-10T04:06:33.432177Z",
          "shell.execute_reply": "2025-11-10T04:06:33.431611Z",
          "shell.execute_reply.started": "2025-11-10T04:06:33.300551Z"
        },
        "lines_to_next_cell": 2,
        "id": "39vOmsbL0EBu"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# FULL FUSION: ROI + OD/OC + Clinical   →  VF (59)\n",
        "# ==========================\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# ----------------- PATHS & CONFIG -----------------\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59  # VF1..VF59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:36.730860Z",
          "iopub.status.busy": "2025-11-10T04:06:36.730557Z",
          "iopub.status.idle": "2025-11-10T04:06:36.740736Z",
          "shell.execute_reply": "2025-11-10T04:06:36.740178Z",
          "shell.execute_reply.started": "2025-11-10T04:06:36.730834Z"
        },
        "id": "SYxhjJHd0EBu"
      },
      "outputs": [],
      "source": [
        "# Clinical features for fusion (present or computed)\n",
        "CLIN_NUM_COLS = [\"AGE\", \"IOP_y\", \"CDR\"]\n",
        "CLIN_CAT_COLS = [\"GENDER\"]\n",
        "IMAGE_COLS_CANDIDATES = [\n",
        "    \"Corresponding CFP\",\n",
        "    \"image\",\n",
        "    \"image_name\",\n",
        "    \"img\",\n",
        "    \"image_path\",\n",
        "    \"filename\",\n",
        "    \"file\",\n",
        "]\n",
        "\n",
        "\n",
        "# ----------------- CSV UTILS -----------------\n",
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]) -> Tuple[str, List[str]]:\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # image column (prefer Corresponding CFP)\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # prefer explicit VF1..VF59\n",
        "    vf_pref = [f\"VF{i}\" for i in range(1, 60)]\n",
        "    if all(c in cols for c in vf_pref):\n",
        "        return image_col, vf_pref\n",
        "\n",
        "    # fallback: detect numeric columns (exclude clinical & image & VF0/VF60)\n",
        "    excluded = set([image_col] + CLIN_NUM_COLS + CLIN_CAT_COLS + [\"VF0\", \"VF60\"])\n",
        "    cand = []\n",
        "    for c in cols:\n",
        "        if c in excluded:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            cand.append(c)\n",
        "    if len(cand) < NUM_POINTS:\n",
        "        raise ValueError(f\"Not enough numeric VF columns; found {len(cand)}, need {NUM_POINTS}.\")\n",
        "\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    cand = sorted(cand, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, cand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:40.269978Z",
          "iopub.status.busy": "2025-11-10T04:06:40.269252Z",
          "iopub.status.idle": "2025-11-10T04:06:40.280558Z",
          "shell.execute_reply": "2025-11-10T04:06:40.279748Z",
          "shell.execute_reply.started": "2025-11-10T04:06:40.269944Z"
        },
        "id": "m4mw1CgE0EBu"
      },
      "outputs": [],
      "source": [
        "# ----------------- OD/OC POLYGONS → CDR & MASKS -----------------\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _read_labelme_polys(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        lab = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = sh.get(\"points\", [])\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if lab in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        if lab in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "\n",
        "    # keep polygon with max vertical height\n",
        "    def vheight(poly):\n",
        "        ys = [p[1] for p in poly]\n",
        "        return (max(ys) - min(ys)) if ys else 0.0\n",
        "\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=vheight)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=vheight)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        p = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def compute_cdr_from_json(img_name: str):\n",
        "    \"\"\"CDR = vertical cup height / vertical disc height (from polygons).\"\"\"\n",
        "    jpath = _guess_json(img_name)\n",
        "    if not jpath:\n",
        "        return None\n",
        "    try:\n",
        "        od_polys, oc_polys = _read_labelme_polys(jpath)\n",
        "        if not od_polys or not oc_polys:\n",
        "            return None\n",
        "\n",
        "        def vheight(poly):\n",
        "            ys = [float(y) for _, y in poly]\n",
        "            return max(ys) - min(ys) if ys else 0.0\n",
        "\n",
        "        h_od = vheight(od_polys[0])\n",
        "        h_oc = vheight(oc_polys[0])\n",
        "        if h_od <= 0:\n",
        "            return None\n",
        "        return float(h_oc / h_od)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] CDR parse failed for {img_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def build_masks_from_labelme(img_pil: Image.Image, img_name: str, out_size: int):\n",
        "    \"\"\"Binary OD/OC masks (L mode), resized to out_size.\"\"\"\n",
        "    W, H = img_pil.size\n",
        "    od_mask = Image.new(\"L\", (W, H), 0)\n",
        "    oc_mask = Image.new(\"L\", (W, H), 0)\n",
        "    jpath = _guess_json(img_name)\n",
        "    if jpath:\n",
        "        try:\n",
        "            od_polys, oc_polys = _read_labelme_polys(jpath)\n",
        "            d_od = ImageDraw.Draw(od_mask)\n",
        "            d_oc = ImageDraw.Draw(oc_mask)\n",
        "            for poly in od_polys:\n",
        "                d_od.polygon(poly, outline=1, fill=1)\n",
        "            for poly in oc_polys:\n",
        "                d_oc.polygon(poly, outline=1, fill=1)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] mask parse {jpath}: {e}\")\n",
        "    od_mask = od_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    oc_mask = oc_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    return od_mask, oc_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:43.829520Z",
          "iopub.status.busy": "2025-11-10T04:06:43.828915Z",
          "iopub.status.idle": "2025-11-10T04:06:43.833579Z",
          "shell.execute_reply": "2025-11-10T04:06:43.832931Z",
          "shell.execute_reply.started": "2025-11-10T04:06:43.829498Z"
        },
        "id": "SrM_ozPQ0EBv"
      },
      "outputs": [],
      "source": [
        "# ----------------- AUGMENT ROWS WITH CDR  -----------------\n",
        "def augment_rows_with_cdr_psd(rows, image_col, vf_cols):\n",
        "    augmented = []\n",
        "    miss_cdr = miss_psd = 0\n",
        "    for r in rows:\n",
        "        r2 = dict(r)\n",
        "        cdr = compute_cdr_from_json(r2[image_col])\n",
        "        if cdr is None:\n",
        "            miss_cdr += 1\n",
        "        r2[\"CDR\"] = cdr\n",
        "\n",
        "        augmented.append(r2)\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:47.264094Z",
          "iopub.status.busy": "2025-11-10T04:06:47.263774Z",
          "iopub.status.idle": "2025-11-10T04:06:47.271273Z",
          "shell.execute_reply": "2025-11-10T04:06:47.270696Z",
          "shell.execute_reply.started": "2025-11-10T04:06:47.264074Z"
        },
        "id": "p2kQIhbo0EBv"
      },
      "outputs": [],
      "source": [
        "# ----------------- CLINICAL PREPROCESS -----------------\n",
        "def to_float(x):\n",
        "    x = str(x).strip()\n",
        "    if x == \"\":\n",
        "        return None\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def fit_clinical_stats(rows, clin_num_cols):\n",
        "    stats = {}\n",
        "    for c in clin_num_cols:\n",
        "        vals = [to_float(r.get(c, \"\")) for r in rows]\n",
        "        vals = [v for v in vals if v is not None]\n",
        "        mean = np.mean(vals) if vals else 0.0\n",
        "        std = np.std(vals) if vals else 1.0\n",
        "        if std == 0:\n",
        "            std = 1.0\n",
        "        stats[c] = (float(mean), float(std))\n",
        "    return stats\n",
        "\n",
        "\n",
        "def encode_gender(x):\n",
        "    s = str(x).strip().lower()\n",
        "    if s in (\"m\", \"male\", \"man\"):\n",
        "        return 1.0\n",
        "    if s in (\"f\", \"female\", \"woman\"):\n",
        "        return 0.0\n",
        "    return 0.5  # unknown/other\n",
        "\n",
        "\n",
        "def build_clinical_vector(r, stats):\n",
        "    vec = []\n",
        "    for c in CLIN_NUM_COLS:\n",
        "        v = to_float(r.get(c, \"\"))\n",
        "        mean, std = stats[c]\n",
        "        v = mean if v is None else v\n",
        "        v = (v - mean) / std\n",
        "        vec.append(v)\n",
        "    for c in CLIN_CAT_COLS:\n",
        "        if c == \"GENDER\":\n",
        "            vec.append(encode_gender(r.get(c, \"\")))\n",
        "        else:\n",
        "            vec.append(0.0)\n",
        "    return torch.tensor(vec, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:50.779769Z",
          "iopub.status.busy": "2025-11-10T04:06:50.779086Z",
          "iopub.status.idle": "2025-11-10T04:06:50.787036Z",
          "shell.execute_reply": "2025-11-10T04:06:50.786326Z",
          "shell.execute_reply.started": "2025-11-10T04:06:50.779743Z"
        },
        "id": "MbG4fuTf0EBv"
      },
      "outputs": [],
      "source": [
        "# ----------------- DATASET: 5-CH ROI + CLINICAL -----------------\n",
        "class ROI_ODOC_Clinical_Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows, image_col, vf_cols, clin_stats, train=True, img_root=ROI_DIR, img_size=IMG_SIZE\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.clin_stats = clin_stats\n",
        "        self.train = train\n",
        "        self.img_root = img_root\n",
        "        self.img_size = img_size\n",
        "\n",
        "        norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.rgb_tf = transforms.Compose(\n",
        "            [transforms.Resize((img_size, img_size)), transforms.ToTensor(), *aug, norm]\n",
        "        )\n",
        "        self.mask_tf = transforms.ToTensor()  # L→(1,H,W) float {0,1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.rows[idx]\n",
        "        fn = r[self.image_col]\n",
        "        path = fn if os.path.isabs(fn) else os.path.join(self.img_root, fn)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        od_img, oc_img = build_masks_from_labelme(img, fn, self.img_size)\n",
        "\n",
        "        x_rgb = self.rgb_tf(img)  # (3,H,W)\n",
        "        x_od = self.mask_tf(od_img)  # (1,H,W)\n",
        "        x_oc = self.mask_tf(oc_img)  # (1,H,W)\n",
        "        x5 = torch.cat([x_rgb, x_od, x_oc], dim=0)  # (5,H,W)\n",
        "\n",
        "        x_clin = build_clinical_vector(r, self.clin_stats)  # (clin_dim,)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)  # (59,)\n",
        "\n",
        "        return x5, x_clin, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:54.064124Z",
          "iopub.status.busy": "2025-11-10T04:06:54.063332Z",
          "iopub.status.idle": "2025-11-10T04:06:54.071929Z",
          "shell.execute_reply": "2025-11-10T04:06:54.071421Z",
          "shell.execute_reply.started": "2025-11-10T04:06:54.064093Z"
        },
        "id": "Uf8lsY6J0EBw"
      },
      "outputs": [],
      "source": [
        "# ----------------- MODEL: 5-CH RESNET50 + CLINICAL MLP (FUSION) -----------------\n",
        "class DenseNet201_5ch_Clinical(nn.Module):\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        base = models.densenet201(\n",
        "            weights=models.DenseNet201_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        # adapt conv0: 3→5 channels (init extra channels with mean RGB weights)\n",
        "        old = base.features[0]\n",
        "        new = nn.Conv2d(\n",
        "            5,\n",
        "            old.out_channels,\n",
        "            kernel_size=old.kernel_size,\n",
        "            stride=old.stride,\n",
        "            padding=old.padding,\n",
        "            bias=(old.bias is not None),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            new.weight[:, :3, :, :] = old.weight\n",
        "            mean_w = old.weight.mean(dim=1, keepdim=True)\n",
        "            new.weight[:, 3:5, :, :] = mean_w.repeat(1, 2, 1, 1)\n",
        "            if old.bias is not None:\n",
        "                new.bias.copy_(old.bias)\n",
        "        base.features[0] = new\n",
        "\n",
        "        in_f = base.classifier.in_features\n",
        "        base.classifier = nn.Identity()\n",
        "        self.backbone = base\n",
        "\n",
        "        self.img_head = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25)\n",
        "        )\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.10),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(512 + 64, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x5, xclin):\n",
        "        f = self.backbone(x5)  # (B, 2048)\n",
        "        f = self.img_head(f)  # (B, 512)\n",
        "        g = self.clin_head(xclin)  # (B, 64)\n",
        "        z = torch.cat([f, g], dim=1)\n",
        "        out = self.fuse(z)  # (B, 59)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:57.415483Z",
          "iopub.status.busy": "2025-11-10T04:06:57.414915Z",
          "iopub.status.idle": "2025-11-10T04:06:57.422155Z",
          "shell.execute_reply": "2025-11-10T04:06:57.421329Z",
          "shell.execute_reply.started": "2025-11-10T04:06:57.415462Z"
        },
        "id": "tXKXIsDr0EBw"
      },
      "outputs": [],
      "source": [
        "# ----------------- METRICS & EPOCH LOOP (same logic as your earlier code) -----------------\n",
        "@torch.no_grad()\n",
        "def mae(pred, true):  # pointwise MAE\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):  # MS per-sample then MAE\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "    for x5, xclin, y in loader:\n",
        "        x5, xclin, y = x5.to(DEVICE), xclin.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "        pred = model(x5, xclin)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x5.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:08:38.414507Z",
          "iopub.status.busy": "2025-11-10T04:08:38.414201Z",
          "iopub.status.idle": "2025-11-10T04:17:55.812102Z",
          "shell.execute_reply": "2025-11-10T04:17:55.810937Z",
          "shell.execute_reply.started": "2025-11-10T04:08:38.414480Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZQudJYw0EBw",
        "outputId": "b0147cb0-bcad-43af-8e2e-252234c3f043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] 59 VF cols detected: ['VF1', 'VF2', 'VF3', 'VF4', 'VF5'] ... ['VF55', 'VF56', 'VF57', 'VF58', 'VF59']\n",
            "Epoch 01 | train_loss=475.7823 train_pMAE=20.187 train_MS=20.031 || val_loss=394.6463 val_pMAE=18.162 val_MS=17.934\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=18.162)\n",
            "Epoch 02 | train_loss=250.8251 train_pMAE=13.869 train_MS=13.273 || val_loss=85.6430 val_pMAE=7.184 val_MS=4.746\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=7.184)\n",
            "Epoch 03 | train_loss=51.7986 train_pMAE=5.648 train_MS=2.751 || val_loss=42.5156 val_pMAE=5.225 val_MS=3.442\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=5.225)\n",
            "Epoch 04 | train_loss=42.3323 train_pMAE=5.105 train_MS=2.542 || val_loss=38.6398 val_pMAE=4.775 val_MS=2.873\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.775)\n",
            "Epoch 05 | train_loss=40.9362 train_pMAE=5.005 train_MS=2.406 || val_loss=35.9330 val_pMAE=4.430 val_MS=2.434\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.430)\n",
            "Epoch 06 | train_loss=39.7419 train_pMAE=4.930 train_MS=2.361 || val_loss=35.7390 val_pMAE=4.435 val_MS=2.311\n",
            "Epoch 07 | train_loss=37.8398 train_pMAE=4.779 train_MS=2.133 || val_loss=35.5626 val_pMAE=4.378 val_MS=2.280\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.378)\n",
            "Epoch 08 | train_loss=37.5314 train_pMAE=4.772 train_MS=2.134 || val_loss=36.0333 val_pMAE=4.515 val_MS=2.585\n",
            "Epoch 09 | train_loss=35.8772 train_pMAE=4.635 train_MS=1.943 || val_loss=34.6137 val_pMAE=4.223 val_MS=2.140\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.223)\n",
            "Epoch 10 | train_loss=35.8641 train_pMAE=4.671 train_MS=2.048 || val_loss=35.2459 val_pMAE=4.374 val_MS=2.359\n",
            "Epoch 11 | train_loss=34.3850 train_pMAE=4.527 train_MS=1.851 || val_loss=35.2685 val_pMAE=4.306 val_MS=2.263\n",
            "Epoch 12 | train_loss=33.7120 train_pMAE=4.481 train_MS=1.794 || val_loss=34.7197 val_pMAE=4.261 val_MS=2.201\n",
            "Epoch 13 | train_loss=34.0035 train_pMAE=4.525 train_MS=1.866 || val_loss=35.2726 val_pMAE=4.266 val_MS=2.226\n",
            "Epoch 14 | train_loss=33.9785 train_pMAE=4.496 train_MS=1.923 || val_loss=33.5927 val_pMAE=4.242 val_MS=2.011\n",
            "Epoch 15 | train_loss=33.1501 train_pMAE=4.449 train_MS=1.760 || val_loss=33.7710 val_pMAE=4.179 val_MS=2.058\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.179)\n",
            "Epoch 16 | train_loss=32.2564 train_pMAE=4.391 train_MS=1.638 || val_loss=33.6476 val_pMAE=4.259 val_MS=2.082\n",
            "Epoch 17 | train_loss=32.1687 train_pMAE=4.336 train_MS=1.636 || val_loss=34.0944 val_pMAE=4.206 val_MS=2.115\n",
            "Epoch 18 | train_loss=32.7161 train_pMAE=4.436 train_MS=1.782 || val_loss=34.4711 val_pMAE=4.402 val_MS=2.414\n",
            "Epoch 19 | train_loss=31.6561 train_pMAE=4.332 train_MS=1.537 || val_loss=34.6587 val_pMAE=4.254 val_MS=2.259\n",
            "Epoch 20 | train_loss=31.5185 train_pMAE=4.330 train_MS=1.559 || val_loss=34.1668 val_pMAE=4.280 val_MS=2.226\n",
            "Epoch 21 | train_loss=31.6589 train_pMAE=4.302 train_MS=1.606 || val_loss=34.0903 val_pMAE=4.272 val_MS=2.217\n",
            "Epoch 22 | train_loss=30.9436 train_pMAE=4.284 train_MS=1.541 || val_loss=34.2270 val_pMAE=4.260 val_MS=2.207\n",
            "Epoch 23 | train_loss=30.8811 train_pMAE=4.280 train_MS=1.449 || val_loss=34.0452 val_pMAE=4.257 val_MS=2.246\n",
            "Epoch 24 | train_loss=31.0965 train_pMAE=4.276 train_MS=1.572 || val_loss=33.9607 val_pMAE=4.292 val_MS=2.289\n",
            "Epoch 25 | train_loss=30.4640 train_pMAE=4.225 train_MS=1.451 || val_loss=34.2148 val_pMAE=4.220 val_MS=2.210\n",
            "Epoch 26 | train_loss=30.4221 train_pMAE=4.229 train_MS=1.431 || val_loss=33.3375 val_pMAE=4.207 val_MS=2.143\n",
            "Early stopping at epoch 26 (best val pMAE=4.179)\n"
          ]
        }
      ],
      "source": [
        "# ----------------- TRAIN -----------------\n",
        "def train_full_fusion(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01):\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] {len(vf_cols)} VF cols detected: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # augment with CDR BEFORE splitting\n",
        "    rows = augment_rows_with_cdr_psd(rows, image_col, vf_cols)\n",
        "\n",
        "    import random\n",
        "\n",
        "    random.shuffle(rows)\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    # fit clinical stats on train (handles None via mean imputation)\n",
        "    clin_stats = fit_clinical_stats(train_rows, CLIN_NUM_COLS)\n",
        "    clin_dim = len(CLIN_NUM_COLS) + len(CLIN_CAT_COLS)\n",
        "\n",
        "    # datasets / loaders\n",
        "    train_ds = ROI_ODOC_Clinical_Dataset(train_rows, image_col, vf_cols, clin_stats, train=True)\n",
        "    val_ds = ROI_ODOC_Clinical_Dataset(val_rows, image_col, vf_cols, clin_stats, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # model / optimizer\n",
        "    model = DenseNet201_5ch_Clinical(clin_dim=clin_dim, out_dim=NUM_POINTS, pretrained=True).to(\n",
        "        DEVICE\n",
        "    )\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    best = float(\"inf\")\n",
        "    no_improve = 0\n",
        "    ckpt = os.path.join(CHECK_DIR, \"best_full_fusion_ROI_ODOC_CLIN1.pth\")\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f} train_pMAE={tr['pointwise_mae']:.3f} train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f} val_pMAE={va['pointwise_mae']:.3f} val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # early stopping on val pointwise MAE\n",
        "        if va[\"pointwise_mae\"] < best - MIN_DELTA:\n",
        "            best = va[\"pointwise_mae\"]\n",
        "            no_improve = 0\n",
        "            torch.save(\n",
        "                {\"epoch\": epoch, \"model\": model.state_dict(), \"val_pointwise_mae\": best}, ckpt\n",
        "            )\n",
        "            print(f\"  -> saved new best to {ckpt} (val pMAE={best:.3f})\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve > PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch} (best val pMAE={best:.3f})\")\n",
        "                break\n",
        "\n",
        "    # expose objects / paths like before\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt, clin_dim\n",
        "\n",
        "\n",
        "# run training and expose globals\n",
        "model_full, train_dl_full, val_dl_full, image_col_full, vf_cols_full, CKPT_FULL, CLIN_DIM = (\n",
        "    train_full_fusion(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:18:02.710520Z",
          "iopub.status.busy": "2025-11-10T04:18:02.709945Z",
          "iopub.status.idle": "2025-11-10T04:18:04.452619Z",
          "shell.execute_reply": "2025-11-10T04:18:04.451838Z",
          "shell.execute_reply.started": "2025-11-10T04:18:02.710498Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukmYWMmZ0EBx",
        "outputId": "3d854477-b7b7-4bcd-9eac-e8e7d295cc52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n",
            "\n",
            "== FULL FUSION (ROI + OD/OC + Clinical ): POINTWISE ==\n",
            "RMSE: 5.8113 | MAE: 4.1793 | R²: 0.5951\n",
            "== FULL FUSION: POINTWISE-MEAN / MS ==\n",
            "RMSE: 3.0141 | MAE: 2.0579 | R²: 0.7465\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -------EVALUATE BEST + PRINT METRICS -----------------\n",
        "# reload best\n",
        "best_full = DenseNet201_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(\n",
        "    DEVICE\n",
        ")\n",
        "state = torch.load(CKPT_FULL, map_location=DEVICE)\n",
        "best_full.load_state_dict(state[\"model\"])\n",
        "best_full.eval()\n",
        "\n",
        "# collect preds\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x5, xclin, y in val_dl_full:\n",
        "        x5, xclin, y = x5.to(DEVICE), xclin.to(DEVICE), y.to(DEVICE)\n",
        "        p = best_full(x5, xclin)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)\n",
        "\n",
        "\n",
        "# paper-style metrics\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_value(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "print(\"\\n== FULL FUSION (ROI + OD/OC + Clinical ): POINTWISE ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(pw_true, pw_pred):.4f} | MAE: {mae_value(pw_true, pw_pred):.4f} | R²: {r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "\n",
        "t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "print(\"== FULL FUSION: POINTWISE-MEAN / MS ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | MAE: {mae_value(t_mean, p_mean):.4f} | R²: {r2(t_mean, p_mean):.4f}\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WpzA3H40EBx"
      },
      "source": [
        "# 6. ROI + OD/OC + Clinical SWIN-T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:20:07.695024Z",
          "iopub.status.busy": "2025-11-10T04:20:07.694736Z",
          "iopub.status.idle": "2025-11-10T04:20:07.713069Z",
          "shell.execute_reply": "2025-11-10T04:20:07.712345Z",
          "shell.execute_reply.started": "2025-11-10T04:20:07.695004Z"
        },
        "lines_to_next_cell": 2,
        "id": "eZMCCIt20EBx"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# SWIN-T for FULL FUSION (ROI + OD/OC + Clinical) → VF (59)\n",
        "# Reuses train_dl_full, val_dl_full, and CLIN_DIM from your existing setup\n",
        "# ==========================\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "# ---- metrics used inside the loop\n",
        "@torch.no_grad()\n",
        "def _mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def _run_epoch_ff(model, loader, device, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "\n",
        "    for batch in loader:\n",
        "        # x5: (B,5,H,W) ; x_clin: (B, CLIN_DIM) ; y: (B,59)\n",
        "        x5, x_clin, y = batch\n",
        "        x5 = x5.to(device, non_blocking=True)\n",
        "        x_clin = x_clin.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "        pred = model(x5, x_clin)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x5.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += _mae(pred, y) * bs\n",
        "        msmae_sum += _ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss_sum / max(1, n),\n",
        "        \"pointwise_mae\": pmae_sum / max(1, n),\n",
        "        \"ms_mae\": msmae_sum / max(1, n),\n",
        "    }\n",
        "\n",
        "\n",
        "# ---- Model: Swin-T (RGB) + small CNN for OD/OC masks + clinical MLP → fused regressor\n",
        "class SwinT_5ch_Clinical(nn.Module):\n",
        "    \"\"\"\n",
        "    Uses Swin-T on RGB (x5[:, :3]),\n",
        "    Encodes OD/OC masks (x5[:, 3:5]) with a lightweight CNN,\n",
        "    Encodes clinical features with an MLP,\n",
        "    Concats [swin_feat, mask_feat, clin_feat] → MLP → 59-D VF regression.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True, dropout=0.25):\n",
        "        super().__init__()\n",
        "        # Swin-T backbone (ImageNet weights) → (B, feat_dim)\n",
        "        self.backbone = models.swin_t(\n",
        "            weights=models.Swin_T_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        feat_dim = self.backbone.head.in_features\n",
        "        self.backbone.head = nn.Identity()  # keep pooled features\n",
        "\n",
        "        # OD/OC mask encoder (2xHxW → vector)\n",
        "        self.mask_enc = nn.Sequential(\n",
        "            nn.Conv2d(2, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),  # (B,128)\n",
        "        )\n",
        "        mask_dim = 128\n",
        "\n",
        "        # clinical encoder\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        clin_out = 64\n",
        "\n",
        "        # fusion & regressor\n",
        "        fused_in = feat_dim + mask_dim + clin_out\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(fused_in, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x5, xclin):\n",
        "        x_rgb = x5[:, :3, :, :]\n",
        "        x_msk = x5[:, 3:, :, :]\n",
        "        f_rgb = self.backbone(x_rgb)\n",
        "        f_msk = self.mask_enc(x_msk)\n",
        "        f_cln = self.clin_head(xclin)\n",
        "        z = torch.cat([f_rgb, f_msk, f_cln], dim=1)\n",
        "        return self.regressor(z)\n",
        "\n",
        "\n",
        "# ---- Early Stopping helper\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = patience\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best = float(\"inf\")\n",
        "        self.bad_epochs = 0\n",
        "\n",
        "    def step(self, current, model, epoch_meta=None):\n",
        "        if current < self.best - self.min_delta:\n",
        "            self.best = current\n",
        "            self.bad_epochs = 0\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            return False  # do not stop\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "            return self.bad_epochs > self.patience  # stop if exceeded\n",
        "\n",
        "\n",
        "# ---- Train\n",
        "def train_full_fusion_swinT(\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    clin_dim,\n",
        "    out_dim=59,\n",
        "    epochs=80,\n",
        "    lr=1e-4,\n",
        "    wd=1e-4,\n",
        "    device=\"cuda\",\n",
        "    pretrained=True,\n",
        "    patience=10,\n",
        "    min_delta=0.01,\n",
        "    ckpt_dir=\"./checkpoints\",\n",
        "):\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    ckpt = os.path.join(ckpt_dir, \"best_full_fusion_swint.pth\")\n",
        "\n",
        "    model = SwinT_5ch_Clinical(clin_dim=clin_dim, out_dim=out_dim, pretrained=pretrained).to(\n",
        "        device\n",
        "    )\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    # reduce LR when val pMAE plateaus\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    stopper = EarlyStopper(patience=patience, min_delta=min_delta, ckpt_path=ckpt)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        tr = _run_epoch_ff(model, train_loader, device, opt=opt)\n",
        "        va = _run_epoch_ff(model, val_loader, device, opt=None)\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f} train_pMAE={tr['pointwise_mae']:.3f} train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f} val_pMAE={va['pointwise_mae']:.3f} val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        should_stop = stopper.step(va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch})\n",
        "        if should_stop:\n",
        "            print(f\"Early stopping at epoch {epoch} (best val pMAE={stopper.best:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best\n",
        "    state = torch.load(ckpt, map_location=device)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:22:55.770223Z",
          "iopub.status.busy": "2025-11-10T04:22:55.769445Z",
          "iopub.status.idle": "2025-11-10T04:35:02.461704Z",
          "shell.execute_reply": "2025-11-10T04:35:02.461076Z",
          "shell.execute_reply.started": "2025-11-10T04:22:55.770197Z"
        },
        "lines_to_next_cell": 2,
        "id": "vy97MSzX0EBy"
      },
      "outputs": [],
      "source": [
        "# # ===== RUN: SWIN-T FULL-FUSION TRAIN + EVAL =====\n",
        "# from math import sqrt\n",
        "# import os\n",
        "\n",
        "# import torch\n",
        "\n",
        "# # ---- config (tweak if you like)\n",
        "# EPOCHS = 80\n",
        "# LR = 1e-4\n",
        "# WD = 1e-4\n",
        "# PATIENCE = 10\n",
        "# MIN_DELTA = 0.01\n",
        "# CHECK_DIR = \"./checkpoints\"\n",
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "# # ---- quick checks\n",
        "# assert \"train_dl_full\" in globals() and \"val_dl_full\" in globals(), (\n",
        "#     \"Missing loaders. Make sure you created train_dl_full and val_dl_full.\"\n",
        "# )\n",
        "# assert \"CLIN_DIM\" in globals(), \"Missing CLIN_DIM.\"\n",
        "# OUT_DIM = 59 if \"NUM_POINTS\" not in globals() else int(NUM_POINTS)\n",
        "\n",
        "# # ---- train\n",
        "# swinT_model, SWINT_CKPT = train_full_fusion_swinT(\n",
        "#     train_loader=train_dl_full,\n",
        "#     val_loader=val_dl_full,\n",
        "#     clin_dim=CLIN_DIM,\n",
        "#     out_dim=OUT_DIM,\n",
        "#     epochs=EPOCHS,\n",
        "#     lr=LR,\n",
        "#     wd=WD,\n",
        "#     device=DEVICE,\n",
        "#     patience=PATIENCE,\n",
        "#     min_delta=MIN_DELTA,\n",
        "#     ckpt_dir=CHECK_DIR,\n",
        "#     pretrained=True,\n",
        "# )\n",
        "\n",
        "# print(f\"\\n[OK] Training finished. Best checkpoint: {SWINT_CKPT}\")\n",
        "\n",
        "\n",
        "# # ---- evaluation helpers (pointwise + mean-of-points/“MS”)\n",
        "# @torch.no_grad()\n",
        "# def _rmse(a, b):\n",
        "#     return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def _mae_scalar(a, b):\n",
        "#     return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def _r2(a, b):\n",
        "#     ss_res = torch.sum((a - b) ** 2)\n",
        "#     ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "#     return float(1.0 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# # ---- load best and evaluate on val\n",
        "# best_swinT = SwinT_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=OUT_DIM, pretrained=False).to(DEVICE)\n",
        "# state = torch.load(SWINT_CKPT, map_location=DEVICE)\n",
        "# best_swinT.load_state_dict(state[\"model\"])\n",
        "# best_swinT.eval()\n",
        "\n",
        "# y_true, y_pred = [], []\n",
        "# with torch.no_grad():\n",
        "#     for x5, xclin, y in val_dl_full:\n",
        "#         x5 = x5.to(DEVICE)\n",
        "#         xclin = xclin.to(DEVICE)\n",
        "#         y = y.to(DEVICE)\n",
        "#         p = best_swinT(x5, xclin)\n",
        "#         y_true.append(y.cpu())\n",
        "#         y_pred.append(p.cpu())\n",
        "\n",
        "# y_true = torch.cat(y_true, dim=0)\n",
        "# y_pred = torch.cat(y_pred, dim=0)\n",
        "\n",
        "# # pointwise metrics\n",
        "# pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "# print(\"\\n== SWIN-T FULL FUSION: POINTWISE ==\")\n",
        "# print(\n",
        "#     f\"RMSE: {_rmse(pw_true, pw_pred):.4f} | MAE: {_mae_scalar(pw_true, pw_pred):.4f} | R²: {_r2(pw_true, pw_pred):.4f}\"\n",
        "# )\n",
        "\n",
        "# # mean-of-points (“MS”) metrics\n",
        "# t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "# print(\"== SWIN-T FULL FUSION: MEAN (MS) ==\")\n",
        "# print(\n",
        "#     f\"RMSE: {_rmse(t_mean, p_mean):.4f} | MAE: {_mae_scalar(t_mean, p_mean):.4f} | R²: {_r2(t_mean, p_mean):.4f}\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T11:59:52.375643Z",
          "iopub.status.busy": "2025-11-08T11:59:52.375380Z",
          "iopub.status.idle": "2025-11-08T11:59:52.379449Z",
          "shell.execute_reply": "2025-11-08T11:59:52.378654Z",
          "shell.execute_reply.started": "2025-11-08T11:59:52.375625Z"
        },
        "id": "cXGQO8NG0EBy"
      },
      "source": [
        "# 7. Weighted Averaging Ensemble Technique (DenseNet201 + SWIN-T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:36:47.133505Z",
          "iopub.status.busy": "2025-11-10T04:36:47.132776Z",
          "iopub.status.idle": "2025-11-10T04:36:50.037569Z",
          "shell.execute_reply": "2025-11-10T04:36:50.036982Z",
          "shell.execute_reply.started": "2025-11-10T04:36:47.133480Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSrbUItD0EBy",
        "outputId": "fe672bfb-47e4-43a8-e773-8b1eb2c0cbde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== INDIVIDUAL MODELS (pointwise) ==\n",
            "DenseNet  → RMSE: 5.8113 | MAE: 4.1793 | R²: 0.5951\n",
            "Swin-B  → RMSE: 5.8988 | MAE: 4.1052 | R²: 0.5829\n",
            "\n",
            "== ENSEMBLE (pointwise) ==\n",
            "Avg (α=0.50) → RMSE: 5.7192 | MAE: 4.0365 | R²: 0.6079\n",
            "\n",
            "== INDIVIDUAL MODELS (MS) ==\n",
            "DenseNet  → RMSE: 3.0141 | MAE: 2.0579 | R²: 0.7465\n",
            "Swin-B  → RMSE: 3.4719 | MAE: 2.4142 | R²: 0.6637\n",
            "\n",
            "== ENSEMBLE (MS) ==\n",
            "Avg (α=0.50) → RMSE: 3.1077 | MAE: 2.0992 | R²: 0.7306\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# Simple Ensemble: DenseNet201_5ch_Clinical + Swint_5ch_Clinical\n",
        "# Uses the SAME loaders (val_dl_full / test_dl_full) as your full-fusion setup\n",
        "# ==========================\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ---- your checkpoint paths\n",
        "RESNET_CKPT_PATH = CHECK_DIR + \"/best_full_fusion_ROI_ODOC_CLIN1.pth\"\n",
        "SWIN_CKPT_PATH = CHECK_DIR + \"/best_full_fusion_swint.pth\"\n",
        "\n",
        "\n",
        "# ---- small metric helpers (names won't clash with your existing ones)\n",
        "@torch.no_grad()\n",
        "def _rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _mae(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "def _load_model_states():\n",
        "    # Build the exact architectures (no pretrain needed when loading checkpoints)\n",
        "    resnet = DenseNet201_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(\n",
        "        DEVICE\n",
        "    )\n",
        "    swin = SwinT_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "\n",
        "    r_state = torch.load(RESNET_CKPT_PATH, map_location=DEVICE)\n",
        "    s_state = torch.load(SWIN_CKPT_PATH, map_location=DEVICE)\n",
        "\n",
        "    resnet.load_state_dict(r_state[\"model\"])\n",
        "    resnet.eval()\n",
        "    swin.load_state_dict(s_state[\"model\"])\n",
        "    swin.eval()\n",
        "    return resnet, swin\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ensemble_eval(loader, alpha=0.5):\n",
        "    \"\"\"\n",
        "    alpha: weight for SWIN (0..1). 0.5 = simple average\n",
        "    pred = (1-alpha)*resnet + alpha*swin\n",
        "    \"\"\"\n",
        "    assert 0.0 <= alpha <= 1.0\n",
        "    resnet, swin = _load_model_states()\n",
        "\n",
        "    y_true_chunks, y_pred_res_chunks, y_pred_swin_chunks, y_pred_ens_chunks = [], [], [], []\n",
        "\n",
        "    for x5, xclin, y in loader:\n",
        "        x5 = x5.to(DEVICE, non_blocking=True)\n",
        "        xcli = xclin.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        p_r = resnet(x5, xcli)  # (B, 59)\n",
        "        p_s = swin(x5, xcli)  # (B, 59)\n",
        "        p_e = (1.0 - alpha) * p_r + alpha * p_s\n",
        "\n",
        "        y_true_chunks.append(y.cpu())\n",
        "        y_pred_res_chunks.append(p_r.cpu())\n",
        "        y_pred_swin_chunks.append(p_s.cpu())\n",
        "        y_pred_ens_chunks.append(p_e.cpu())\n",
        "\n",
        "    y_true = torch.cat(y_true_chunks, dim=0)\n",
        "    p_res = torch.cat(y_pred_res_chunks, dim=0)\n",
        "    p_swin = torch.cat(y_pred_swin_chunks, dim=0)\n",
        "    p_ens = torch.cat(y_pred_ens_chunks, dim=0)\n",
        "\n",
        "    # pointwise metrics (flatten all 59 points)\n",
        "    pw_true, pw_res, pw_swin, pw_ens = (\n",
        "        y_true.reshape(-1),\n",
        "        p_res.reshape(-1),\n",
        "        p_swin.reshape(-1),\n",
        "        p_ens.reshape(-1),\n",
        "    )\n",
        "\n",
        "    print(\"\\n== INDIVIDUAL MODELS (pointwise) ==\")\n",
        "    print(\n",
        "        f\"DenseNet  → RMSE: {_rmse(pw_true, pw_res):.4f} | MAE: {_mae(pw_true, pw_res):.4f} | R²: {_r2(pw_true, pw_res):.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Swin-B  → RMSE: {_rmse(pw_true, pw_swin):.4f} | MAE: {_mae(pw_true, pw_swin):.4f} | R²: {_r2(pw_true, pw_swin):.4f}\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n== ENSEMBLE (pointwise) ==\")\n",
        "    print(\n",
        "        f\"Avg (α={alpha:.2f}) → RMSE: {_rmse(pw_true, pw_ens):.4f} | MAE: {_mae(pw_true, pw_ens):.4f} | R²: {_r2(pw_true, pw_ens):.4f}\"\n",
        "    )\n",
        "\n",
        "    # MS metrics = mean of 59 points per sample\n",
        "    t_mean, r_mean, s_mean, e_mean = (\n",
        "        y_true.mean(dim=1),\n",
        "        p_res.mean(dim=1),\n",
        "        p_swin.mean(dim=1),\n",
        "        p_ens.mean(dim=1),\n",
        "    )\n",
        "\n",
        "    print(\"\\n== INDIVIDUAL MODELS (MS) ==\")\n",
        "    print(\n",
        "        f\"DenseNet  → RMSE: {_rmse(t_mean, r_mean):.4f} | MAE: {_mae(t_mean, r_mean):.4f} | R²: {_r2(t_mean, r_mean):.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Swin-B  → RMSE: {_rmse(t_mean, s_mean):.4f} | MAE: {_mae(t_mean, s_mean):.4f} | R²: {_r2(t_mean, s_mean):.4f}\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n== ENSEMBLE (MS) ==\")\n",
        "    print(\n",
        "        f\"Avg (α={alpha:.2f}) → RMSE: {_rmse(t_mean, e_mean):.4f} | MAE: {_mae(t_mean, e_mean):.4f} | R²: {_r2(t_mean, e_mean):.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"y_true\": y_true,\n",
        "        \"pred_resnet\": p_res,\n",
        "        \"pred_swin\": p_swin,\n",
        "        \"pred_ensemble\": p_ens,\n",
        "    }\n",
        "\n",
        "\n",
        "# ===== RUN on your existing loaders =====\n",
        "# Use val set:\n",
        "assert \"val_dl_full\" in globals(), \"val_dl_full not found. Run your full-fusion data cell first.\"\n",
        "_ = ensemble_eval(val_dl_full, alpha=0.5)  # try alpha=0.3, 0.7, etc.\n",
        "\n",
        "# If you also have test_dl_full:\n",
        "# assert 'test_dl_full' in globals()\n",
        "# _ = ensemble_eval(test_dl_full, alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:37:07.068710Z",
          "iopub.status.busy": "2025-11-10T04:37:07.068455Z",
          "iopub.status.idle": "2025-11-10T04:37:09.933957Z",
          "shell.execute_reply": "2025-11-10T04:37:09.933312Z",
          "shell.execute_reply.started": "2025-11-10T04:37:07.068691Z"
        },
        "lines_to_next_cell": 2,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Phw7u30EBz",
        "outputId": "edefef93-0df3-45ff-fd83-069a231f2761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Ensemble] best alpha=0.60 (weights: SWIN=0.60, DENSENET=0.40)\n",
            "POINTWISE:\n",
            "  RMSE=5.7337 | MAE=4.0335 | R²=0.6059\n",
            "MS (mean sensitivity):\n",
            "  RMSE=3.1609 | MAE=2.1410 | R²=0.7213\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# SIMPLE ENSEMBLE: ResNet (full fusion) + Swin-B (full fusion)\n",
        "# Grid-search alpha on VAL to weight SWIN higher/lower\n",
        "# ==========================\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# --- Metrics (same style you used)\n",
        "@torch.no_grad()\n",
        "def _rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _mae(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# --- Load both models\n",
        "RESNET_CKPT = CHECK_DIR + \"/best_full_fusion_ROI_ODOC_CLIN1.pth\"\n",
        "SWIN_CKPT = CHECK_DIR + \"/best_full_fusion_swint.pth\"\n",
        "\n",
        "resnet = DenseNet201_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(\n",
        "    DEVICE\n",
        ")\n",
        "swin = SwinT_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "\n",
        "resnet.load_state_dict(torch.load(RESNET_CKPT, map_location=DEVICE)[\"model\"])\n",
        "swin.load_state_dict(torch.load(SWIN_CKPT, map_location=DEVICE)[\"model\"])\n",
        "resnet.eval()\n",
        "swin.eval()\n",
        "\n",
        "# --- Collect full VAL predictions once for speed\n",
        "y_true_list, pred_resnet_list, pred_swin_list = [], [], []\n",
        "with torch.no_grad():\n",
        "    for x5, xclin, y in val_dl_full:\n",
        "        x5 = x5.to(DEVICE)\n",
        "        xclin = xclin.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        pr = resnet(x5, xclin)\n",
        "        ps = swin(x5, xclin)\n",
        "        y_true_list.append(y.cpu())\n",
        "        pred_resnet_list.append(pr.cpu())\n",
        "        pred_swin_list.append(ps.cpu())\n",
        "\n",
        "y_true = torch.cat(y_true_list, dim=0)  # (N, 59)\n",
        "pred_r = torch.cat(pred_resnet_list, dim=0)  # (N, 59)\n",
        "pred_s = torch.cat(pred_swin_list, dim=0)  # (N, 59)\n",
        "\n",
        "# --- Grid-search alpha in [0,1] to minimize pointwise MAE (you can switch to MS if preferred)\n",
        "best_alpha, best_mae = None, float(\"inf\")\n",
        "for a in [i / 20 for i in range(21)]:  # 0.00, 0.05, ..., 1.00\n",
        "    ens = a * pred_s + (1 - a) * pred_r\n",
        "    mae_pw = _mae(ens.reshape(-1), y_true.reshape(-1))\n",
        "    if mae_pw < best_mae:\n",
        "        best_mae = mae_pw\n",
        "        best_alpha = a\n",
        "\n",
        "# --- Final ensemble metrics with the chosen alpha\n",
        "ens = best_alpha * pred_s + (1 - best_alpha) * pred_r\n",
        "pw_true, pw_pred = y_true.reshape(-1), ens.reshape(-1)\n",
        "print(\n",
        "    f\"\\n[Ensemble] best alpha={best_alpha:.2f} (weights: SWIN={best_alpha:.2f}, DENSENET={(1 - best_alpha):.2f})\"\n",
        ")\n",
        "print(\"POINTWISE:\")\n",
        "print(\n",
        "    f\"  RMSE={_rmse(pw_true, pw_pred):.4f} | MAE={_mae(pw_true, pw_pred):.4f} | R²={_r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "t_mean, p_mean = y_true.mean(dim=1), ens.mean(dim=1)\n",
        "print(\"MS (mean sensitivity):\")\n",
        "print(\n",
        "    f\"  RMSE={_rmse(t_mean, p_mean):.4f} | MAE={_mae(t_mean, p_mean):.4f} | R²={_r2(t_mean, p_mean):.4f}\"\n",
        ")\n",
        "\n",
        "# Save alpha if you want to reuse for test-time ensembling\n",
        "BEST_ALPHA = best_alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:35:33.870233Z",
          "iopub.status.busy": "2025-11-10T06:35:33.869481Z",
          "iopub.status.idle": "2025-11-10T06:35:34.053693Z",
          "shell.execute_reply": "2025-11-10T06:35:34.052990Z",
          "shell.execute_reply.started": "2025-11-10T06:35:33.870203Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4SaoYON0EBz",
        "outputId": "a67df02f-1cd8-4013-8d51-7af08263b848"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_densenet201_original_cfp.pth  best_densenet201_ROI_ODOC.pth\n",
            "best_densenet201_original_roi.pth  best_full_fusion_ROI_ODOC_CLIN1.pth\n",
            "best_densenet201_ROI_ODOC_CDR.pth  best_full_fusion_swint.pth\n"
          ]
        }
      ],
      "source": [
        "!ls $CHECK_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:37:19.823311Z",
          "iopub.status.busy": "2025-11-10T06:37:19.823011Z",
          "iopub.status.idle": "2025-11-10T06:37:49.186221Z",
          "shell.execute_reply": "2025-11-10T06:37:49.185475Z",
          "shell.execute_reply.started": "2025-11-10T06:37:19.823284Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ktZ9O5oT0EBz",
        "outputId": "e12065ac-5abe-4c17-aa19-295ae6e2582d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/final_checkpoints_archive.zip"
            ],
            "text/html": [
              "<a href='final_checkpoints_archive.zip' target='_blank'>final_checkpoints_archive.zip</a><br>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "from IPython.display import FileLink\n",
        "\n",
        "shutil.make_archive(\"final_checkpoints_archive\", \"zip\", CHECK_DIR)\n",
        "FileLink(\"final_checkpoints_archive.zip\")"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "formats": "ipynb,py:percent"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 8671492,
          "sourceId": 13641781,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}