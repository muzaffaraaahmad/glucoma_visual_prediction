{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:47:26.641075Z",
          "iopub.status.busy": "2025-11-08T09:47:26.640756Z",
          "iopub.status.idle": "2025-11-08T09:47:26.659110Z",
          "shell.execute_reply": "2025-11-08T09:47:26.658596Z",
          "shell.execute_reply.started": "2025-11-08T09:47:26.641051Z"
        },
        "id": "Ggvez8Fzil_b"
      },
      "source": [
        "# 1. CFP Images MobileNetV3L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:12.542505Z",
          "iopub.status.busy": "2025-11-10T05:17:12.542179Z",
          "iopub.status.idle": "2025-11-10T05:17:12.550563Z",
          "shell.execute_reply": "2025-11-10T05:17:12.550006Z",
          "shell.execute_reply.started": "2025-11-10T05:17:12.542470Z"
        },
        "id": "a0BCQ2tpil_e"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "CSV_PATH = \"./filtered_glaucoma.csv\"\n",
        "IMG_ROOT = \"./glaucoma_data/CFPs\"  # <-- CFP images folder\n",
        "CHECK_DIR = \"./checkpoints\"\n",
        "CFP_DIR = \"./glaucoma_data/ROI images\"  # <-- ROI images folder\n",
        "ROI_DIR = \"./glaucoma_data/ROI images\"  # ROI images folder\n",
        "JSON_DIR = \"./glaucoma_data/json\"  # LabelMe JSON files matching image names\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59\n",
        "PATIENCE = 10\n",
        "MIN_DELTA = 0.01\n",
        "\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9-dNUmQjil_h"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(\"./glaucoma_data.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"./\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:15.582725Z",
          "iopub.status.busy": "2025-11-10T05:17:15.582037Z",
          "iopub.status.idle": "2025-11-10T05:17:15.592790Z",
          "shell.execute_reply": "2025-11-10T05:17:15.592080Z",
          "shell.execute_reply.started": "2025-11-10T05:17:15.582701Z"
        },
        "id": "wi5YZQUeil_j"
      },
      "outputs": [],
      "source": [
        "# ---- tiny CSV reader (no pandas) ------------------------------------------------\n",
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]  # ensure equal length\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ---- detect columns --------------------------------------------------------------\n",
        "IMAGE_COLS_CANDIDATES = [\"image\", \"image_name\", \"img\", \"image_path\", \"filename\", \"file\"]\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]):\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # find image column\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # pick 59 VF columns: prefer v1..v59\n",
        "    vf_cols = [f\"v{i}\" for i in range(1, NUM_POINTS + 1)]\n",
        "    if all(c in cols for c in vf_cols):\n",
        "        return image_col, vf_cols\n",
        "\n",
        "    # fallback: numeric columns\n",
        "    candidates = []\n",
        "    for c in cols:\n",
        "        if c == image_col:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            candidates.append(c)\n",
        "\n",
        "    if len(candidates) < NUM_POINTS:\n",
        "        raise ValueError(\"Not enough numeric VF columns detected.\")\n",
        "\n",
        "    # sort by trailing number if exists\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    candidates_sorted = sorted(candidates, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, candidates_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:18.937424Z",
          "iopub.status.busy": "2025-11-10T05:17:18.936885Z",
          "iopub.status.idle": "2025-11-10T05:17:18.944011Z",
          "shell.execute_reply": "2025-11-10T05:17:18.943261Z",
          "shell.execute_reply.started": "2025-11-10T05:17:18.937404Z"
        },
        "id": "8l1m3vwIil_m"
      },
      "outputs": [],
      "source": [
        "class CFPDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows: List[Dict[str, str]], image_col: str, vf_cols: List[str], train: bool\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.train = train\n",
        "\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "\n",
        "        self.tf = transforms.Compose(\n",
        "            [transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(), *aug, normalize]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        name = r[self.image_col]\n",
        "\n",
        "        path = name\n",
        "        if not os.path.isabs(path):\n",
        "            if os.path.basename(path) == path:\n",
        "                path = os.path.join(IMG_ROOT, path)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        x = self.tf(img)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:22.158208Z",
          "iopub.status.busy": "2025-11-10T05:17:22.157482Z",
          "iopub.status.idle": "2025-11-10T05:17:22.163262Z",
          "shell.execute_reply": "2025-11-10T05:17:22.162535Z",
          "shell.execute_reply.started": "2025-11-10T05:17:22.158181Z"
        },
        "id": "UHG0vuzQil_o"
      },
      "outputs": [],
      "source": [
        "class MobileNetV3LVF(nn.Module):\n",
        "    def __init__(self, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.mobilenet_v3_large(\n",
        "            weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        )\n",
        "        in_f = self.backbone.classifier[-1].in_features\n",
        "        self.backbone.classifier[-1] = nn.Identity()\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25), nn.Linear(512, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x)\n",
        "        return self.regressor(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:25.577607Z",
          "iopub.status.busy": "2025-11-10T05:17:25.577128Z",
          "iopub.status.idle": "2025-11-10T05:17:25.584146Z",
          "shell.execute_reply": "2025-11-10T05:17:25.583424Z",
          "shell.execute_reply.started": "2025-11-10T05:17:25.577575Z"
        },
        "id": "XKqFCT9iil_p"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = crit(pred, y)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        n += bs\n",
        "\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:17:29.978088Z",
          "iopub.status.busy": "2025-11-10T05:17:29.977820Z",
          "iopub.status.idle": "2025-11-10T05:28:21.822456Z",
          "shell.execute_reply": "2025-11-10T05:28:21.821528Z",
          "shell.execute_reply.started": "2025-11-10T05:17:29.978068Z"
        },
        "id": "ZlKmkA4eil_r",
        "lines_to_next_cell": 2,
        "outputId": "f6012a1c-6c31-416a-f9ff-870d1127dc93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] using 59 VF columns: ['AGE', 'CCT', 'IOP_y', 'Interval Years', 'MD'] ... ['VF50', 'VF51', 'VF52', 'VF53', 'VF54']\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-5c1a4163.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-5c1a4163.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100%|██████████| 21.1M/21.1M [00:00<00:00, 180MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 01 | train_loss=5404.9284  train_pMAE=28.422  train_msMAE=28.259 || val_loss=5333.1244  val_pMAE=27.152  val_msMAE=26.899\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_cfp.pth (pMAE=27.152)\n",
            "\n",
            "Epoch 02 | train_loss=3867.5985  train_pMAE=20.059  train_msMAE=13.692 || val_loss=2189.8170  val_pMAE=26.853  val_msMAE=15.710\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_cfp.pth (pMAE=26.853)\n",
            "\n",
            "Epoch 03 | train_loss=1062.7709  train_pMAE=15.617  train_msMAE=4.944 || val_loss=397.3284  val_pMAE=10.038  val_msMAE=7.261\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_cfp.pth (pMAE=10.038)\n",
            "\n",
            "Epoch 04 | train_loss=301.3700  train_pMAE=13.165  train_msMAE=3.995 || val_loss=191.8438  val_pMAE=7.447  val_msMAE=4.968\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_cfp.pth (pMAE=7.447)\n",
            "\n",
            "Epoch 05 | train_loss=265.4436  train_pMAE=12.205  train_msMAE=3.822 || val_loss=129.9627  val_pMAE=6.882  val_msMAE=4.337\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_cfp.pth (pMAE=6.882)\n",
            "\n",
            "Epoch 06 | train_loss=248.6945  train_pMAE=11.964  train_msMAE=3.533 || val_loss=133.5561  val_pMAE=6.886  val_msMAE=4.343\n",
            "\n",
            "Epoch 07 | train_loss=241.5917  train_pMAE=11.725  train_msMAE=3.639 || val_loss=128.5760  val_pMAE=6.721  val_msMAE=4.278\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_cfp.pth (pMAE=6.721)\n",
            "\n",
            "Epoch 08 | train_loss=226.1644  train_pMAE=11.317  train_msMAE=3.599 || val_loss=111.3375  val_pMAE=5.940  val_msMAE=3.395\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_cfp.pth (pMAE=5.940)\n",
            "\n",
            "Epoch 09 | train_loss=218.0658  train_pMAE=11.097  train_msMAE=3.521 || val_loss=105.0580  val_pMAE=5.866  val_msMAE=3.272\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_cfp.pth (pMAE=5.866)\n",
            "\n",
            "Epoch 10 | train_loss=211.9508  train_pMAE=10.905  train_msMAE=3.514 || val_loss=105.0029  val_pMAE=6.198  val_msMAE=3.636\n",
            "\n",
            "Epoch 11 | train_loss=204.2404  train_pMAE=10.695  train_msMAE=3.471 || val_loss=102.8264  val_pMAE=6.138  val_msMAE=3.596\n",
            "\n",
            "Epoch 12 | train_loss=196.6721  train_pMAE=10.461  train_msMAE=3.461 || val_loss=102.5239  val_pMAE=6.025  val_msMAE=3.546\n",
            "\n",
            "Epoch 13 | train_loss=188.5625  train_pMAE=10.238  train_msMAE=3.468 || val_loss=99.9833  val_pMAE=5.937  val_msMAE=3.361\n",
            "\n",
            "Epoch 14 | train_loss=186.5173  train_pMAE=10.179  train_msMAE=3.574 || val_loss=99.4582  val_pMAE=5.759  val_msMAE=3.218\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_cfp.pth (pMAE=5.759)\n",
            "\n",
            "Epoch 15 | train_loss=184.5000  train_pMAE=10.064  train_msMAE=3.430 || val_loss=106.1528  val_pMAE=6.280  val_msMAE=3.726\n",
            "\n",
            "Epoch 16 | train_loss=179.0420  train_pMAE=9.909  train_msMAE=3.343 || val_loss=112.1013  val_pMAE=6.264  val_msMAE=3.751\n",
            "\n",
            "Epoch 17 | train_loss=176.6185  train_pMAE=9.787  train_msMAE=3.426 || val_loss=112.1991  val_pMAE=6.400  val_msMAE=3.925\n",
            "\n",
            "Epoch 18 | train_loss=171.4084  train_pMAE=9.675  train_msMAE=3.485 || val_loss=111.9468  val_pMAE=6.369  val_msMAE=3.939\n",
            "\n",
            "Epoch 19 | train_loss=174.3114  train_pMAE=9.743  train_msMAE=3.514 || val_loss=100.9799  val_pMAE=5.987  val_msMAE=3.524\n",
            "\n",
            "Epoch 20 | train_loss=170.1029  train_pMAE=9.627  train_msMAE=3.413 || val_loss=100.0367  val_pMAE=5.979  val_msMAE=3.519\n",
            "\n",
            "Epoch 21 | train_loss=170.0111  train_pMAE=9.574  train_msMAE=3.309 || val_loss=100.6704  val_pMAE=6.000  val_msMAE=3.557\n",
            "\n",
            "Epoch 22 | train_loss=167.0142  train_pMAE=9.497  train_msMAE=3.333 || val_loss=103.7086  val_pMAE=6.114  val_msMAE=3.663\n",
            "\n",
            "Epoch 23 | train_loss=168.3542  train_pMAE=9.582  train_msMAE=3.389 || val_loss=102.6116  val_pMAE=6.046  val_msMAE=3.566\n",
            "\n",
            "Epoch 24 | train_loss=168.3157  train_pMAE=9.574  train_msMAE=3.384 || val_loss=102.5069  val_pMAE=6.048  val_msMAE=3.614\n",
            "\n",
            "Epoch 25 | train_loss=167.9062  train_pMAE=9.532  train_msMAE=3.318 || val_loss=103.2440  val_pMAE=6.098  val_msMAE=3.674\n",
            "\n",
            "Early stopping at epoch 25 (best val pMAE=5.759)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- reproducibility (same as before)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class EarlyStopper:\n",
        "    \"\"\"\n",
        "    - Saves whenever val_metric strictly improves over 'best' (tolerance=1e-12).\n",
        "    - Uses 'min_delta' only to decide whether to reset patience (ref metric).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best_save = float(\"inf\")  # for checkpoint saving (any improvement)\n",
        "        self.best_ref = float(\"inf\")  # for patience (needs >= min_delta improvement)\n",
        "        self.bad_epochs = 0\n",
        "        if self.ckpt_path:\n",
        "            os.makedirs(os.path.dirname(self.ckpt_path), exist_ok=True)\n",
        "\n",
        "    def update(self, val_metric, model, epoch_meta=None):\n",
        "        saved = False\n",
        "        # --- Save on ANY strict improvement\n",
        "        if val_metric < self.best_save - 1e-12:\n",
        "            self.best_save = val_metric\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best_save,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            saved = True\n",
        "\n",
        "        # --- Early-stopping patience uses min_delta\n",
        "        if val_metric < self.best_ref - self.min_delta:\n",
        "            self.best_ref = val_metric\n",
        "            self.bad_epochs = 0\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "\n",
        "        should_stop = self.bad_epochs > self.patience\n",
        "        return should_stop, saved\n",
        "\n",
        "\n",
        "def main():\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] using {len(vf_cols)} VF columns: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # split\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    random.shuffle(rows)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    train_ds = CFPDataset(train_rows, image_col, vf_cols, train=True)\n",
        "    val_ds = CFPDataset(val_rows, image_col, vf_cols, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = MobileNetV3LVF(out_dim=NUM_POINTS, pretrained=True).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt_path = os.path.join(CHECK_DIR, \"best_mobilenetv3l_original_cfp.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt_path)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        print(\n",
        "            f\"\\nEpoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_msMAE={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_msMAE={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # scheduler on validation MAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        # early stopping + save best\n",
        "        should_stop, saved = stopper.update(\n",
        "            va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch}\n",
        "        )\n",
        "        if saved:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt_path} (pMAE={stopper.best_save:.3f})\")\n",
        "        if should_stop:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (best val pMAE={stopper.best_save:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best before returning\n",
        "    state = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt_path\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, train_dl, val_dl, image_col, vf_cols, CKPT = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:29:03.478902Z",
          "iopub.status.busy": "2025-11-10T05:29:03.478577Z",
          "iopub.status.idle": "2025-11-10T05:29:07.443091Z",
          "shell.execute_reply": "2025-11-10T05:29:07.442164Z",
          "shell.execute_reply.started": "2025-11-10T05:29:03.478871Z"
        },
        "id": "Fxq-_LW1il_v",
        "lines_to_next_cell": 2,
        "outputId": "4b88edbf-d4cc-4302-dd8b-c7329358d842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Using BEST checkpoint: ./checkpoints/best_mobilenetv3l_original_cfp.pth\n",
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n"
          ]
        }
      ],
      "source": [
        "# --- reload BEST checkpoint and evaluate ---\n",
        "assert \"CKPT\" in globals(), (\n",
        "    \"CKPT not found. Make sure you ran the training cell that returns CKPT.\"\n",
        ")\n",
        "assert \"val_dl\" in globals(), (\n",
        "    \"val_dl not found. Make sure you ran the training cell that defines val_dl.\"\n",
        ")\n",
        "\n",
        "# rebuild the exact architecture\n",
        "best_model = MobileNetV3LVF(out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "\n",
        "state = torch.load(CKPT, map_location=DEVICE)\n",
        "best_model.load_state_dict(state[\"model\"])\n",
        "best_model.eval()\n",
        "\n",
        "# collect predictions on the validation set\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in val_dl:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        p = best_model(x)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "\n",
        "print(\"✅ Using BEST checkpoint:\", CKPT)\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:29:10.997372Z",
          "iopub.status.busy": "2025-11-10T05:29:10.997079Z",
          "iopub.status.idle": "2025-11-10T05:29:11.006552Z",
          "shell.execute_reply": "2025-11-10T05:29:11.005959Z",
          "shell.execute_reply.started": "2025-11-10T05:29:10.997345Z"
        },
        "id": "QRPvYk4jil_y",
        "lines_to_next_cell": 2,
        "outputId": "0924adaf-b260-486a-fd51-cee880c6acce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== POINTWISE ==\n",
            "RMSE: 9.9729 | MAE: 5.7592 | R²: 0.9792\n",
            "== POINTWISE-MEAN ==\n",
            "RMSE: 4.3776 | MAE: 3.2183 | R²: 0.2939\n",
            "== MS (same as pointwise-mean) ==\n",
            "RMSE: 4.3776 | MAE: 3.2183 | R²: 0.2939\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_val(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "print(\"\\n== POINTWISE ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(pw_true, pw_pred):.4f} | MAE: {mae_val(pw_true, pw_pred):.4f} | R²: {r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "\n",
        "t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "print(\"== POINTWISE-MEAN ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | MAE: {mae_val(t_mean, p_mean):.4f} | R²: {r2(t_mean, p_mean):.4f}\"\n",
        ")\n",
        "\n",
        "print(\"== MS (same as pointwise-mean) ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | MAE: {mae_val(t_mean, p_mean):.4f} | R²: {r2(t_mean, p_mean):.4f}\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.117107Z",
          "iopub.status.busy": "2025-11-08T09:56:44.116818Z",
          "iopub.status.idle": "2025-11-08T09:56:44.137012Z",
          "shell.execute_reply": "2025-11-08T09:56:44.136317Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.117090Z"
        },
        "id": "pZ4mwoPtil_0"
      },
      "source": [
        "# 2. ROI Images MobileNetV3L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.138029Z",
          "iopub.status.busy": "2025-11-08T09:56:44.137733Z",
          "iopub.status.idle": "2025-11-08T09:56:44.156020Z",
          "shell.execute_reply": "2025-11-08T09:56:44.155523Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.138001Z"
        },
        "id": "W6D4GBQHil_1",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.156862Z",
          "iopub.status.busy": "2025-11-08T09:56:44.156704Z",
          "iopub.status.idle": "2025-11-08T09:56:44.195467Z",
          "shell.execute_reply": "2025-11-08T09:56:44.194795Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.156849Z"
        },
        "id": "rpaWhnfTil_2"
      },
      "outputs": [],
      "source": [
        "# ===================== LABELME JSON → OD/OC MASKS =====================\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _poly_area(pts):\n",
        "    x = [p[0] for p in pts]\n",
        "    y = [p[1] for p in pts]\n",
        "    return 0.5 * abs(\n",
        "        sum(x[i] * y[(i + 1) % len(pts)] - x[(i + 1) % len(pts)] * y[i] for i in range(len(pts)))\n",
        "    )\n",
        "\n",
        "\n",
        "def _read_labelme(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        label = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = [(float(x), float(y)) for x, y in sh.get(\"points\", [])]\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if label in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        elif label in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=_poly_area)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=_poly_area)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json_path(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        cand = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(cand):\n",
        "            return cand\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def build_masks_from_labelme(img_pil: Image.Image, img_name: str, out_size: int):\n",
        "    W, H = img_pil.size\n",
        "    od_mask = Image.new(\"L\", (W, H), 0)\n",
        "    oc_mask = Image.new(\"L\", (W, H), 0)\n",
        "\n",
        "    jpath = _guess_json_path(img_name)\n",
        "    if jpath:\n",
        "        try:\n",
        "            od_polys, oc_polys = _read_labelme(jpath)\n",
        "            d_od = ImageDraw.Draw(od_mask)\n",
        "            d_oc = ImageDraw.Draw(oc_mask)\n",
        "            for poly in od_polys:\n",
        "                d_od.polygon(poly, outline=1, fill=1)\n",
        "            for poly in oc_polys:\n",
        "                d_oc.polygon(poly, outline=1, fill=1)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] parsing {jpath}: {e}\")\n",
        "\n",
        "    od_mask = od_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    oc_mask = oc_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    return od_mask, oc_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.196439Z",
          "iopub.status.busy": "2025-11-08T09:56:44.196246Z",
          "iopub.status.idle": "2025-11-08T09:56:44.218107Z",
          "shell.execute_reply": "2025-11-08T09:56:44.217506Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.196424Z"
        },
        "id": "xX79bAtoil_3"
      },
      "outputs": [],
      "source": [
        "class CFPDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows: List[Dict[str, str]], image_col: str, vf_cols: List[str], train: bool\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.train = train\n",
        "\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "\n",
        "        self.tf = transforms.Compose(\n",
        "            [transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(), *aug, normalize]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        name = r[self.image_col]\n",
        "\n",
        "        path = name\n",
        "        if not os.path.isabs(path):\n",
        "            if os.path.basename(path) == path:\n",
        "                path = os.path.join(CFP_DIR, path)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        x = self.tf(img)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.219231Z",
          "iopub.status.busy": "2025-11-08T09:56:44.218972Z",
          "iopub.status.idle": "2025-11-08T09:56:44.240915Z",
          "shell.execute_reply": "2025-11-08T09:56:44.240375Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.219208Z"
        },
        "id": "TMQ_N71lil_5"
      },
      "outputs": [],
      "source": [
        "class MobileNetV3LVF(nn.Module):\n",
        "    def __init__(self, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.mobilenet_v3_large(\n",
        "            weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        )\n",
        "        in_f = self.backbone.classifier[-1].in_features\n",
        "        self.backbone.classifier[-1] = nn.Identity()\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25), nn.Linear(512, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x)\n",
        "        return self.regressor(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T09:56:44.243731Z",
          "iopub.status.busy": "2025-11-08T09:56:44.243485Z",
          "iopub.status.idle": "2025-11-08T09:56:44.256968Z",
          "shell.execute_reply": "2025-11-08T09:56:44.256437Z",
          "shell.execute_reply.started": "2025-11-08T09:56:44.243714Z"
        },
        "id": "vLlmwaeSil_6"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = crit(pred, y)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        n += bs\n",
        "\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:29:55.027957Z",
          "iopub.status.busy": "2025-11-10T05:29:55.027457Z",
          "iopub.status.idle": "2025-11-10T05:40:46.560388Z",
          "shell.execute_reply": "2025-11-10T05:40:46.559609Z",
          "shell.execute_reply.started": "2025-11-10T05:29:55.027932Z"
        },
        "id": "XQy5A7dril_7",
        "lines_to_next_cell": 2,
        "outputId": "cf079187-824f-42c3-ad8f-5dcf8dd32830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] using 59 VF columns: ['AGE', 'CCT', 'IOP_y', 'Interval Years', 'MD'] ... ['VF50', 'VF51', 'VF52', 'VF53', 'VF54']\n",
            "\n",
            "Epoch 01 | train_loss=5414.5734  train_pMAE=28.475  train_msMAE=28.315 || val_loss=5417.1320  val_pMAE=27.656  val_msMAE=27.453\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_roi.pth (pMAE=27.656)\n",
            "\n",
            "Epoch 02 | train_loss=3974.9825  train_pMAE=20.565  train_msMAE=14.576 || val_loss=2725.2064  val_pMAE=18.350  val_msMAE=6.962\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_roi.pth (pMAE=18.350)\n",
            "\n",
            "Epoch 03 | train_loss=1162.0608  train_pMAE=15.938  train_msMAE=5.374 || val_loss=357.6270  val_pMAE=9.239  val_msMAE=6.112\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_roi.pth (pMAE=9.239)\n",
            "\n",
            "Epoch 04 | train_loss=311.6845  train_pMAE=13.164  train_msMAE=4.081 || val_loss=484.8784  val_pMAE=9.342  val_msMAE=7.954\n",
            "\n",
            "Epoch 05 | train_loss=265.9235  train_pMAE=12.184  train_msMAE=3.851 || val_loss=209.0681  val_pMAE=6.884  val_msMAE=4.341\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_roi.pth (pMAE=6.884)\n",
            "\n",
            "Epoch 06 | train_loss=249.0659  train_pMAE=11.940  train_msMAE=3.655 || val_loss=193.8653  val_pMAE=6.797  val_msMAE=4.410\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_roi.pth (pMAE=6.797)\n",
            "\n",
            "Epoch 07 | train_loss=241.6432  train_pMAE=11.701  train_msMAE=3.633 || val_loss=136.5658  val_pMAE=6.339  val_msMAE=3.906\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_roi.pth (pMAE=6.339)\n",
            "\n",
            "Epoch 08 | train_loss=227.3091  train_pMAE=11.328  train_msMAE=3.705 || val_loss=165.3814  val_pMAE=6.430  val_msMAE=4.358\n",
            "\n",
            "Epoch 09 | train_loss=218.3440  train_pMAE=11.098  train_msMAE=3.640 || val_loss=156.7568  val_pMAE=6.269  val_msMAE=3.838\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_roi.pth (pMAE=6.269)\n",
            "\n",
            "Epoch 10 | train_loss=211.7162  train_pMAE=10.888  train_msMAE=3.539 || val_loss=150.3110  val_pMAE=6.431  val_msMAE=3.929\n",
            "\n",
            "Epoch 11 | train_loss=204.8471  train_pMAE=10.694  train_msMAE=3.494 || val_loss=155.2084  val_pMAE=6.541  val_msMAE=4.051\n",
            "\n",
            "Epoch 12 | train_loss=199.2912  train_pMAE=10.504  train_msMAE=3.619 || val_loss=155.7804  val_pMAE=6.395  val_msMAE=3.955\n",
            "\n",
            "Epoch 13 | train_loss=190.4118  train_pMAE=10.247  train_msMAE=3.482 || val_loss=147.3803  val_pMAE=6.298  val_msMAE=3.828\n",
            "\n",
            "Epoch 14 | train_loss=188.4698  train_pMAE=10.187  train_msMAE=3.566 || val_loss=146.9494  val_pMAE=6.132  val_msMAE=3.729\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_original_roi.pth (pMAE=6.132)\n",
            "\n",
            "Epoch 15 | train_loss=184.7438  train_pMAE=10.068  train_msMAE=3.488 || val_loss=148.3798  val_pMAE=6.492  val_msMAE=4.066\n",
            "\n",
            "Epoch 16 | train_loss=179.3488  train_pMAE=9.919  train_msMAE=3.379 || val_loss=146.3367  val_pMAE=6.236  val_msMAE=3.815\n",
            "\n",
            "Epoch 17 | train_loss=178.5089  train_pMAE=9.794  train_msMAE=3.404 || val_loss=145.1998  val_pMAE=6.253  val_msMAE=3.824\n",
            "\n",
            "Epoch 18 | train_loss=172.1671  train_pMAE=9.678  train_msMAE=3.483 || val_loss=159.1573  val_pMAE=6.638  val_msMAE=4.285\n",
            "\n",
            "Epoch 19 | train_loss=176.6803  train_pMAE=9.765  train_msMAE=3.525 || val_loss=149.3589  val_pMAE=6.344  val_msMAE=3.977\n",
            "\n",
            "Epoch 20 | train_loss=170.5154  train_pMAE=9.625  train_msMAE=3.418 || val_loss=147.9562  val_pMAE=6.322  val_msMAE=3.956\n",
            "\n",
            "Epoch 21 | train_loss=170.3196  train_pMAE=9.573  train_msMAE=3.319 || val_loss=145.3567  val_pMAE=6.257  val_msMAE=3.940\n",
            "\n",
            "Epoch 22 | train_loss=168.0917  train_pMAE=9.520  train_msMAE=3.348 || val_loss=145.6133  val_pMAE=6.258  val_msMAE=3.932\n",
            "\n",
            "Epoch 23 | train_loss=169.4181  train_pMAE=9.608  train_msMAE=3.485 || val_loss=145.2679  val_pMAE=6.241  val_msMAE=3.919\n",
            "\n",
            "Epoch 24 | train_loss=168.1465  train_pMAE=9.564  train_msMAE=3.435 || val_loss=145.3758  val_pMAE=6.233  val_msMAE=3.919\n",
            "\n",
            "Epoch 25 | train_loss=168.4670  train_pMAE=9.541  train_msMAE=3.345 || val_loss=147.2120  val_pMAE=6.309  val_msMAE=3.982\n",
            "\n",
            "Early stopping at epoch 25 (best val pMAE=6.132)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# --- reproducibility (same as before)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class EarlyStopper:\n",
        "    \"\"\"\n",
        "    - Saves whenever val_metric strictly improves over 'best' (tolerance=1e-12).\n",
        "    - Uses 'min_delta' only to decide whether to reset patience (ref metric).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best_save = float(\"inf\")  # for checkpoint saving (any improvement)\n",
        "        self.best_ref = float(\"inf\")  # for patience (needs >= min_delta improvement)\n",
        "        self.bad_epochs = 0\n",
        "        if self.ckpt_path:\n",
        "            os.makedirs(os.path.dirname(self.ckpt_path), exist_ok=True)\n",
        "\n",
        "    def update(self, val_metric, model, epoch_meta=None):\n",
        "        saved = False\n",
        "        # --- Save on ANY strict improvement\n",
        "        if val_metric < self.best_save - 1e-12:\n",
        "            self.best_save = val_metric\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best_save,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            saved = True\n",
        "\n",
        "        # --- Early-stopping patience uses min_delta\n",
        "        if val_metric < self.best_ref - self.min_delta:\n",
        "            self.best_ref = val_metric\n",
        "            self.bad_epochs = 0\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "\n",
        "        should_stop = self.bad_epochs > self.patience\n",
        "        return should_stop, saved\n",
        "\n",
        "\n",
        "def main():\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] using {len(vf_cols)} VF columns: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # split\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    random.shuffle(rows)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    train_ds = CFPDataset(train_rows, image_col, vf_cols, train=True)\n",
        "    val_ds = CFPDataset(val_rows, image_col, vf_cols, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = MobileNetV3LVF(out_dim=NUM_POINTS, pretrained=True).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt_path = os.path.join(CHECK_DIR, \"best_mobilenetv3l_original_roi.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt_path)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        print(\n",
        "            f\"\\nEpoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_msMAE={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_msMAE={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # scheduler on validation MAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        # early stopping + save best\n",
        "        should_stop, saved = stopper.update(\n",
        "            va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch}\n",
        "        )\n",
        "        if saved:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt_path} (pMAE={stopper.best_save:.3f})\")\n",
        "        if should_stop:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (best val pMAE={stopper.best_save:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best before returning\n",
        "    state = torch.load(ckpt_path, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt_path\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, train_dl, val_dl, image_col, vf_cols, CKPT = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:41:26.754434Z",
          "iopub.status.busy": "2025-11-10T05:41:26.753893Z",
          "iopub.status.idle": "2025-11-10T05:41:30.190236Z",
          "shell.execute_reply": "2025-11-10T05:41:30.189451Z",
          "shell.execute_reply.started": "2025-11-10T05:41:26.754411Z"
        },
        "id": "qKgPYlCTil_8",
        "lines_to_next_cell": 2,
        "outputId": "c48f2202-0af8-4e40-dcb1-d08a746a6576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ ROI predictions collected: torch.Size([127, 59]) torch.Size([127, 59])\n"
          ]
        }
      ],
      "source": [
        "# reload best ROI checkpoint\n",
        "best_roi = MobileNetV3LVF(out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "state = torch.load(CKPT, map_location=DEVICE)\n",
        "best_roi.load_state_dict(state[\"model\"])\n",
        "best_roi.eval()\n",
        "\n",
        "# collect predictions\n",
        "all_true = []\n",
        "all_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in val_dl:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        p = best_roi(x)\n",
        "\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "\n",
        "print(\"✅ ROI predictions collected:\", y_true.shape, y_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:41:33.799312Z",
          "iopub.status.busy": "2025-11-10T05:41:33.798656Z",
          "iopub.status.idle": "2025-11-10T05:41:33.808988Z",
          "shell.execute_reply": "2025-11-10T05:41:33.808148Z",
          "shell.execute_reply.started": "2025-11-10T05:41:33.799282Z"
        },
        "id": "4mEofKC6il_9",
        "lines_to_next_cell": 2,
        "outputId": "29bf09b7-cbe7-46a7-a03b-1ef1e0c5c7c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== ROI: POINTWISE ==\n",
            "RMSE: 12.1223\n",
            "MAE : 6.1318\n",
            "R²  : 0.9692\n",
            "\n",
            "== ROI: POINTWISE-MEAN / MS ==\n",
            "RMSE: 5.5650\n",
            "MAE : 3.7295\n",
            "R²  : -0.1412\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# helpers\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_val(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# POINTWISE\n",
        "pw_true = y_true.reshape(-1)\n",
        "pw_pred = y_pred.reshape(-1)\n",
        "\n",
        "print(\"\\n== ROI: POINTWISE ==\")\n",
        "print(f\"RMSE: {rmse(pw_true, pw_pred):.4f}\")\n",
        "print(f\"MAE : {mae_val(pw_true, pw_pred):.4f}\")\n",
        "print(f\"R²  : {r2(pw_true, pw_pred):.4f}\")\n",
        "\n",
        "# POINTWISE-MEAN / MS\n",
        "t_mean = y_true.mean(dim=1)\n",
        "p_mean = y_pred.mean(dim=1)\n",
        "\n",
        "print(\"\\n== ROI: POINTWISE-MEAN / MS ==\")\n",
        "print(f\"RMSE: {rmse(t_mean, p_mean):.4f}\")\n",
        "print(f\"MAE : {mae_val(t_mean, p_mean):.4f}\")\n",
        "print(f\"R²  : {r2(t_mean, p_mean):.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T10:00:28.759901Z",
          "iopub.status.busy": "2025-11-08T10:00:28.759605Z",
          "iopub.status.idle": "2025-11-08T10:00:28.775895Z",
          "shell.execute_reply": "2025-11-08T10:00:28.775339Z",
          "shell.execute_reply.started": "2025-11-08T10:00:28.759884Z"
        },
        "id": "-tTNGuxDil__"
      },
      "source": [
        "# 3. ROI + OD/OD Segmentation MobileNetV3L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:45:55.379375Z",
          "iopub.status.busy": "2025-11-10T05:45:55.378706Z",
          "iopub.status.idle": "2025-11-10T05:45:55.386060Z",
          "shell.execute_reply": "2025-11-10T05:45:55.385342Z",
          "shell.execute_reply.started": "2025-11-10T05:45:55.379352Z"
        },
        "id": "muATk7Rtil__"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# ---- paths ----\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "# ---- training ----\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:45:58.655021Z",
          "iopub.status.busy": "2025-11-10T05:45:58.654715Z",
          "iopub.status.idle": "2025-11-10T05:45:58.665138Z",
          "shell.execute_reply": "2025-11-10T05:45:58.664425Z",
          "shell.execute_reply.started": "2025-11-10T05:45:58.655000Z"
        },
        "id": "aunPxddDimAB"
      },
      "outputs": [],
      "source": [
        "# ===================== CSV UTILS =====================\n",
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "IMAGE_COLS_CANDIDATES = [\"image\", \"image_name\", \"img\", \"image_path\", \"filename\", \"file\"]\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]) -> Tuple[str, List[str]]:\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # image col\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # VF cols prefer v1..v59 else numeric fallback\n",
        "    vf = [f\"v{i}\" for i in range(1, NUM_POINTS + 1)]\n",
        "    if all(c in cols for c in vf):\n",
        "        return image_col, vf\n",
        "\n",
        "    cand = []\n",
        "    for c in cols:\n",
        "        if c == image_col:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            cand.append(c)\n",
        "    if len(cand) < NUM_POINTS:\n",
        "        raise ValueError(f\"Need {NUM_POINTS} VF cols, found {len(cand)}.\")\n",
        "\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    cand = sorted(cand, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, cand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:02.177236Z",
          "iopub.status.busy": "2025-11-10T05:46:02.176683Z",
          "iopub.status.idle": "2025-11-10T05:46:02.187291Z",
          "shell.execute_reply": "2025-11-10T05:46:02.186682Z",
          "shell.execute_reply.started": "2025-11-10T05:46:02.177213Z"
        },
        "id": "WS1sfw9AimAC"
      },
      "outputs": [],
      "source": [
        "# ===================== LABELME JSON → OD/OC MASKS =====================\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _poly_area(pts):\n",
        "    x = [p[0] for p in pts]\n",
        "    y = [p[1] for p in pts]\n",
        "    return 0.5 * abs(\n",
        "        sum(x[i] * y[(i + 1) % len(pts)] - x[(i + 1) % len(pts)] * y[i] for i in range(len(pts)))\n",
        "    )\n",
        "\n",
        "\n",
        "def _read_labelme(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        label = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = [(float(x), float(y)) for x, y in sh.get(\"points\", [])]\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if label in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        elif label in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=_poly_area)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=_poly_area)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json_path(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        cand = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(cand):\n",
        "            return cand\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def build_masks_from_labelme(img_pil: Image.Image, img_name: str, out_size: int):\n",
        "    W, H = img_pil.size\n",
        "    od_mask = Image.new(\"L\", (W, H), 0)\n",
        "    oc_mask = Image.new(\"L\", (W, H), 0)\n",
        "\n",
        "    jpath = _guess_json_path(img_name)\n",
        "    if jpath:\n",
        "        try:\n",
        "            od_polys, oc_polys = _read_labelme(jpath)\n",
        "            d_od = ImageDraw.Draw(od_mask)\n",
        "            d_oc = ImageDraw.Draw(oc_mask)\n",
        "            for poly in od_polys:\n",
        "                d_od.polygon(poly, outline=1, fill=1)\n",
        "            for poly in oc_polys:\n",
        "                d_oc.polygon(poly, outline=1, fill=1)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] parsing {jpath}: {e}\")\n",
        "\n",
        "    od_mask = od_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    oc_mask = oc_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    return od_mask, oc_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:05.752315Z",
          "iopub.status.busy": "2025-11-10T05:46:05.751834Z",
          "iopub.status.idle": "2025-11-10T05:46:05.759405Z",
          "shell.execute_reply": "2025-11-10T05:46:05.758759Z",
          "shell.execute_reply.started": "2025-11-10T05:46:05.752295Z"
        },
        "id": "-28PQad5imAD"
      },
      "outputs": [],
      "source": [
        "# ===================== DATASET (5-channel RGB+OD+OC) =====================\n",
        "class ROI_OD_OC_Dataset(Dataset):\n",
        "    def __init__(self, rows, image_col, vf_cols, train=True, img_root=ROI_DIR, img_size=IMG_SIZE):\n",
        "        self.rows, self.image_col, self.vf_cols = rows, image_col, vf_cols\n",
        "        self.train, self.img_root, self.img_size = train, img_root, img_size\n",
        "\n",
        "        norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.rgb_tf = transforms.Compose(\n",
        "            [transforms.Resize((img_size, img_size)), transforms.ToTensor(), *aug, norm]\n",
        "        )\n",
        "        self.mask_tf = transforms.ToTensor()  # L → (1,H,W) float {0,1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        fn = r[self.image_col]\n",
        "        path = fn if os.path.isabs(fn) else os.path.join(self.img_root, fn)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        od_img, oc_img = build_masks_from_labelme(img, fn, self.img_size)\n",
        "\n",
        "        x_rgb = self.rgb_tf(img)  # (3,H,W)\n",
        "        x_od = self.mask_tf(od_img)  # (1,H,W)\n",
        "        x_oc = self.mask_tf(oc_img)  # (1,H,W)\n",
        "        x = torch.cat([x_rgb, x_od, x_oc], dim=0)  # (5,H,W)\n",
        "\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:11.357022Z",
          "iopub.status.busy": "2025-11-10T05:46:11.356328Z",
          "iopub.status.idle": "2025-11-10T05:46:11.363275Z",
          "shell.execute_reply": "2025-11-10T05:46:11.362574Z",
          "shell.execute_reply.started": "2025-11-10T05:46:11.356999Z"
        },
        "id": "IFpCHhFBimAE"
      },
      "outputs": [],
      "source": [
        "# ===================== MODEL (ResNet-50 with 5-ch input) =====================\n",
        "class MobileNetV3L_5ch_VF(nn.Module):\n",
        "    def __init__(self, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        base = models.mobilenet_v3_large(\n",
        "            weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        )\n",
        "        # adapt Conv2dNormActivation: 3→5 channels (init extra channels with mean RGB weights)\n",
        "        old = base.features[0][0]\n",
        "        new = nn.Conv2d(\n",
        "            5,\n",
        "            old.out_channels,\n",
        "            kernel_size=old.kernel_size,\n",
        "            stride=old.stride,\n",
        "            padding=old.padding,\n",
        "            bias=(old.bias is not None),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            new.weight[:, :3, :, :] = old.weight\n",
        "            mean_w = old.weight.mean(dim=1, keepdim=True)\n",
        "            new.weight[:, 3:5, :, :] = mean_w.repeat(1, 2, 1, 1)\n",
        "            if old.bias is not None:\n",
        "                new.bias.copy_(old.bias)\n",
        "        base.features[0][0] = new\n",
        "\n",
        "        in_f = base.classifier[-1].in_features\n",
        "        base.classifier[-1] = nn.Identity()\n",
        "        self.backbone = base\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25), nn.Linear(512, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x5):\n",
        "        f = self.backbone(x5)\n",
        "        return self.regressor(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:14.956708Z",
          "iopub.status.busy": "2025-11-10T05:46:14.955948Z",
          "iopub.status.idle": "2025-11-10T05:46:14.963048Z",
          "shell.execute_reply": "2025-11-10T05:46:14.962411Z",
          "shell.execute_reply.started": "2025-11-10T05:46:14.956686Z"
        },
        "id": "qOCQtX42imAF"
      },
      "outputs": [],
      "source": [
        "# ===================== METRICS + EPOCH LOOP =====================\n",
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:46:18.407433Z",
          "iopub.status.busy": "2025-11-10T05:46:18.406736Z",
          "iopub.status.idle": "2025-11-10T05:52:34.919034Z",
          "shell.execute_reply": "2025-11-10T05:52:34.918137Z",
          "shell.execute_reply.started": "2025-11-10T05:46:18.407410Z"
        },
        "id": "WEwg7-animAG",
        "lines_to_next_cell": 2,
        "outputId": "149bd998-0ed3-4d31-b8d4-efc81823106c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] 59 VF cols: ['AGE', 'CCT', 'IOP_y', 'Interval Years', 'MD'] ... ['VF50', 'VF51', 'VF52', 'VF53', 'VF54']\n",
            "Epoch 01 | train_loss=5405.0561  train_pMAE=28.408  train_MS=28.255 || val_loss=5188.2580  val_pMAE=26.195  val_MS=25.855\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC.pth (pMAE=26.195)\n",
            "\n",
            "Epoch 02 | train_loss=3792.9892  train_pMAE=19.962  train_MS=13.381 || val_loss=2230.6751  val_pMAE=20.818  val_MS=8.704\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC.pth (pMAE=20.818)\n",
            "\n",
            "Epoch 03 | train_loss=1001.0242  train_pMAE=15.370  train_MS=5.395 || val_loss=148.0195  val_pMAE=7.393  val_MS=4.556\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC.pth (pMAE=7.393)\n",
            "\n",
            "Epoch 04 | train_loss=298.7601  train_pMAE=13.078  train_MS=3.963 || val_loss=154.1055  val_pMAE=6.761  val_MS=4.421\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC.pth (pMAE=6.761)\n",
            "\n",
            "Epoch 05 | train_loss=265.8528  train_pMAE=12.276  train_MS=3.731 || val_loss=144.9575  val_pMAE=6.546  val_MS=4.191\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC.pth (pMAE=6.546)\n",
            "\n",
            "Epoch 06 | train_loss=253.0069  train_pMAE=11.976  train_MS=3.762 || val_loss=129.6173  val_pMAE=6.393  val_MS=3.927\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC.pth (pMAE=6.393)\n",
            "\n",
            "Epoch 07 | train_loss=237.9584  train_pMAE=11.613  train_MS=3.564 || val_loss=155.5029  val_pMAE=6.431  val_MS=3.997\n",
            "Epoch 08 | train_loss=228.6797  train_pMAE=11.370  train_MS=3.734 || val_loss=157.2422  val_pMAE=6.397  val_MS=3.864\n",
            "Epoch 09 | train_loss=218.3061  train_pMAE=11.102  train_MS=3.537 || val_loss=155.0787  val_pMAE=6.393  val_MS=3.977\n",
            "Epoch 10 | train_loss=213.5115  train_pMAE=10.938  train_MS=3.564 || val_loss=160.6901  val_pMAE=6.691  val_MS=4.256\n",
            "Epoch 11 | train_loss=203.4615  train_pMAE=10.639  train_MS=3.525 || val_loss=150.1084  val_pMAE=6.366  val_MS=4.069\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC.pth (pMAE=6.366)\n",
            "\n",
            "Epoch 12 | train_loss=200.0430  train_pMAE=10.611  train_MS=3.783 || val_loss=150.1404  val_pMAE=6.391  val_MS=4.063\n",
            "Epoch 13 | train_loss=198.2556  train_pMAE=10.569  train_MS=3.521 || val_loss=150.8887  val_pMAE=6.346  val_MS=4.036\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC.pth (pMAE=6.346)\n",
            "\n",
            "Epoch 14 | train_loss=193.3888  train_pMAE=10.354  train_MS=3.399 || val_loss=147.8874  val_pMAE=6.307  val_MS=3.997\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC.pth (pMAE=6.307)\n",
            "\n",
            "Epoch 15 | train_loss=189.9767  train_pMAE=10.217  train_MS=3.515 || val_loss=151.6178  val_pMAE=6.472  val_MS=4.154\n",
            "Epoch 16 | train_loss=184.7052  train_pMAE=10.166  train_MS=3.551 || val_loss=147.6821  val_pMAE=6.299  val_MS=3.942\n",
            "Epoch 17 | train_loss=183.6827  train_pMAE=10.069  train_MS=3.463 || val_loss=147.7929  val_pMAE=6.250  val_MS=3.926\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC.pth (pMAE=6.250)\n",
            "\n",
            "Epoch 18 | train_loss=179.3114  train_pMAE=9.947  train_MS=3.427 || val_loss=150.0390  val_pMAE=6.365  val_MS=4.078\n",
            "Epoch 19 | train_loss=176.3914  train_pMAE=9.816  train_MS=3.329 || val_loss=147.2541  val_pMAE=6.319  val_MS=4.042\n",
            "Epoch 20 | train_loss=176.4078  train_pMAE=9.824  train_MS=3.390 || val_loss=150.0625  val_pMAE=6.567  val_MS=4.247\n",
            "Epoch 21 | train_loss=173.9519  train_pMAE=9.710  train_MS=3.331 || val_loss=147.0754  val_pMAE=6.238  val_MS=3.987\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC.pth (pMAE=6.238)\n",
            "\n",
            "Epoch 22 | train_loss=171.6293  train_pMAE=9.624  train_MS=3.376 || val_loss=146.2536  val_pMAE=6.288  val_MS=3.996\n",
            "Epoch 23 | train_loss=167.6041  train_pMAE=9.525  train_MS=3.486 || val_loss=152.2166  val_pMAE=6.463  val_MS=4.208\n",
            "Epoch 24 | train_loss=168.5737  train_pMAE=9.497  train_MS=3.464 || val_loss=148.9819  val_pMAE=6.344  val_MS=3.941\n",
            "Epoch 25 | train_loss=163.2415  train_pMAE=9.378  train_MS=3.454 || val_loss=150.3490  val_pMAE=6.427  val_MS=4.146\n",
            "Epoch 26 | train_loss=162.7598  train_pMAE=9.346  train_MS=3.476 || val_loss=150.8219  val_pMAE=6.442  val_MS=4.224\n",
            "Epoch 27 | train_loss=159.5949  train_pMAE=9.252  train_MS=3.419 || val_loss=149.1772  val_pMAE=6.469  val_MS=4.240\n",
            "Epoch 28 | train_loss=158.1066  train_pMAE=9.162  train_MS=3.328 || val_loss=148.1317  val_pMAE=6.355  val_MS=4.150\n",
            "Epoch 29 | train_loss=161.5229  train_pMAE=9.303  train_MS=3.419 || val_loss=147.4985  val_pMAE=6.365  val_MS=4.174\n",
            "Epoch 30 | train_loss=160.2238  train_pMAE=9.201  train_MS=3.434 || val_loss=145.4976  val_pMAE=6.304  val_MS=4.101\n",
            "Epoch 31 | train_loss=158.5420  train_pMAE=9.183  train_MS=3.313 || val_loss=147.0102  val_pMAE=6.356  val_MS=4.129\n",
            "Epoch 32 | train_loss=155.6515  train_pMAE=9.106  train_MS=3.341 || val_loss=145.9063  val_pMAE=6.307  val_MS=4.090\n",
            "Early stopping at epoch 32 (best val pMAE=6.238)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ---- (optional) reproducibility on small data\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---- Early Stopping helper\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best = float(\"inf\")\n",
        "        self.bad_epochs = 0\n",
        "\n",
        "    def step(self, val_metric, model, epoch_meta=None):\n",
        "        # returns True if we should stop\n",
        "        if val_metric < self.best - self.min_delta:\n",
        "            self.best = val_metric\n",
        "            self.bad_epochs = 0\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            return False\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "            return self.bad_epochs > self.patience\n",
        "\n",
        "\n",
        "# ===================== TRAIN =====================\n",
        "def train_resnet50_roi_odoc(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01):\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] {len(vf_cols)} VF cols: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    random.shuffle(rows)\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    train_ds = ROI_OD_OC_Dataset(train_rows, image_col, vf_cols, train=True)\n",
        "    val_ds = ROI_OD_OC_Dataset(val_rows, image_col, vf_cols, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = MobileNetV3L_5ch_VF(out_dim=NUM_POINTS, pretrained=True).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "    # ---- LR scheduler (plateau)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt = os.path.join(CHECK_DIR, \"best_mobilenetv3l_ROI_ODOC.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        # step scheduler on validation pMAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # save best (and detect improvement for pretty print)\n",
        "        prev_best = stopper.best\n",
        "        should_stop = stopper.step(va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch})\n",
        "        if stopper.best < prev_best - MIN_DELTA:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt} (pMAE={stopper.best:.3f})\\n\")\n",
        "\n",
        "        if should_stop:\n",
        "            print(f\"Early stopping at epoch {epoch} (best val pMAE={stopper.best:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best weights before returning\n",
        "    state = torch.load(ckpt, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt\n",
        "\n",
        "\n",
        "# run training and expose globals\n",
        "model_odoc, train_dl_odoc, val_dl_odoc, image_col_odoc, vf_cols_odoc, CKPT_ODOC = (\n",
        "    train_resnet50_roi_odoc(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T05:52:45.952603Z",
          "iopub.status.busy": "2025-11-10T05:52:45.951688Z",
          "iopub.status.idle": "2025-11-10T05:52:47.343625Z",
          "shell.execute_reply": "2025-11-10T05:52:47.342644Z",
          "shell.execute_reply.started": "2025-11-10T05:52:45.952569Z"
        },
        "id": "DSPTkFELimAH",
        "outputId": "e0c5ab37-2929-477c-8d6e-a3903f2e4d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n",
            "\n",
            "== ROI+OD/OC: POINTWISE ==\n",
            "RMSE: 12.1275\n",
            "MAE : 6.2384\n",
            "R²  : 0.9692\n",
            "\n",
            "== ROI+OD/OC: POINTWISE-MEAN / MS ==\n",
            "RMSE: 5.5862\n",
            "MAE : 3.9871\n",
            "R²  : -0.1499\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ===================== EVALUATE BEST + PAPER METRICS =====================\n",
        "# reload best\n",
        "best_odoc = MobileNetV3L_5ch_VF(out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "state = torch.load(CKPT_ODOC, map_location=DEVICE)\n",
        "best_odoc.load_state_dict(state[\"model\"])\n",
        "best_odoc.eval()\n",
        "\n",
        "# predictions\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in val_dl_odoc:\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        p = best_odoc(x)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)\n",
        "\n",
        "\n",
        "# metrics (safe names to avoid clobbering mae/ms_mae)\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_value(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "print(\"\\n== ROI+OD/OC: POINTWISE ==\")\n",
        "print(f\"RMSE: {rmse(pw_true, pw_pred):.4f}\")\n",
        "print(f\"MAE : {mae_value(pw_true, pw_pred):.4f}\")\n",
        "print(f\"R²  : {r2(pw_true, pw_pred):.4f}\")\n",
        "\n",
        "t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "print(\"\\n== ROI+OD/OC: POINTWISE-MEAN / MS ==\")\n",
        "print(f\"RMSE: {rmse(t_mean, p_mean):.4f}\")\n",
        "print(f\"MAE : {mae_value(t_mean, p_mean):.4f}\")\n",
        "print(f\"R²  : {r2(t_mean, p_mean):.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T10:04:17.910859Z",
          "iopub.status.busy": "2025-11-08T10:04:17.910580Z",
          "iopub.status.idle": "2025-11-08T10:04:17.914574Z",
          "shell.execute_reply": "2025-11-08T10:04:17.913924Z",
          "shell.execute_reply.started": "2025-11-08T10:04:17.910833Z"
        },
        "id": "DFBQvGqYimAJ"
      },
      "source": [
        "# 4. ROI + Clinical Features MobileNetV3L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:04:44.866421Z",
          "iopub.status.busy": "2025-11-10T06:04:44.865721Z",
          "iopub.status.idle": "2025-11-10T06:04:44.873765Z",
          "shell.execute_reply": "2025-11-10T06:04:44.873117Z",
          "shell.execute_reply.started": "2025-11-10T06:04:44.866399Z"
        },
        "id": "GnQ1eiYGimAJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "# ---- training ----\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59  # VF1..VF59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# clinical columns present in your CSV (plus computed ones)\n",
        "# from your columns: AGE, GENDER, IOP_y, MD exist; we’ll add computed CDR & PSD\n",
        "CLIN_NUM_COLS = [\"AGE\", \"IOP_y\", \"CDR\"]  # numeric\n",
        "CLIN_CAT_COLS = [\"GENDER\"]  # categorical (mapped to 0/1)\n",
        "IMAGE_COLS_CANDIDATES = [\n",
        "    \"Corresponding CFP\",\n",
        "    \"image\",\n",
        "    \"image_name\",\n",
        "    \"img\",\n",
        "    \"image_path\",\n",
        "    \"filename\",\n",
        "    \"file\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:04:52.367164Z",
          "iopub.status.busy": "2025-11-10T06:04:52.366737Z",
          "iopub.status.idle": "2025-11-10T06:04:52.378136Z",
          "shell.execute_reply": "2025-11-10T06:04:52.377479Z",
          "shell.execute_reply.started": "2025-11-10T06:04:52.367140Z"
        },
        "id": "3qt7jQ-_imAK"
      },
      "outputs": [],
      "source": [
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]) -> Tuple[str, List[str]]:\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # image column: prefer \"Corresponding CFP\" if present\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # prefer explicit VF1..VF59 (ignore VF0, VF60)\n",
        "    vf_cols_pref = [f\"VF{i}\" for i in range(1, 60)]\n",
        "    if all(c in cols for c in vf_cols_pref):\n",
        "        return image_col, vf_cols_pref\n",
        "\n",
        "    # fallback: numeric detection (exclude clinical & image)\n",
        "    excluded = set([image_col] + CLIN_NUM_COLS + CLIN_CAT_COLS + [\"VF0\", \"VF60\"])\n",
        "    candidates = []\n",
        "    for c in cols:\n",
        "        if c in excluded:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            candidates.append(c)\n",
        "\n",
        "    if len(candidates) < NUM_POINTS:\n",
        "        raise ValueError(\n",
        "            f\"Not enough numeric VF columns; found {len(candidates)}, need {NUM_POINTS}.\"\n",
        "        )\n",
        "\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    candidates_sorted = sorted(candidates, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, candidates_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:01.646023Z",
          "iopub.status.busy": "2025-11-10T06:05:01.645439Z",
          "iopub.status.idle": "2025-11-10T06:05:01.654860Z",
          "shell.execute_reply": "2025-11-10T06:05:01.654051Z",
          "shell.execute_reply.started": "2025-11-10T06:05:01.646002Z"
        },
        "id": "TD3NcY98imAL"
      },
      "outputs": [],
      "source": [
        "# ---- CDR from LabelMe polygons (vertical cup/disc ratio) ----\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _read_labelme_polys(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        lab = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = sh.get(\"points\", [])\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if lab in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        if lab in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "\n",
        "    # keep polygon with max vertical height if multiple\n",
        "    def vheight(poly):\n",
        "        ys = [p[1] for p in poly]\n",
        "        return (max(ys) - min(ys)) if ys else 0.0\n",
        "\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=vheight)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=vheight)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        p = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def compute_cdr_from_json(img_name: str):\n",
        "    \"\"\"\n",
        "    CDR = vertical height of OC / vertical height of OD.\n",
        "    Returns None if JSON missing or polygons absent.\n",
        "    \"\"\"\n",
        "    jpath = _guess_json(img_name)\n",
        "    if not jpath:\n",
        "        return None\n",
        "    try:\n",
        "        od_polys, oc_polys = _read_labelme_polys(jpath)\n",
        "        if not od_polys or not oc_polys:\n",
        "            return None\n",
        "\n",
        "        def vheight(poly):\n",
        "            ys = [float(y) for _, y in poly]\n",
        "            return max(ys) - min(ys) if ys else 0.0\n",
        "\n",
        "        h_od = vheight(od_polys[0])\n",
        "        h_oc = vheight(oc_polys[0])\n",
        "        if h_od <= 0:\n",
        "            return None\n",
        "        return float(h_oc / h_od)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] CDR parse failed for {img_name}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:08.135660Z",
          "iopub.status.busy": "2025-11-10T06:05:08.134961Z",
          "iopub.status.idle": "2025-11-10T06:05:08.139853Z",
          "shell.execute_reply": "2025-11-10T06:05:08.139232Z",
          "shell.execute_reply.started": "2025-11-10T06:05:08.135637Z"
        },
        "id": "llHu2rFoimAM"
      },
      "outputs": [],
      "source": [
        "def augment_rows_with_cdr(rows, image_col, vf_cols):\n",
        "    augmented = []\n",
        "    miss_cdr = miss_psd = 0\n",
        "    for r in rows:\n",
        "        r2 = dict(r)\n",
        "        # compute CDR from JSON polygons\n",
        "        cdr = compute_cdr_from_json(r2[image_col])\n",
        "        if cdr is None:\n",
        "            miss_cdr += 1\n",
        "        r2[\"CDR\"] = cdr\n",
        "\n",
        "        augmented.append(r2)\n",
        "    print(f\"✅ Augmented rows: CDR missing={miss_cdr}, PSD missing={miss_psd}\")\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "cQpuhAifimAN"
      },
      "outputs": [],
      "source": [
        "# ----------------- AUGMENT ROWS WITH CDR  -----------------\n",
        "def augment_rows_with_cdr_psd(rows, image_col, vf_cols):\n",
        "    augmented = []\n",
        "    miss_cdr = miss_psd = 0\n",
        "    for r in rows:\n",
        "        r2 = dict(r)\n",
        "        cdr = compute_cdr_from_json(r2[image_col])\n",
        "        if cdr is None:\n",
        "            miss_cdr += 1\n",
        "        r2[\"CDR\"] = cdr\n",
        "\n",
        "        augmented.append(r2)\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:11.650178Z",
          "iopub.status.busy": "2025-11-10T06:05:11.649913Z",
          "iopub.status.idle": "2025-11-10T06:05:11.657823Z",
          "shell.execute_reply": "2025-11-10T06:05:11.657129Z",
          "shell.execute_reply.started": "2025-11-10T06:05:11.650160Z"
        },
        "id": "_WszwiYnimAO"
      },
      "outputs": [],
      "source": [
        "def to_float(x):\n",
        "    x = str(x).strip()\n",
        "    if x == \"\":\n",
        "        return None\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def fit_clinical_stats(rows, clin_num_cols):\n",
        "    stats = {}\n",
        "    for c in clin_num_cols:\n",
        "        vals = [to_float(r.get(c, \"\")) for r in rows]\n",
        "        vals = [v for v in vals if v is not None]\n",
        "        mean = np.mean(vals) if vals else 0.0\n",
        "        std = np.std(vals) if vals else 1.0\n",
        "        if std == 0:\n",
        "            std = 1.0\n",
        "        stats[c] = (float(mean), float(std))\n",
        "    return stats\n",
        "\n",
        "\n",
        "def encode_gender(x):\n",
        "    s = str(x).strip().lower()\n",
        "    if s in (\"m\", \"male\", \"man\"):\n",
        "        return 1.0\n",
        "    if s in (\"f\", \"female\", \"woman\"):\n",
        "        return 0.0\n",
        "    return 0.5  # unknown/other\n",
        "\n",
        "\n",
        "def build_clinical_vector(r, stats):\n",
        "    vec = []\n",
        "    for c in CLIN_NUM_COLS:\n",
        "        v = to_float(r.get(c, \"\"))\n",
        "        mean, std = stats[c]\n",
        "        v = mean if v is None else v\n",
        "        v = (v - mean) / std\n",
        "        vec.append(v)\n",
        "    for c in CLIN_CAT_COLS:\n",
        "        if c == \"GENDER\":\n",
        "            vec.append(encode_gender(r.get(c, \"\")))\n",
        "        else:\n",
        "            vec.append(0.0)\n",
        "    return torch.tensor(vec, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:17.437004Z",
          "iopub.status.busy": "2025-11-10T06:05:17.436286Z",
          "iopub.status.idle": "2025-11-10T06:05:17.443298Z",
          "shell.execute_reply": "2025-11-10T06:05:17.442595Z",
          "shell.execute_reply.started": "2025-11-10T06:05:17.436978Z"
        },
        "id": "IYhKhbgJimAO"
      },
      "outputs": [],
      "source": [
        "class ROIClinicalDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows, image_col, vf_cols, clin_stats, train=True, img_root=ROI_DIR, img_size=IMG_SIZE\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.clin_stats = clin_stats\n",
        "        self.train = train\n",
        "        self.img_root = img_root\n",
        "\n",
        "        normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.tf = transforms.Compose(\n",
        "            [transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(), *aug, normalize]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.rows[idx]\n",
        "        name = r[self.image_col]\n",
        "        path = name if os.path.isabs(name) else os.path.join(self.img_root, name)\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        x_img = self.tf(img)\n",
        "\n",
        "        x_clin = build_clinical_vector(r, self.clin_stats)  # (clin_dim,)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)  # (59,)\n",
        "\n",
        "        return x_img, x_clin, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7q_HK0DFimAP"
      },
      "outputs": [],
      "source": [
        "# ----------------- DATASET: 5-CH ROI + CLINICAL -----------------\n",
        "class ROI_ODOC_Clinical_Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows, image_col, vf_cols, clin_stats, train=True, img_root=ROI_DIR, img_size=IMG_SIZE\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.clin_stats = clin_stats\n",
        "        self.train = train\n",
        "        self.img_root = img_root\n",
        "        self.img_size = img_size\n",
        "\n",
        "        norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.rgb_tf = transforms.Compose(\n",
        "            [transforms.Resize((img_size, img_size)), transforms.ToTensor(), *aug, norm]\n",
        "        )\n",
        "        self.mask_tf = transforms.ToTensor()  # L→(1,H,W) float {0,1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.rows[idx]\n",
        "        fn = r[self.image_col]\n",
        "        path = fn if os.path.isabs(fn) else os.path.join(self.img_root, fn)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        od_img, oc_img = build_masks_from_labelme(img, fn, self.img_size)\n",
        "\n",
        "        x_rgb = self.rgb_tf(img)  # (3,H,W)\n",
        "        x_od = self.mask_tf(od_img)  # (1,H,W)\n",
        "        x_oc = self.mask_tf(oc_img)  # (1,H,W)\n",
        "        x5 = torch.cat([x_rgb, x_od, x_oc], dim=0)  # (5,H,W)\n",
        "\n",
        "        x_clin = build_clinical_vector(r, self.clin_stats)  # (clin_dim,)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)  # (59,)\n",
        "\n",
        "        return x5, x_clin, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:23.335737Z",
          "iopub.status.busy": "2025-11-10T06:05:23.335473Z",
          "iopub.status.idle": "2025-11-10T06:05:23.342292Z",
          "shell.execute_reply": "2025-11-10T06:05:23.341478Z",
          "shell.execute_reply.started": "2025-11-10T06:05:23.335717Z"
        },
        "id": "1-xki1EnimAQ"
      },
      "outputs": [],
      "source": [
        "class MobileNetV3L_ROI_Clinical(nn.Module):\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.backbone = models.mobilenet_v3_large(\n",
        "            weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        )\n",
        "        in_f = self.backbone.classifier[-1].in_features\n",
        "        self.backbone.classifier[-1] = nn.Identity()\n",
        "\n",
        "        self.img_head = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.10),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(512 + 64, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_img, x_clin):\n",
        "        f = self.backbone(x_img)  # (B, 2048)\n",
        "        f = self.img_head(f)  # (B, 512)\n",
        "        g = self.clin_head(x_clin)  # (B, 64)\n",
        "        z = torch.cat([f, g], dim=1)  # (B, 576)\n",
        "        out = self.fuse(z)  # (B, 59)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "MfqktIHDimAQ"
      },
      "outputs": [],
      "source": [
        "# ----------------- MODEL: 5-CH RESNET50 + CLINICAL MLP (FUSION) -----------------\n",
        "class MobileNetV3L_5ch_Clinical(nn.Module):\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        base = models.mobilenet_v3_large(\n",
        "            weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        )\n",
        "        # adapt Conv2dNormActivation: 3→5 channels (init extra channels with mean RGB weights)\n",
        "        old = base.features[0][0]\n",
        "        new = nn.Conv2d(\n",
        "            5,\n",
        "            old.out_channels,\n",
        "            kernel_size=old.kernel_size,\n",
        "            stride=old.stride,\n",
        "            padding=old.padding,\n",
        "            bias=(old.bias is not None),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            new.weight[:, :3, :, :] = old.weight\n",
        "            mean_w = old.weight.mean(dim=1, keepdim=True)\n",
        "            new.weight[:, 3:5, :, :] = mean_w.repeat(1, 2, 1, 1)\n",
        "            if old.bias is not None:\n",
        "                new.bias.copy_(old.bias)\n",
        "        base.features[0][0] = new\n",
        "\n",
        "        in_f = base.classifier[-1].in_features\n",
        "        base.classifier[-1] = nn.Identity()\n",
        "        self.backbone = base\n",
        "\n",
        "        self.img_head = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25)\n",
        "        )\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.10),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(512 + 64, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x5, xclin):\n",
        "        f = self.backbone(x5)  # (B, 2048)\n",
        "        f = self.img_head(f)  # (B, 512)\n",
        "        g = self.clin_head(xclin)  # (B, 64)\n",
        "        z = torch.cat([f, g], dim=1)\n",
        "        out = self.fuse(z)  # (B, 59)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:28.785280Z",
          "iopub.status.busy": "2025-11-10T06:05:28.785012Z",
          "iopub.status.idle": "2025-11-10T06:05:28.792074Z",
          "shell.execute_reply": "2025-11-10T06:05:28.791195Z",
          "shell.execute_reply.started": "2025-11-10T06:05:28.785262Z"
        },
        "id": "TzfmS9nYimAR"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "    for x_img, x_clin, y in loader:\n",
        "        x_img = x_img.to(DEVICE)\n",
        "        x_clin = x_clin.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "        pred = model(x_img, x_clin)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x_img.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T06:05:52.866937Z",
          "iopub.status.busy": "2025-11-10T06:05:52.866250Z",
          "iopub.status.idle": "2025-11-10T06:09:44.041224Z",
          "shell.execute_reply": "2025-11-10T06:09:44.040337Z",
          "shell.execute_reply.started": "2025-11-10T06:05:52.866915Z"
        },
        "id": "qNE75zqYimAR",
        "lines_to_next_cell": 2,
        "outputId": "0c5b92f7-9d45-48f7-8ccf-9549a8abf20d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] 59 VF cols: ['VF1', 'VF2', 'VF3', 'VF4', 'VF5'] ... ['VF55', 'VF56', 'VF57', 'VF58', 'VF59']\n",
            "\n",
            "Epoch 01 | train_loss=482.6727  train_pMAE=20.338  train_MS=20.182 || val_loss=450.2610  val_pMAE=19.408  val_MS=19.210\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC_CDR.pth (pMAE=19.408)\n",
            "\n",
            "Epoch 02 | train_loss=179.4450  train_pMAE=11.041  train_MS=8.822 || val_loss=135.4271  val_pMAE=10.412  val_MS=9.022\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC_CDR.pth (pMAE=10.412)\n",
            "\n",
            "Epoch 03 | train_loss=60.1799  train_pMAE=6.198  train_MS=3.536 || val_loss=49.0235  val_pMAE=5.135  val_MS=3.822\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC_CDR.pth (pMAE=5.135)\n",
            "\n",
            "Epoch 04 | train_loss=50.5560  train_pMAE=5.685  train_MS=2.990 || val_loss=48.7353  val_pMAE=5.167  val_MS=3.908\n",
            "\n",
            "Epoch 05 | train_loss=47.1727  train_pMAE=5.445  train_MS=2.726 || val_loss=48.8136  val_pMAE=5.586  val_MS=3.948\n",
            "\n",
            "Epoch 06 | train_loss=46.1599  train_pMAE=5.389  train_MS=2.791 || val_loss=56.7647  val_pMAE=6.276  val_MS=4.758\n",
            "\n",
            "Epoch 07 | train_loss=43.6963  train_pMAE=5.261  train_MS=2.641 || val_loss=48.6998  val_pMAE=4.980  val_MS=3.600\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC_CDR.pth (pMAE=4.980)\n",
            "\n",
            "Epoch 08 | train_loss=42.3814  train_pMAE=5.145  train_MS=2.584 || val_loss=48.7552  val_pMAE=5.069  val_MS=3.667\n",
            "\n",
            "Epoch 09 | train_loss=38.6134  train_pMAE=4.903  train_MS=2.115 || val_loss=48.4660  val_pMAE=5.028  val_MS=3.610\n",
            "\n",
            "Epoch 10 | train_loss=38.6486  train_pMAE=4.885  train_MS=2.191 || val_loss=47.4802  val_pMAE=4.988  val_MS=3.536\n",
            "\n",
            "Epoch 11 | train_loss=37.5879  train_pMAE=4.777  train_MS=2.083 || val_loss=47.5179  val_pMAE=5.316  val_MS=3.772\n",
            "\n",
            "Epoch 12 | train_loss=37.2180  train_pMAE=4.740  train_MS=2.086 || val_loss=46.4840  val_pMAE=5.229  val_MS=3.657\n",
            "\n",
            "Epoch 13 | train_loss=35.1983  train_pMAE=4.637  train_MS=1.866 || val_loss=44.4853  val_pMAE=4.836  val_MS=3.242\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC_CDR.pth (pMAE=4.836)\n",
            "\n",
            "Epoch 14 | train_loss=35.5128  train_pMAE=4.650  train_MS=1.984 || val_loss=43.0958  val_pMAE=4.733  val_MS=3.061\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC_CDR.pth (pMAE=4.733)\n",
            "\n",
            "Epoch 15 | train_loss=35.5108  train_pMAE=4.623  train_MS=1.954 || val_loss=43.3249  val_pMAE=4.776  val_MS=3.161\n",
            "\n",
            "Epoch 16 | train_loss=34.8652  train_pMAE=4.610  train_MS=1.868 || val_loss=42.1608  val_pMAE=4.736  val_MS=2.991\n",
            "\n",
            "Epoch 17 | train_loss=34.7675  train_pMAE=4.567  train_MS=1.871 || val_loss=42.3059  val_pMAE=4.729  val_MS=3.022\n",
            "\n",
            "Epoch 18 | train_loss=34.2115  train_pMAE=4.528  train_MS=1.784 || val_loss=43.0145  val_pMAE=4.920  val_MS=3.196\n",
            "\n",
            "Epoch 19 | train_loss=34.9325  train_pMAE=4.598  train_MS=1.972 || val_loss=42.3674  val_pMAE=4.736  val_MS=3.042\n",
            "\n",
            "Epoch 20 | train_loss=33.2211  train_pMAE=4.455  train_MS=1.745 || val_loss=43.8952  val_pMAE=4.991  val_MS=3.385\n",
            "\n",
            "Epoch 21 | train_loss=33.2338  train_pMAE=4.490  train_MS=1.794 || val_loss=42.5685  val_pMAE=4.815  val_MS=3.144\n",
            "\n",
            "Epoch 22 | train_loss=33.2904  train_pMAE=4.458  train_MS=1.797 || val_loss=42.2639  val_pMAE=4.801  val_MS=3.120\n",
            "\n",
            "Epoch 23 | train_loss=33.7006  train_pMAE=4.520  train_MS=1.881 || val_loss=41.8723  val_pMAE=4.661  val_MS=2.952\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC_CDR.pth (pMAE=4.661)\n",
            "\n",
            "Epoch 24 | train_loss=33.5133  train_pMAE=4.465  train_MS=1.803 || val_loss=42.9872  val_pMAE=4.947  val_MS=3.254\n",
            "\n",
            "Epoch 25 | train_loss=32.8969  train_pMAE=4.409  train_MS=1.806 || val_loss=43.2376  val_pMAE=4.983  val_MS=3.330\n",
            "\n",
            "Epoch 26 | train_loss=32.3752  train_pMAE=4.388  train_MS=1.726 || val_loss=42.0063  val_pMAE=4.775  val_MS=3.085\n",
            "\n",
            "Epoch 27 | train_loss=33.2411  train_pMAE=4.448  train_MS=1.833 || val_loss=42.1933  val_pMAE=4.704  val_MS=3.067\n",
            "\n",
            "Epoch 28 | train_loss=33.0241  train_pMAE=4.460  train_MS=1.932 || val_loss=41.6531  val_pMAE=4.678  val_MS=3.002\n",
            "\n",
            "Epoch 29 | train_loss=31.6051  train_pMAE=4.317  train_MS=1.596 || val_loss=42.0529  val_pMAE=4.744  val_MS=3.090\n",
            "\n",
            "Epoch 30 | train_loss=31.9319  train_pMAE=4.356  train_MS=1.730 || val_loss=41.9858  val_pMAE=4.670  val_MS=3.019\n",
            "\n",
            "Epoch 31 | train_loss=32.2412  train_pMAE=4.378  train_MS=1.748 || val_loss=42.1140  val_pMAE=4.636  val_MS=3.002\n",
            "\n",
            " ✅ Saved BEST → ./checkpoints/best_mobilenetv3l_ROI_ODOC_CDR.pth (pMAE=4.636)\n",
            "\n",
            "Epoch 32 | train_loss=32.2524  train_pMAE=4.381  train_MS=1.739 || val_loss=41.7340  val_pMAE=4.639  val_MS=2.983\n",
            "\n",
            "Epoch 33 | train_loss=31.7079  train_pMAE=4.344  train_MS=1.693 || val_loss=41.4183  val_pMAE=4.654  val_MS=2.963\n",
            "\n",
            "Epoch 34 | train_loss=31.8250  train_pMAE=4.349  train_MS=1.704 || val_loss=41.2640  val_pMAE=4.663  val_MS=2.969\n",
            "\n",
            "Epoch 35 | train_loss=31.3282  train_pMAE=4.304  train_MS=1.588 || val_loss=41.5606  val_pMAE=4.702  val_MS=3.038\n",
            "\n",
            "Epoch 36 | train_loss=32.1307  train_pMAE=4.359  train_MS=1.685 || val_loss=41.3659  val_pMAE=4.647  val_MS=2.976\n",
            "\n",
            "Epoch 37 | train_loss=31.3711  train_pMAE=4.319  train_MS=1.600 || val_loss=41.1869  val_pMAE=4.673  val_MS=2.991\n",
            "\n",
            "Epoch 38 | train_loss=31.7329  train_pMAE=4.328  train_MS=1.650 || val_loss=41.1028  val_pMAE=4.674  val_MS=2.989\n",
            "\n",
            "Epoch 39 | train_loss=31.5326  train_pMAE=4.326  train_MS=1.649 || val_loss=40.8511  val_pMAE=4.656  val_MS=2.954\n",
            "\n",
            "Epoch 40 | train_loss=31.6929  train_pMAE=4.299  train_MS=1.682 || val_loss=40.8371  val_pMAE=4.648  val_MS=2.943\n",
            "\n",
            "Epoch 41 | train_loss=31.2122  train_pMAE=4.316  train_MS=1.601 || val_loss=41.0007  val_pMAE=4.682  val_MS=2.987\n",
            "\n",
            "Epoch 42 | train_loss=31.7680  train_pMAE=4.327  train_MS=1.683 || val_loss=40.9159  val_pMAE=4.636  val_MS=2.941\n",
            "\n",
            "Early stopping at epoch 42 (best val pMAE=4.636)\n"
          ]
        }
      ],
      "source": [
        "def train_resnet50_roi_odoc_with_cdr(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01):\n",
        "    import os\n",
        "    import random\n",
        "\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    # --- Early stopper that saves best on val pMAE ---\n",
        "    class EarlyStopper:\n",
        "        def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "            self.patience = int(patience)\n",
        "            self.min_delta = float(min_delta)\n",
        "            self.ckpt_path = ckpt_path\n",
        "            self.best = float(\"inf\")\n",
        "            self.bad_epochs = 0\n",
        "\n",
        "        def step(self, val_pmae, model, epoch_meta=None):\n",
        "            if val_pmae < self.best - self.min_delta:\n",
        "                self.best = val_pmae\n",
        "                self.bad_epochs = 0\n",
        "                if self.ckpt_path:\n",
        "                    torch.save(\n",
        "                        {\n",
        "                            \"model\": model.state_dict(),\n",
        "                            \"val_pointwise_mae\": self.best,\n",
        "                            **(epoch_meta or {}),\n",
        "                        },\n",
        "                        self.ckpt_path,\n",
        "                    )\n",
        "                return False\n",
        "            else:\n",
        "                self.bad_epochs += 1\n",
        "                return self.bad_epochs > self.patience\n",
        "\n",
        "    # --- read & detect columns ---\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] {len(vf_cols)} VF cols: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # --- ADD CDR BEFORE SPLIT ---\n",
        "    rows = augment_rows_with_cdr_psd(rows, image_col, vf_cols)  # <-- adds CDR fields per row\n",
        "\n",
        "    random.shuffle(rows)\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    # --- fit clinical stats on TRAIN only (handles imputation/encoding) ---\n",
        "    clin_stats = fit_clinical_stats(train_rows, CLIN_NUM_COLS)\n",
        "    clin_dim = len(CLIN_NUM_COLS) + len(CLIN_CAT_COLS)\n",
        "\n",
        "    # --- datasets / loaders: use the dataset that returns (x5, x_clin, y) ---\n",
        "    train_ds = ROI_ODOC_Clinical_Dataset(train_rows, image_col, vf_cols, clin_stats, train=True)\n",
        "    val_ds = ROI_ODOC_Clinical_Dataset(val_rows, image_col, vf_cols, clin_stats, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # --- model that accepts 5ch image + clinical vector ---\n",
        "    model = MobileNetV3L_5ch_Clinical(clin_dim=clin_dim, out_dim=NUM_POINTS, pretrained=True).to(\n",
        "        DEVICE\n",
        "    )\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    ckpt = os.path.join(CHECK_DIR, \"best_mobilenetv3l_ROI_ODOC_CDR.pth\")\n",
        "    stopper = EarlyStopper(patience=PATIENCE, min_delta=MIN_DELTA, ckpt_path=ckpt)\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)  # should read (x5, x_clin, y) inside\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        print(\n",
        "            f\"\\nEpoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f}  train_pMAE={tr['pointwise_mae']:.3f}  train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f}  val_pMAE={va['pointwise_mae']:.3f}  val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # step LR on validation pMAE\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        # save best & decide stopping\n",
        "        improved = va[\"pointwise_mae\"] < stopper.best - MIN_DELTA\n",
        "        should_stop = stopper.step(va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch})\n",
        "        if improved:\n",
        "            print(f\"\\n ✅ Saved BEST → {ckpt} (pMAE={stopper.best:.3f})\")\n",
        "\n",
        "        if should_stop:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (best val pMAE={stopper.best:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best before returning\n",
        "    state = torch.load(ckpt, map_location=DEVICE)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt, clin_dim\n",
        "\n",
        "\n",
        "# run training and expose globals\n",
        "model_odoc, train_dl_odoc, val_dl_odoc, image_col_odoc, vf_cols_odoc, CKPT_ODOC, CLIN_DIM_OD = (\n",
        "    train_resnet50_roi_odoc_with_cdr(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T06:31:38.896271Z",
          "iopub.status.busy": "2025-11-10T06:31:38.895650Z",
          "iopub.status.idle": "2025-11-10T06:31:39.930251Z",
          "shell.execute_reply": "2025-11-10T06:31:39.929374Z",
          "shell.execute_reply.started": "2025-11-10T06:31:38.896246Z"
        },
        "id": "IornJeKTimAS",
        "lines_to_next_cell": 2,
        "outputId": "fa23fceb-963a-4a6d-a483-909d102e6b20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n",
            "\n",
            "== ROI+Clinical (with CDR): POINTWISE ==\n",
            "RMSE: 6.4895 | MAE: 4.6358 | R²: 0.4951\n",
            "\n",
            "== ROI+Clinical (with CDR): MEAN SENSITIVITY ==\n",
            "RMSE: 4.2246 | MAE: 3.0022 | R²: 0.5021\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------\n",
        "# ✅ Corrected Cell 10: Reload best model & evaluate\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Use the model already trained in Cell 9\n",
        "# model_odoc → best model returned by train_resnet50_roi_odoc_with_cdr\n",
        "# val_dl_odoc → validation dataloader\n",
        "# CKPT_ODOC → checkpoint path\n",
        "# CLIN_DIM_OD → clinical feature dimension\n",
        "# image_col_odoc, vf_cols_odoc already created\n",
        "\n",
        "best_roi_clin = model_odoc  # model already returned from training\n",
        "best_roi_clin.eval()\n",
        "\n",
        "# Load the best checkpoint\n",
        "state = torch.load(CKPT_ODOC, map_location=DEVICE)\n",
        "best_roi_clin.load_state_dict(state[\"model\"])\n",
        "best_roi_clin.eval()\n",
        "\n",
        "# Collect predictions\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x_img, x_clin, y in val_dl_odoc:\n",
        "        x_img = x_img.to(DEVICE)\n",
        "        x_clin = x_clin.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        p = best_roi_clin(x_img, x_clin)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)\n",
        "\n",
        "\n",
        "# --- Metrics ---\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_value(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# Pointwise metrics (VF1..VF59 flattened)\n",
        "pw_true = y_true.reshape(-1)\n",
        "pw_pred = y_pred.reshape(-1)\n",
        "\n",
        "print(\"\\n== ROI+Clinical (with CDR): POINTWISE ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(pw_true, pw_pred):.4f} | \"\n",
        "    f\"MAE: {mae_value(pw_true, pw_pred):.4f} | \"\n",
        "    f\"R²: {r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "\n",
        "# Mean Sensitivity metrics\n",
        "t_mean = y_true.mean(dim=1)\n",
        "p_mean = y_pred.mean(dim=1)\n",
        "\n",
        "print(\"\\n== ROI+Clinical (with CDR): MEAN SENSITIVITY ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | \"\n",
        "    f\"MAE: {mae_value(t_mean, p_mean):.4f} | \"\n",
        "    f\"R²: {r2(t_mean, p_mean):.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T10:08:07.655701Z",
          "iopub.status.busy": "2025-11-08T10:08:07.655425Z",
          "iopub.status.idle": "2025-11-08T10:08:07.659428Z"
        },
        "id": "RphJ5W-RimAT"
      },
      "source": [
        "# 5. ROI+ OD/OC + Clinical MobileNetV3L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:33.300575Z",
          "iopub.status.busy": "2025-11-10T04:06:33.299912Z",
          "iopub.status.idle": "2025-11-10T04:06:33.432177Z",
          "shell.execute_reply": "2025-11-10T04:06:33.431611Z",
          "shell.execute_reply.started": "2025-11-10T04:06:33.300551Z"
        },
        "id": "mPP-E45DimAT",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# FULL FUSION: ROI + OD/OC + Clinical   →  VF (59)\n",
        "# ==========================\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "\n",
        "# ----------------- PATHS & CONFIG -----------------\n",
        "\n",
        "os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LR = 1e-4\n",
        "WD = 1e-4\n",
        "NUM_WORKERS = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "NUM_POINTS = 59  # VF1..VF59\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:36.730860Z",
          "iopub.status.busy": "2025-11-10T04:06:36.730557Z",
          "iopub.status.idle": "2025-11-10T04:06:36.740736Z",
          "shell.execute_reply": "2025-11-10T04:06:36.740178Z",
          "shell.execute_reply.started": "2025-11-10T04:06:36.730834Z"
        },
        "id": "GRVLg6NgimAU"
      },
      "outputs": [],
      "source": [
        "# Clinical features for fusion (present or computed)\n",
        "CLIN_NUM_COLS = [\"AGE\", \"IOP_y\", \"CDR\"]\n",
        "CLIN_CAT_COLS = [\"GENDER\"]\n",
        "IMAGE_COLS_CANDIDATES = [\n",
        "    \"Corresponding CFP\",\n",
        "    \"image\",\n",
        "    \"image_name\",\n",
        "    \"img\",\n",
        "    \"image_path\",\n",
        "    \"filename\",\n",
        "    \"file\",\n",
        "]\n",
        "\n",
        "\n",
        "# ----------------- CSV UTILS -----------------\n",
        "def read_csv(fp: str) -> List[Dict[str, str]]:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()]\n",
        "    header = [h.strip() for h in lines[0].split(\",\")]\n",
        "    rows = []\n",
        "    for line in lines[1:]:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        parts = (parts + [\"\"] * len(header))[: len(header)]\n",
        "        rows.append({h: parts[i] for i, h in enumerate(header)})\n",
        "    return rows\n",
        "\n",
        "\n",
        "def detect_columns(rows: List[Dict[str, str]]) -> Tuple[str, List[str]]:\n",
        "    if not rows:\n",
        "        raise ValueError(\"CSV has no rows.\")\n",
        "    cols = list(rows[0].keys())\n",
        "\n",
        "    # image column (prefer Corresponding CFP)\n",
        "    image_col = None\n",
        "    for c in IMAGE_COLS_CANDIDATES:\n",
        "        if c in cols:\n",
        "            image_col = c\n",
        "            break\n",
        "    if image_col is None:\n",
        "        for c in cols:\n",
        "            if any(\n",
        "                rows[i][c].lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                for i in range(min(10, len(rows)))\n",
        "            ):\n",
        "                image_col = c\n",
        "                break\n",
        "    if image_col is None:\n",
        "        raise ValueError(\"Could not find image filename column.\")\n",
        "\n",
        "    # prefer explicit VF1..VF59\n",
        "    vf_pref = [f\"VF{i}\" for i in range(1, 60)]\n",
        "    if all(c in cols for c in vf_pref):\n",
        "        return image_col, vf_pref\n",
        "\n",
        "    # fallback: detect numeric columns (exclude clinical & image & VF0/VF60)\n",
        "    excluded = set([image_col] + CLIN_NUM_COLS + CLIN_CAT_COLS + [\"VF0\", \"VF60\"])\n",
        "    cand = []\n",
        "    for c in cols:\n",
        "        if c in excluded:\n",
        "            continue\n",
        "        ok = True\n",
        "        for r in rows[: min(20, len(rows))]:\n",
        "            v = r[c].strip()\n",
        "            if v == \"\":\n",
        "                ok = False\n",
        "                break\n",
        "            try:\n",
        "                float(v)\n",
        "            except:\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            cand.append(c)\n",
        "    if len(cand) < NUM_POINTS:\n",
        "        raise ValueError(f\"Not enough numeric VF columns; found {len(cand)}, need {NUM_POINTS}.\")\n",
        "\n",
        "    def keyfun(name):\n",
        "        m = re.search(r\"(\\d+)$\", name)\n",
        "        return (name, int(m.group(1)) if m else 9999)\n",
        "\n",
        "    cand = sorted(cand, key=keyfun)[:NUM_POINTS]\n",
        "    return image_col, cand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:40.269978Z",
          "iopub.status.busy": "2025-11-10T04:06:40.269252Z",
          "iopub.status.idle": "2025-11-10T04:06:40.280558Z",
          "shell.execute_reply": "2025-11-10T04:06:40.279748Z",
          "shell.execute_reply.started": "2025-11-10T04:06:40.269944Z"
        },
        "id": "UEMSMEL_imAV"
      },
      "outputs": [],
      "source": [
        "# ----------------- OD/OC POLYGONS → CDR & MASKS -----------------\n",
        "OD_LABELS = {\"od\", \"disc\", \"optic_disc\", \"optic-disc\", \"optic disc\"}\n",
        "OC_LABELS = {\"oc\", \"cup\", \"optic_cup\", \"optic-cup\", \"optic cup\"}\n",
        "\n",
        "\n",
        "def _read_labelme_polys(json_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    od_polys, oc_polys = [], []\n",
        "    for sh in data.get(\"shapes\", []):\n",
        "        lab = str(sh.get(\"label\", \"\")).strip().lower()\n",
        "        pts = sh.get(\"points\", [])\n",
        "        if len(pts) < 3:\n",
        "            continue\n",
        "        if lab in OD_LABELS:\n",
        "            od_polys.append(pts)\n",
        "        if lab in OC_LABELS:\n",
        "            oc_polys.append(pts)\n",
        "\n",
        "    # keep polygon with max vertical height\n",
        "    def vheight(poly):\n",
        "        ys = [p[1] for p in poly]\n",
        "        return (max(ys) - min(ys)) if ys else 0.0\n",
        "\n",
        "    if len(od_polys) > 1:\n",
        "        od_polys = [max(od_polys, key=vheight)]\n",
        "    if len(oc_polys) > 1:\n",
        "        oc_polys = [max(oc_polys, key=vheight)]\n",
        "    return od_polys, oc_polys\n",
        "\n",
        "\n",
        "def _guess_json(img_name: str):\n",
        "    base = os.path.splitext(os.path.basename(img_name))[0]\n",
        "    for ext in (\".json\", \".JSON\", \".Json\"):\n",
        "        p = os.path.join(JSON_DIR, base + ext)\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def compute_cdr_from_json(img_name: str):\n",
        "    \"\"\"CDR = vertical cup height / vertical disc height (from polygons).\"\"\"\n",
        "    jpath = _guess_json(img_name)\n",
        "    if not jpath:\n",
        "        return None\n",
        "    try:\n",
        "        od_polys, oc_polys = _read_labelme_polys(jpath)\n",
        "        if not od_polys or not oc_polys:\n",
        "            return None\n",
        "\n",
        "        def vheight(poly):\n",
        "            ys = [float(y) for _, y in poly]\n",
        "            return max(ys) - min(ys) if ys else 0.0\n",
        "\n",
        "        h_od = vheight(od_polys[0])\n",
        "        h_oc = vheight(oc_polys[0])\n",
        "        if h_od <= 0:\n",
        "            return None\n",
        "        return float(h_oc / h_od)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] CDR parse failed for {img_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def build_masks_from_labelme(img_pil: Image.Image, img_name: str, out_size: int):\n",
        "    \"\"\"Binary OD/OC masks (L mode), resized to out_size.\"\"\"\n",
        "    W, H = img_pil.size\n",
        "    od_mask = Image.new(\"L\", (W, H), 0)\n",
        "    oc_mask = Image.new(\"L\", (W, H), 0)\n",
        "    jpath = _guess_json(img_name)\n",
        "    if jpath:\n",
        "        try:\n",
        "            od_polys, oc_polys = _read_labelme_polys(jpath)\n",
        "            d_od = ImageDraw.Draw(od_mask)\n",
        "            d_oc = ImageDraw.Draw(oc_mask)\n",
        "            for poly in od_polys:\n",
        "                d_od.polygon(poly, outline=1, fill=1)\n",
        "            for poly in oc_polys:\n",
        "                d_oc.polygon(poly, outline=1, fill=1)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] mask parse {jpath}: {e}\")\n",
        "    od_mask = od_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    oc_mask = oc_mask.resize((out_size, out_size), resample=Image.NEAREST)\n",
        "    return od_mask, oc_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:43.829520Z",
          "iopub.status.busy": "2025-11-10T04:06:43.828915Z",
          "iopub.status.idle": "2025-11-10T04:06:43.833579Z",
          "shell.execute_reply": "2025-11-10T04:06:43.832931Z",
          "shell.execute_reply.started": "2025-11-10T04:06:43.829498Z"
        },
        "id": "dJ3twnEcimAW"
      },
      "outputs": [],
      "source": [
        "# ----------------- AUGMENT ROWS WITH CDR  -----------------\n",
        "def augment_rows_with_cdr_psd(rows, image_col, vf_cols):\n",
        "    augmented = []\n",
        "    miss_cdr = miss_psd = 0\n",
        "    for r in rows:\n",
        "        r2 = dict(r)\n",
        "        cdr = compute_cdr_from_json(r2[image_col])\n",
        "        if cdr is None:\n",
        "            miss_cdr += 1\n",
        "        r2[\"CDR\"] = cdr\n",
        "\n",
        "        augmented.append(r2)\n",
        "    return augmented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:47.264094Z",
          "iopub.status.busy": "2025-11-10T04:06:47.263774Z",
          "iopub.status.idle": "2025-11-10T04:06:47.271273Z",
          "shell.execute_reply": "2025-11-10T04:06:47.270696Z",
          "shell.execute_reply.started": "2025-11-10T04:06:47.264074Z"
        },
        "id": "0bmZvum1imAW"
      },
      "outputs": [],
      "source": [
        "# ----------------- CLINICAL PREPROCESS -----------------\n",
        "def to_float(x):\n",
        "    x = str(x).strip()\n",
        "    if x == \"\":\n",
        "        return None\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def fit_clinical_stats(rows, clin_num_cols):\n",
        "    stats = {}\n",
        "    for c in clin_num_cols:\n",
        "        vals = [to_float(r.get(c, \"\")) for r in rows]\n",
        "        vals = [v for v in vals if v is not None]\n",
        "        mean = np.mean(vals) if vals else 0.0\n",
        "        std = np.std(vals) if vals else 1.0\n",
        "        if std == 0:\n",
        "            std = 1.0\n",
        "        stats[c] = (float(mean), float(std))\n",
        "    return stats\n",
        "\n",
        "\n",
        "def encode_gender(x):\n",
        "    s = str(x).strip().lower()\n",
        "    if s in (\"m\", \"male\", \"man\"):\n",
        "        return 1.0\n",
        "    if s in (\"f\", \"female\", \"woman\"):\n",
        "        return 0.0\n",
        "    return 0.5  # unknown/other\n",
        "\n",
        "\n",
        "def build_clinical_vector(r, stats):\n",
        "    vec = []\n",
        "    for c in CLIN_NUM_COLS:\n",
        "        v = to_float(r.get(c, \"\"))\n",
        "        mean, std = stats[c]\n",
        "        v = mean if v is None else v\n",
        "        v = (v - mean) / std\n",
        "        vec.append(v)\n",
        "    for c in CLIN_CAT_COLS:\n",
        "        if c == \"GENDER\":\n",
        "            vec.append(encode_gender(r.get(c, \"\")))\n",
        "        else:\n",
        "            vec.append(0.0)\n",
        "    return torch.tensor(vec, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:50.779769Z",
          "iopub.status.busy": "2025-11-10T04:06:50.779086Z",
          "iopub.status.idle": "2025-11-10T04:06:50.787036Z",
          "shell.execute_reply": "2025-11-10T04:06:50.786326Z",
          "shell.execute_reply.started": "2025-11-10T04:06:50.779743Z"
        },
        "id": "Eqw31EDoimAX"
      },
      "outputs": [],
      "source": [
        "# ----------------- DATASET: 5-CH ROI + CLINICAL -----------------\n",
        "class ROI_ODOC_Clinical_Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self, rows, image_col, vf_cols, clin_stats, train=True, img_root=ROI_DIR, img_size=IMG_SIZE\n",
        "    ):\n",
        "        self.rows = rows\n",
        "        self.image_col = image_col\n",
        "        self.vf_cols = vf_cols\n",
        "        self.clin_stats = clin_stats\n",
        "        self.train = train\n",
        "        self.img_root = img_root\n",
        "        self.img_size = img_size\n",
        "\n",
        "        norm = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        aug = []\n",
        "        if train:\n",
        "            aug = [\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "            ]\n",
        "        self.rgb_tf = transforms.Compose(\n",
        "            [transforms.Resize((img_size, img_size)), transforms.ToTensor(), *aug, norm]\n",
        "        )\n",
        "        self.mask_tf = transforms.ToTensor()  # L→(1,H,W) float {0,1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.rows[idx]\n",
        "        fn = r[self.image_col]\n",
        "        path = fn if os.path.isabs(fn) else os.path.join(self.img_root, fn)\n",
        "\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        od_img, oc_img = build_masks_from_labelme(img, fn, self.img_size)\n",
        "\n",
        "        x_rgb = self.rgb_tf(img)  # (3,H,W)\n",
        "        x_od = self.mask_tf(od_img)  # (1,H,W)\n",
        "        x_oc = self.mask_tf(oc_img)  # (1,H,W)\n",
        "        x5 = torch.cat([x_rgb, x_od, x_oc], dim=0)  # (5,H,W)\n",
        "\n",
        "        x_clin = build_clinical_vector(r, self.clin_stats)  # (clin_dim,)\n",
        "        y = torch.tensor([float(r[c]) for c in self.vf_cols], dtype=torch.float32)  # (59,)\n",
        "\n",
        "        return x5, x_clin, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:54.064124Z",
          "iopub.status.busy": "2025-11-10T04:06:54.063332Z",
          "iopub.status.idle": "2025-11-10T04:06:54.071929Z",
          "shell.execute_reply": "2025-11-10T04:06:54.071421Z",
          "shell.execute_reply.started": "2025-11-10T04:06:54.064093Z"
        },
        "id": "RiThfSdMimAX"
      },
      "outputs": [],
      "source": [
        "# ----------------- MODEL: 5-CH RESNET50 + CLINICAL MLP (FUSION) -----------------\n",
        "class MobileNetV3L_5ch_Clinical(nn.Module):\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True):\n",
        "        super().__init__()\n",
        "        base = models.mobilenet_v3_large(\n",
        "            weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "        )\n",
        "        # adapt Conv2dNormActivation: 3→5 channels (init extra channels with mean RGB weights)\n",
        "        old = base.features[0][0]\n",
        "        new = nn.Conv2d(\n",
        "            5,\n",
        "            old.out_channels,\n",
        "            kernel_size=old.kernel_size,\n",
        "            stride=old.stride,\n",
        "            padding=old.padding,\n",
        "            bias=(old.bias is not None),\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            new.weight[:, :3, :, :] = old.weight\n",
        "            mean_w = old.weight.mean(dim=1, keepdim=True)\n",
        "            new.weight[:, 3:5, :, :] = mean_w.repeat(1, 2, 1, 1)\n",
        "            if old.bias is not None:\n",
        "                new.bias.copy_(old.bias)\n",
        "        base.features[0][0] = new\n",
        "\n",
        "        in_f = base.classifier[-1].in_features\n",
        "        base.classifier[-1] = nn.Identity()\n",
        "        self.backbone = base\n",
        "\n",
        "        self.img_head = nn.Sequential(\n",
        "            nn.Linear(in_f, 512), nn.ReLU(inplace=True), nn.Dropout(0.25)\n",
        "        )\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.10),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(512 + 64, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(256, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x5, xclin):\n",
        "        f = self.backbone(x5)  # (B, 2048)\n",
        "        f = self.img_head(f)  # (B, 512)\n",
        "        g = self.clin_head(xclin)  # (B, 64)\n",
        "        z = torch.cat([f, g], dim=1)\n",
        "        out = self.fuse(z)  # (B, 59)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:06:57.415483Z",
          "iopub.status.busy": "2025-11-10T04:06:57.414915Z",
          "iopub.status.idle": "2025-11-10T04:06:57.422155Z",
          "shell.execute_reply": "2025-11-10T04:06:57.421329Z",
          "shell.execute_reply.started": "2025-11-10T04:06:57.415462Z"
        },
        "id": "mPROHcAdimAY"
      },
      "outputs": [],
      "source": [
        "# ----------------- METRICS & EPOCH LOOP (same logic as your earlier code) -----------------\n",
        "@torch.no_grad()\n",
        "def mae(pred, true):  # pointwise MAE\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ms_mae(pred, true):  # MS per-sample then MAE\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def run_epoch(model, loader, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "    for x5, xclin, y in loader:\n",
        "        x5, xclin, y = x5.to(DEVICE), xclin.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "        pred = model(x5, xclin)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x5.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += mae(pred, y) * bs\n",
        "        msmae_sum += ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\"loss\": loss_sum / n, \"pointwise_mae\": pmae_sum / n, \"ms_mae\": msmae_sum / n}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T04:08:38.414507Z",
          "iopub.status.busy": "2025-11-10T04:08:38.414201Z",
          "iopub.status.idle": "2025-11-10T04:17:55.812102Z",
          "shell.execute_reply": "2025-11-10T04:17:55.810937Z",
          "shell.execute_reply.started": "2025-11-10T04:08:38.414480Z"
        },
        "id": "SCOJrDfwimAY",
        "lines_to_next_cell": 2,
        "outputId": "cc0715b8-42f2-4b66-8528-3e6e809c0192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] image column: Corresponding CFP\n",
            "[OK] 59 VF cols detected: ['VF1', 'VF2', 'VF3', 'VF4', 'VF5'] ... ['VF55', 'VF56', 'VF57', 'VF58', 'VF59']\n",
            "Epoch 01 | train_loss=482.3802 train_pMAE=20.331 train_MS=20.176 || val_loss=442.9284 val_pMAE=19.239 val_MS=19.030\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=19.239)\n",
            "Epoch 02 | train_loss=175.4235 train_pMAE=10.899 train_MS=8.607 || val_loss=86.2756 val_pMAE=8.033 val_MS=6.457\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=8.033)\n",
            "Epoch 03 | train_loss=60.6013 train_pMAE=6.258 train_MS=3.625 || val_loss=61.5353 val_pMAE=5.596 val_MS=4.567\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=5.596)\n",
            "Epoch 04 | train_loss=50.2914 train_pMAE=5.644 train_MS=2.971 || val_loss=49.5158 val_pMAE=5.480 val_MS=3.975\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=5.480)\n",
            "Epoch 05 | train_loss=47.3519 train_pMAE=5.442 train_MS=2.793 || val_loss=46.4547 val_pMAE=5.048 val_MS=3.722\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=5.048)\n",
            "Epoch 06 | train_loss=42.6037 train_pMAE=5.190 train_MS=2.309 || val_loss=44.0566 val_pMAE=4.890 val_MS=3.471\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.890)\n",
            "Epoch 07 | train_loss=43.4500 train_pMAE=5.251 train_MS=2.591 || val_loss=52.3555 val_pMAE=5.080 val_MS=3.831\n",
            "Epoch 08 | train_loss=40.8806 train_pMAE=5.044 train_MS=2.340 || val_loss=45.2237 val_pMAE=4.791 val_MS=3.218\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.791)\n",
            "Epoch 09 | train_loss=39.7607 train_pMAE=4.922 train_MS=2.267 || val_loss=47.1238 val_pMAE=4.881 val_MS=3.467\n",
            "Epoch 10 | train_loss=38.5877 train_pMAE=4.849 train_MS=2.154 || val_loss=44.8720 val_pMAE=5.166 val_MS=3.462\n",
            "Epoch 11 | train_loss=38.0827 train_pMAE=4.833 train_MS=2.192 || val_loss=43.3892 val_pMAE=4.835 val_MS=3.131\n",
            "Epoch 12 | train_loss=36.9025 train_pMAE=4.766 train_MS=2.107 || val_loss=43.4580 val_pMAE=4.727 val_MS=3.007\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.727)\n",
            "Epoch 13 | train_loss=37.8728 train_pMAE=4.844 train_MS=2.322 || val_loss=42.6766 val_pMAE=4.883 val_MS=3.145\n",
            "Epoch 14 | train_loss=35.4488 train_pMAE=4.622 train_MS=2.021 || val_loss=43.3086 val_pMAE=4.982 val_MS=3.271\n",
            "Epoch 15 | train_loss=34.4700 train_pMAE=4.554 train_MS=1.882 || val_loss=42.3405 val_pMAE=4.921 val_MS=3.171\n",
            "Epoch 16 | train_loss=34.6391 train_pMAE=4.582 train_MS=2.010 || val_loss=42.9447 val_pMAE=4.948 val_MS=3.281\n",
            "Epoch 17 | train_loss=33.6097 train_pMAE=4.474 train_MS=1.858 || val_loss=40.8868 val_pMAE=4.826 val_MS=3.024\n",
            "Epoch 18 | train_loss=33.7091 train_pMAE=4.483 train_MS=1.897 || val_loss=40.4221 val_pMAE=4.676 val_MS=2.889\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.676)\n",
            "Epoch 19 | train_loss=33.5513 train_pMAE=4.488 train_MS=1.839 || val_loss=40.3159 val_pMAE=4.695 val_MS=2.917\n",
            "Epoch 20 | train_loss=33.4736 train_pMAE=4.469 train_MS=1.919 || val_loss=40.4559 val_pMAE=4.797 val_MS=3.013\n",
            "Epoch 21 | train_loss=33.1345 train_pMAE=4.438 train_MS=1.828 || val_loss=39.9818 val_pMAE=4.724 val_MS=2.938\n",
            "Epoch 22 | train_loss=32.3086 train_pMAE=4.376 train_MS=1.767 || val_loss=39.1871 val_pMAE=4.663 val_MS=2.849\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.663)\n",
            "Epoch 23 | train_loss=32.2408 train_pMAE=4.388 train_MS=1.779 || val_loss=41.2368 val_pMAE=4.956 val_MS=3.205\n",
            "Epoch 24 | train_loss=32.2250 train_pMAE=4.385 train_MS=1.797 || val_loss=39.5076 val_pMAE=4.603 val_MS=2.802\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.603)\n",
            "Epoch 25 | train_loss=31.9304 train_pMAE=4.327 train_MS=1.771 || val_loss=39.3961 val_pMAE=4.574 val_MS=2.763\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.574)\n",
            "Epoch 26 | train_loss=31.5185 train_pMAE=4.303 train_MS=1.674 || val_loss=39.5645 val_pMAE=4.713 val_MS=2.924\n",
            "Epoch 27 | train_loss=31.5663 train_pMAE=4.316 train_MS=1.744 || val_loss=38.9010 val_pMAE=4.587 val_MS=2.740\n",
            "Epoch 28 | train_loss=31.1615 train_pMAE=4.287 train_MS=1.681 || val_loss=40.6826 val_pMAE=4.836 val_MS=3.117\n",
            "Epoch 29 | train_loss=30.8921 train_pMAE=4.269 train_MS=1.706 || val_loss=38.7358 val_pMAE=4.458 val_MS=2.661\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.458)\n",
            "Epoch 30 | train_loss=31.0944 train_pMAE=4.280 train_MS=1.772 || val_loss=38.9989 val_pMAE=4.449 val_MS=2.670\n",
            "Epoch 31 | train_loss=30.1519 train_pMAE=4.210 train_MS=1.650 || val_loss=38.6933 val_pMAE=4.523 val_MS=2.755\n",
            "Epoch 32 | train_loss=30.0357 train_pMAE=4.216 train_MS=1.643 || val_loss=38.9788 val_pMAE=4.452 val_MS=2.704\n",
            "Epoch 33 | train_loss=29.7800 train_pMAE=4.179 train_MS=1.645 || val_loss=38.2895 val_pMAE=4.615 val_MS=2.826\n",
            "Epoch 34 | train_loss=30.4387 train_pMAE=4.234 train_MS=1.706 || val_loss=38.7393 val_pMAE=4.651 val_MS=2.929\n",
            "Epoch 35 | train_loss=28.9510 train_pMAE=4.119 train_MS=1.519 || val_loss=38.2883 val_pMAE=4.468 val_MS=2.687\n",
            "Epoch 36 | train_loss=29.3840 train_pMAE=4.144 train_MS=1.587 || val_loss=38.3853 val_pMAE=4.455 val_MS=2.748\n",
            "Epoch 37 | train_loss=29.3975 train_pMAE=4.138 train_MS=1.599 || val_loss=37.8635 val_pMAE=4.369 val_MS=2.598\n",
            "  -> saved new best to ./checkpoints/best_full_fusion_ROI_ODOC_CLIN1.pth (val pMAE=4.369)\n",
            "Epoch 38 | train_loss=29.0496 train_pMAE=4.143 train_MS=1.550 || val_loss=38.3719 val_pMAE=4.522 val_MS=2.816\n",
            "Epoch 39 | train_loss=29.4184 train_pMAE=4.162 train_MS=1.625 || val_loss=38.3680 val_pMAE=4.464 val_MS=2.735\n",
            "Epoch 40 | train_loss=29.4943 train_pMAE=4.173 train_MS=1.686 || val_loss=37.9447 val_pMAE=4.400 val_MS=2.643\n",
            "Epoch 41 | train_loss=28.6523 train_pMAE=4.106 train_MS=1.539 || val_loss=39.0059 val_pMAE=4.677 val_MS=2.967\n",
            "Epoch 42 | train_loss=28.1477 train_pMAE=4.040 train_MS=1.408 || val_loss=37.7680 val_pMAE=4.416 val_MS=2.662\n",
            "Epoch 43 | train_loss=28.7587 train_pMAE=4.109 train_MS=1.606 || val_loss=37.9396 val_pMAE=4.497 val_MS=2.772\n",
            "Epoch 44 | train_loss=28.3636 train_pMAE=4.072 train_MS=1.521 || val_loss=38.3002 val_pMAE=4.552 val_MS=2.863\n",
            "Epoch 45 | train_loss=28.6264 train_pMAE=4.093 train_MS=1.537 || val_loss=37.6483 val_pMAE=4.469 val_MS=2.732\n",
            "Epoch 46 | train_loss=28.4681 train_pMAE=4.090 train_MS=1.539 || val_loss=37.5824 val_pMAE=4.441 val_MS=2.695\n",
            "Epoch 47 | train_loss=28.9984 train_pMAE=4.112 train_MS=1.583 || val_loss=37.7769 val_pMAE=4.463 val_MS=2.739\n",
            "Epoch 48 | train_loss=28.7603 train_pMAE=4.089 train_MS=1.615 || val_loss=37.8289 val_pMAE=4.498 val_MS=2.784\n",
            "Early stopping at epoch 48 (best val pMAE=4.369)\n"
          ]
        }
      ],
      "source": [
        "# ----------------- TRAIN -----------------\n",
        "def train_full_fusion(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01):\n",
        "    rows = read_csv(CSV_PATH)\n",
        "    image_col, vf_cols = detect_columns(rows)\n",
        "    print(f\"[OK] image column: {image_col}\")\n",
        "    print(f\"[OK] {len(vf_cols)} VF cols detected: {vf_cols[:5]} ... {vf_cols[-5:]}\")\n",
        "\n",
        "    # augment with CDR BEFORE splitting\n",
        "    rows = augment_rows_with_cdr_psd(rows, image_col, vf_cols)\n",
        "\n",
        "    import random\n",
        "\n",
        "    random.shuffle(rows)\n",
        "    N = len(rows)\n",
        "    n_train = int(0.8 * N)\n",
        "    train_rows = rows[:n_train]\n",
        "    val_rows = rows[n_train:]\n",
        "\n",
        "    # fit clinical stats on train (handles None via mean imputation)\n",
        "    clin_stats = fit_clinical_stats(train_rows, CLIN_NUM_COLS)\n",
        "    clin_dim = len(CLIN_NUM_COLS) + len(CLIN_CAT_COLS)\n",
        "\n",
        "    # datasets / loaders\n",
        "    train_ds = ROI_ODOC_Clinical_Dataset(train_rows, image_col, vf_cols, clin_stats, train=True)\n",
        "    val_ds = ROI_ODOC_Clinical_Dataset(val_rows, image_col, vf_cols, clin_stats, train=False)\n",
        "\n",
        "    train_dl = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True\n",
        "    )\n",
        "    val_dl = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # model / optimizer\n",
        "    model = MobileNetV3L_5ch_Clinical(clin_dim=clin_dim, out_dim=NUM_POINTS, pretrained=True).to(\n",
        "        DEVICE\n",
        "    )\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    best = float(\"inf\")\n",
        "    no_improve = 0\n",
        "    ckpt = os.path.join(CHECK_DIR, \"best_full_fusion_ROI_ODOC_CLIN1.pth\")\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr = run_epoch(model, train_dl, opt)\n",
        "        va = run_epoch(model, val_dl, None)\n",
        "\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f} train_pMAE={tr['pointwise_mae']:.3f} train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f} val_pMAE={va['pointwise_mae']:.3f} val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        # early stopping on val pointwise MAE\n",
        "        if va[\"pointwise_mae\"] < best - MIN_DELTA:\n",
        "            best = va[\"pointwise_mae\"]\n",
        "            no_improve = 0\n",
        "            torch.save(\n",
        "                {\"epoch\": epoch, \"model\": model.state_dict(), \"val_pointwise_mae\": best}, ckpt\n",
        "            )\n",
        "            print(f\"  -> saved new best to {ckpt} (val pMAE={best:.3f})\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve > PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch} (best val pMAE={best:.3f})\")\n",
        "                break\n",
        "\n",
        "    # expose objects / paths like before\n",
        "    return model, train_dl, val_dl, image_col, vf_cols, ckpt, clin_dim\n",
        "\n",
        "\n",
        "# run training and expose globals\n",
        "model_full, train_dl_full, val_dl_full, image_col_full, vf_cols_full, CKPT_FULL, CLIN_DIM = (\n",
        "    train_full_fusion(EPOCHS=80, PATIENCE=10, MIN_DELTA=0.01)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T04:18:02.710520Z",
          "iopub.status.busy": "2025-11-10T04:18:02.709945Z",
          "iopub.status.idle": "2025-11-10T04:18:04.452619Z",
          "shell.execute_reply": "2025-11-10T04:18:04.451838Z",
          "shell.execute_reply.started": "2025-11-10T04:18:02.710498Z"
        },
        "id": "bw8w9mI4imAZ",
        "outputId": "1e1e2644-7013-4832-e751-6b45e0f63950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Collected predictions: torch.Size([127, 59]) torch.Size([127, 59])\n",
            "\n",
            "== FULL FUSION (ROI + OD/OC + Clinical ): POINTWISE ==\n",
            "RMSE: 6.1533 | MAE: 4.3689 | R²: 0.5461\n",
            "== FULL FUSION: POINTWISE-MEAN / MS ==\n",
            "RMSE: 3.8140 | MAE: 2.5977 | R²: 0.5942\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -------EVALUATE BEST + PRINT METRICS -----------------\n",
        "# reload best\n",
        "best_full = MobileNetV3L_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(\n",
        "    DEVICE\n",
        ")\n",
        "state = torch.load(CKPT_FULL, map_location=DEVICE)\n",
        "best_full.load_state_dict(state[\"model\"])\n",
        "best_full.eval()\n",
        "\n",
        "# collect preds\n",
        "all_true, all_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x5, xclin, y in val_dl_full:\n",
        "        x5, xclin, y = x5.to(DEVICE), xclin.to(DEVICE), y.to(DEVICE)\n",
        "        p = best_full(x5, xclin)\n",
        "        all_true.append(y.cpu())\n",
        "        all_pred.append(p.cpu())\n",
        "\n",
        "y_true = torch.cat(all_true, dim=0)\n",
        "y_pred = torch.cat(all_pred, dim=0)\n",
        "print(\"✅ Collected predictions:\", y_true.shape, y_pred.shape)\n",
        "\n",
        "\n",
        "# paper-style metrics\n",
        "def rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "def mae_value(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "def r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "print(\"\\n== FULL FUSION (ROI + OD/OC + Clinical ): POINTWISE ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(pw_true, pw_pred):.4f} | MAE: {mae_value(pw_true, pw_pred):.4f} | R²: {r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "\n",
        "t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "print(\"== FULL FUSION: POINTWISE-MEAN / MS ==\")\n",
        "print(\n",
        "    f\"RMSE: {rmse(t_mean, p_mean):.4f} | MAE: {mae_value(t_mean, p_mean):.4f} | R²: {r2(t_mean, p_mean):.4f}\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4689tue7imAZ"
      },
      "source": [
        "# 6. ROI + OD/OC + Clinical SWIN-T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:20:07.695024Z",
          "iopub.status.busy": "2025-11-10T04:20:07.694736Z",
          "iopub.status.idle": "2025-11-10T04:20:07.713069Z",
          "shell.execute_reply": "2025-11-10T04:20:07.712345Z",
          "shell.execute_reply.started": "2025-11-10T04:20:07.695004Z"
        },
        "id": "f8OmMqwiimAZ",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# SWIN-T for FULL FUSION (ROI + OD/OC + Clinical) → VF (59)\n",
        "# Reuses train_dl_full, val_dl_full, and CLIN_DIM from your existing setup\n",
        "# ==========================\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "# ---- metrics used inside the loop\n",
        "@torch.no_grad()\n",
        "def _mae(pred, true):\n",
        "    return torch.mean(torch.abs(pred - true)).item()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _ms_mae(pred, true):\n",
        "    pm = pred.mean(dim=1)\n",
        "    tm = true.mean(dim=1)\n",
        "    return torch.mean(torch.abs(pm - tm)).item()\n",
        "\n",
        "\n",
        "def _run_epoch_ff(model, loader, device, opt=None):\n",
        "    train = opt is not None\n",
        "    model.train() if train else model.eval()\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    n = 0\n",
        "    loss_sum = 0.0\n",
        "    pmae_sum = 0.0\n",
        "    msmae_sum = 0.0\n",
        "\n",
        "    for batch in loader:\n",
        "        # x5: (B,5,H,W) ; x_clin: (B, CLIN_DIM) ; y: (B,59)\n",
        "        x5, x_clin, y = batch\n",
        "        x5 = x5.to(device, non_blocking=True)\n",
        "        x_clin = x_clin.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "        pred = model(x5, x_clin)\n",
        "        loss = crit(pred, y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        bs = x5.size(0)\n",
        "        n += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "        pmae_sum += _mae(pred, y) * bs\n",
        "        msmae_sum += _ms_mae(pred, y) * bs\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss_sum / max(1, n),\n",
        "        \"pointwise_mae\": pmae_sum / max(1, n),\n",
        "        \"ms_mae\": msmae_sum / max(1, n),\n",
        "    }\n",
        "\n",
        "\n",
        "# ---- Model: Swin-T (RGB) + small CNN for OD/OC masks + clinical MLP → fused regressor\n",
        "class SwinT_5ch_Clinical(nn.Module):\n",
        "    \"\"\"\n",
        "    Uses Swin-T on RGB (x5[:, :3]),\n",
        "    Encodes OD/OC masks (x5[:, 3:5]) with a lightweight CNN,\n",
        "    Encodes clinical features with an MLP,\n",
        "    Concats [swin_feat, mask_feat, clin_feat] → MLP → 59-D VF regression.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, clin_dim, out_dim=59, pretrained=True, dropout=0.25):\n",
        "        super().__init__()\n",
        "        # Swin-T backbone (ImageNet weights) → (B, feat_dim)\n",
        "        self.backbone = models.swin_t(\n",
        "            weights=models.Swin_T_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        )\n",
        "        feat_dim = self.backbone.head.in_features\n",
        "        self.backbone.head = nn.Identity()  # keep pooled features\n",
        "\n",
        "        # OD/OC mask encoder (2xHxW → vector)\n",
        "        self.mask_enc = nn.Sequential(\n",
        "            nn.Conv2d(2, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),  # (B,128)\n",
        "        )\n",
        "        mask_dim = 128\n",
        "\n",
        "        # clinical encoder\n",
        "        self.clin_head = nn.Sequential(\n",
        "            nn.Linear(clin_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        clin_out = 64\n",
        "\n",
        "        # fusion & regressor\n",
        "        fused_in = feat_dim + mask_dim + clin_out\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(fused_in, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x5, xclin):\n",
        "        x_rgb = x5[:, :3, :, :]\n",
        "        x_msk = x5[:, 3:, :, :]\n",
        "        f_rgb = self.backbone(x_rgb)\n",
        "        f_msk = self.mask_enc(x_msk)\n",
        "        f_cln = self.clin_head(xclin)\n",
        "        z = torch.cat([f_rgb, f_msk, f_cln], dim=1)\n",
        "        return self.regressor(z)\n",
        "\n",
        "\n",
        "# ---- Early Stopping helper\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=10, min_delta=0.01, ckpt_path=None):\n",
        "        self.patience = patience\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.best = float(\"inf\")\n",
        "        self.bad_epochs = 0\n",
        "\n",
        "    def step(self, current, model, epoch_meta=None):\n",
        "        if current < self.best - self.min_delta:\n",
        "            self.best = current\n",
        "            self.bad_epochs = 0\n",
        "            if self.ckpt_path:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": model.state_dict(),\n",
        "                        \"val_pointwise_mae\": self.best,\n",
        "                        **(epoch_meta or {}),\n",
        "                    },\n",
        "                    self.ckpt_path,\n",
        "                )\n",
        "            return False  # do not stop\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "            return self.bad_epochs > self.patience  # stop if exceeded\n",
        "\n",
        "\n",
        "# ---- Train\n",
        "def train_full_fusion_swinT(\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    clin_dim,\n",
        "    out_dim=59,\n",
        "    epochs=80,\n",
        "    lr=1e-4,\n",
        "    wd=1e-4,\n",
        "    device=\"cuda\",\n",
        "    pretrained=True,\n",
        "    patience=10,\n",
        "    min_delta=0.01,\n",
        "    ckpt_dir=\"./checkpoints\",\n",
        "):\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    ckpt = os.path.join(ckpt_dir, \"best_full_fusion_swint.pth\")\n",
        "\n",
        "    model = SwinT_5ch_Clinical(clin_dim=clin_dim, out_dim=out_dim, pretrained=pretrained).to(\n",
        "        device\n",
        "    )\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    # reduce LR when val pMAE plateaus\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", patience=3, factor=0.5)\n",
        "\n",
        "    stopper = EarlyStopper(patience=patience, min_delta=min_delta, ckpt_path=ckpt)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        tr = _run_epoch_ff(model, train_loader, device, opt=opt)\n",
        "        va = _run_epoch_ff(model, val_loader, device, opt=None)\n",
        "        sched.step(va[\"pointwise_mae\"])\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={tr['loss']:.4f} train_pMAE={tr['pointwise_mae']:.3f} train_MS={tr['ms_mae']:.3f} || \"\n",
        "            f\"val_loss={va['loss']:.4f} val_pMAE={va['pointwise_mae']:.3f} val_MS={va['ms_mae']:.3f}\"\n",
        "        )\n",
        "\n",
        "        should_stop = stopper.step(va[\"pointwise_mae\"], model, epoch_meta={\"epoch\": epoch})\n",
        "        if should_stop:\n",
        "            print(f\"Early stopping at epoch {epoch} (best val pMAE={stopper.best:.3f})\")\n",
        "            break\n",
        "\n",
        "    # load best\n",
        "    state = torch.load(ckpt, map_location=device)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    return model, ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T04:22:55.770223Z",
          "iopub.status.busy": "2025-11-10T04:22:55.769445Z",
          "iopub.status.idle": "2025-11-10T04:35:02.461704Z",
          "shell.execute_reply": "2025-11-10T04:35:02.461076Z",
          "shell.execute_reply.started": "2025-11-10T04:22:55.770197Z"
        },
        "id": "aNduTsZFimAa",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# # ===== RUN: SWIN-T FULL-FUSION TRAIN + EVAL =====\n",
        "# from math import sqrt\n",
        "# import os\n",
        "\n",
        "# import torch\n",
        "\n",
        "# # ---- config (tweak if you like)\n",
        "# EPOCHS = 80\n",
        "# LR = 1e-4\n",
        "# WD = 1e-4\n",
        "# PATIENCE = 10\n",
        "# MIN_DELTA = 0.01\n",
        "# CHECK_DIR = \"./checkpoints\"\n",
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# os.makedirs(CHECK_DIR, exist_ok=True)\n",
        "\n",
        "# # ---- quick checks\n",
        "# assert \"train_dl_full\" in globals() and \"val_dl_full\" in globals(), (\n",
        "#     \"Missing loaders. Make sure you created train_dl_full and val_dl_full.\"\n",
        "# )\n",
        "# assert \"CLIN_DIM\" in globals(), \"Missing CLIN_DIM.\"\n",
        "# OUT_DIM = 59 if \"NUM_POINTS\" not in globals() else int(NUM_POINTS)\n",
        "\n",
        "# # ---- train\n",
        "# swinT_model, SWINT_CKPT = train_full_fusion_swinT(\n",
        "#     train_loader=train_dl_full,\n",
        "#     val_loader=val_dl_full,\n",
        "#     clin_dim=CLIN_DIM,\n",
        "#     out_dim=OUT_DIM,\n",
        "#     epochs=EPOCHS,\n",
        "#     lr=LR,\n",
        "#     wd=WD,\n",
        "#     device=DEVICE,\n",
        "#     patience=PATIENCE,\n",
        "#     min_delta=MIN_DELTA,\n",
        "#     ckpt_dir=CHECK_DIR,\n",
        "#     pretrained=True,\n",
        "# )\n",
        "\n",
        "# print(f\"\\n[OK] Training finished. Best checkpoint: {SWINT_CKPT}\")\n",
        "\n",
        "\n",
        "# # ---- evaluation helpers (pointwise + mean-of-points/“MS”)\n",
        "# @torch.no_grad()\n",
        "# def _rmse(a, b):\n",
        "#     return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def _mae_scalar(a, b):\n",
        "#     return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def _r2(a, b):\n",
        "#     ss_res = torch.sum((a - b) ** 2)\n",
        "#     ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "#     return float(1.0 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# # ---- load best and evaluate on val\n",
        "# best_swinT = SwinT_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=OUT_DIM, pretrained=False).to(DEVICE)\n",
        "# state = torch.load(SWINT_CKPT, map_location=DEVICE)\n",
        "# best_swinT.load_state_dict(state[\"model\"])\n",
        "# best_swinT.eval()\n",
        "\n",
        "# y_true, y_pred = [], []\n",
        "# with torch.no_grad():\n",
        "#     for x5, xclin, y in val_dl_full:\n",
        "#         x5 = x5.to(DEVICE)\n",
        "#         xclin = xclin.to(DEVICE)\n",
        "#         y = y.to(DEVICE)\n",
        "#         p = best_swinT(x5, xclin)\n",
        "#         y_true.append(y.cpu())\n",
        "#         y_pred.append(p.cpu())\n",
        "\n",
        "# y_true = torch.cat(y_true, dim=0)\n",
        "# y_pred = torch.cat(y_pred, dim=0)\n",
        "\n",
        "# # pointwise metrics\n",
        "# pw_true, pw_pred = y_true.reshape(-1), y_pred.reshape(-1)\n",
        "# print(\"\\n== SWIN-T FULL FUSION: POINTWISE ==\")\n",
        "# print(\n",
        "#     f\"RMSE: {_rmse(pw_true, pw_pred):.4f} | MAE: {_mae_scalar(pw_true, pw_pred):.4f} | R²: {_r2(pw_true, pw_pred):.4f}\"\n",
        "# )\n",
        "\n",
        "# # mean-of-points (“MS”) metrics\n",
        "# t_mean, p_mean = y_true.mean(dim=1), y_pred.mean(dim=1)\n",
        "# print(\"== SWIN-T FULL FUSION: MEAN (MS) ==\")\n",
        "# print(\n",
        "#     f\"RMSE: {_rmse(t_mean, p_mean):.4f} | MAE: {_mae_scalar(t_mean, p_mean):.4f} | R²: {_r2(t_mean, p_mean):.4f}\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-08T11:59:52.375643Z",
          "iopub.status.busy": "2025-11-08T11:59:52.375380Z",
          "iopub.status.idle": "2025-11-08T11:59:52.379449Z",
          "shell.execute_reply": "2025-11-08T11:59:52.378654Z",
          "shell.execute_reply.started": "2025-11-08T11:59:52.375625Z"
        },
        "id": "swAc1WKsimAb"
      },
      "source": [
        "# 7. Weighted Averaging Ensemble Technique (MobileNetV3L + SWIN-T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T04:36:47.133505Z",
          "iopub.status.busy": "2025-11-10T04:36:47.132776Z",
          "iopub.status.idle": "2025-11-10T04:36:50.037569Z",
          "shell.execute_reply": "2025-11-10T04:36:50.036982Z",
          "shell.execute_reply.started": "2025-11-10T04:36:47.133480Z"
        },
        "id": "qqPL_iAOimAb",
        "lines_to_next_cell": 2,
        "outputId": "2e18ee04-8a8f-4b75-ff34-a8285dbe4117"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "== INDIVIDUAL MODELS (pointwise) ==\n",
            "MobileNet  → RMSE: 6.1533 | MAE: 4.3689 | R²: 0.5461\n",
            "Swin-B  → RMSE: 5.8988 | MAE: 4.1052 | R²: 0.5829\n",
            "\n",
            "== ENSEMBLE (pointwise) ==\n",
            "Avg (α=0.50) → RMSE: 5.8362 | MAE: 4.1308 | R²: 0.5917\n",
            "\n",
            "== INDIVIDUAL MODELS (MS) ==\n",
            "MobileNet  → RMSE: 3.8140 | MAE: 2.5977 | R²: 0.5942\n",
            "Swin-B  → RMSE: 3.4719 | MAE: 2.4142 | R²: 0.6637\n",
            "\n",
            "== ENSEMBLE (MS) ==\n",
            "Avg (α=0.50) → RMSE: 3.3722 | MAE: 2.3352 | R²: 0.6828\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# Simple Ensemble: MobileNetV3L_5ch_Clinical + Swint_5ch_Clinical\n",
        "# Uses the SAME loaders (val_dl_full / test_dl_full) as your full-fusion setup\n",
        "# ==========================\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ---- your checkpoint paths\n",
        "RESNET_CKPT_PATH = CHECK_DIR + \"/best_full_fusion_ROI_ODOC_CLIN1.pth\"\n",
        "SWIN_CKPT_PATH = CHECK_DIR + \"/best_full_fusion_swint.pth\"\n",
        "\n",
        "\n",
        "# ---- small metric helpers (names won't clash with your existing ones)\n",
        "@torch.no_grad()\n",
        "def _rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _mae(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "def _load_model_states():\n",
        "    # Build the exact architectures (no pretrain needed when loading checkpoints)\n",
        "    resnet = MobileNetV3L_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(\n",
        "        DEVICE\n",
        "    )\n",
        "    swin = SwinT_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "\n",
        "    r_state = torch.load(RESNET_CKPT_PATH, map_location=DEVICE)\n",
        "    s_state = torch.load(SWIN_CKPT_PATH, map_location=DEVICE)\n",
        "\n",
        "    resnet.load_state_dict(r_state[\"model\"])\n",
        "    resnet.eval()\n",
        "    swin.load_state_dict(s_state[\"model\"])\n",
        "    swin.eval()\n",
        "    return resnet, swin\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ensemble_eval(loader, alpha=0.5):\n",
        "    \"\"\"\n",
        "    alpha: weight for SWIN (0..1). 0.5 = simple average\n",
        "    pred = (1-alpha)*resnet + alpha*swin\n",
        "    \"\"\"\n",
        "    assert 0.0 <= alpha <= 1.0\n",
        "    resnet, swin = _load_model_states()\n",
        "\n",
        "    y_true_chunks, y_pred_res_chunks, y_pred_swin_chunks, y_pred_ens_chunks = [], [], [], []\n",
        "\n",
        "    for x5, xclin, y in loader:\n",
        "        x5 = x5.to(DEVICE, non_blocking=True)\n",
        "        xcli = xclin.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        p_r = resnet(x5, xcli)  # (B, 59)\n",
        "        p_s = swin(x5, xcli)  # (B, 59)\n",
        "        p_e = (1.0 - alpha) * p_r + alpha * p_s\n",
        "\n",
        "        y_true_chunks.append(y.cpu())\n",
        "        y_pred_res_chunks.append(p_r.cpu())\n",
        "        y_pred_swin_chunks.append(p_s.cpu())\n",
        "        y_pred_ens_chunks.append(p_e.cpu())\n",
        "\n",
        "    y_true = torch.cat(y_true_chunks, dim=0)\n",
        "    p_res = torch.cat(y_pred_res_chunks, dim=0)\n",
        "    p_swin = torch.cat(y_pred_swin_chunks, dim=0)\n",
        "    p_ens = torch.cat(y_pred_ens_chunks, dim=0)\n",
        "\n",
        "    # pointwise metrics (flatten all 59 points)\n",
        "    pw_true, pw_res, pw_swin, pw_ens = (\n",
        "        y_true.reshape(-1),\n",
        "        p_res.reshape(-1),\n",
        "        p_swin.reshape(-1),\n",
        "        p_ens.reshape(-1),\n",
        "    )\n",
        "\n",
        "    print(\"\\n== INDIVIDUAL MODELS (pointwise) ==\")\n",
        "    print(\n",
        "        f\"MobileNet  → RMSE: {_rmse(pw_true, pw_res):.4f} | MAE: {_mae(pw_true, pw_res):.4f} | R²: {_r2(pw_true, pw_res):.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Swin-B  → RMSE: {_rmse(pw_true, pw_swin):.4f} | MAE: {_mae(pw_true, pw_swin):.4f} | R²: {_r2(pw_true, pw_swin):.4f}\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n== ENSEMBLE (pointwise) ==\")\n",
        "    print(\n",
        "        f\"Avg (α={alpha:.2f}) → RMSE: {_rmse(pw_true, pw_ens):.4f} | MAE: {_mae(pw_true, pw_ens):.4f} | R²: {_r2(pw_true, pw_ens):.4f}\"\n",
        "    )\n",
        "\n",
        "    # MS metrics = mean of 59 points per sample\n",
        "    t_mean, r_mean, s_mean, e_mean = (\n",
        "        y_true.mean(dim=1),\n",
        "        p_res.mean(dim=1),\n",
        "        p_swin.mean(dim=1),\n",
        "        p_ens.mean(dim=1),\n",
        "    )\n",
        "\n",
        "    print(\"\\n== INDIVIDUAL MODELS (MS) ==\")\n",
        "    print(\n",
        "        f\"MobileNet  → RMSE: {_rmse(t_mean, r_mean):.4f} | MAE: {_mae(t_mean, r_mean):.4f} | R²: {_r2(t_mean, r_mean):.4f}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Swin-B  → RMSE: {_rmse(t_mean, s_mean):.4f} | MAE: {_mae(t_mean, s_mean):.4f} | R²: {_r2(t_mean, s_mean):.4f}\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n== ENSEMBLE (MS) ==\")\n",
        "    print(\n",
        "        f\"Avg (α={alpha:.2f}) → RMSE: {_rmse(t_mean, e_mean):.4f} | MAE: {_mae(t_mean, e_mean):.4f} | R²: {_r2(t_mean, e_mean):.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"y_true\": y_true,\n",
        "        \"pred_resnet\": p_res,\n",
        "        \"pred_swin\": p_swin,\n",
        "        \"pred_ensemble\": p_ens,\n",
        "    }\n",
        "\n",
        "\n",
        "# ===== RUN on your existing loaders =====\n",
        "# Use val set:\n",
        "assert \"val_dl_full\" in globals(), \"val_dl_full not found. Run your full-fusion data cell first.\"\n",
        "_ = ensemble_eval(val_dl_full, alpha=0.5)  # try alpha=0.3, 0.7, etc.\n",
        "\n",
        "# If you also have test_dl_full:\n",
        "# assert 'test_dl_full' in globals()\n",
        "# _ = ensemble_eval(test_dl_full, alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T04:37:07.068710Z",
          "iopub.status.busy": "2025-11-10T04:37:07.068455Z",
          "iopub.status.idle": "2025-11-10T04:37:09.933957Z",
          "shell.execute_reply": "2025-11-10T04:37:09.933312Z",
          "shell.execute_reply.started": "2025-11-10T04:37:07.068691Z"
        },
        "id": "FHZOSfUQimAc",
        "lines_to_next_cell": 2,
        "outputId": "9692b44c-5a83-4f96-95fc-aad3ca6a1d95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Ensemble] best alpha=0.80 (weights: SWIN=0.80, MOBILENET=0.20)\n",
            "POINTWISE:\n",
            "  RMSE=5.8273 | MAE=4.0901 | R²=0.5929\n",
            "MS (mean sensitivity):\n",
            "  RMSE=3.3642 | MAE=2.3283 | R²=0.6842\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# SIMPLE ENSEMBLE: ResNet (full fusion) + Swin-B (full fusion)\n",
        "# Grid-search alpha on VAL to weight SWIN higher/lower\n",
        "# ==========================\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# --- Metrics (same style you used)\n",
        "@torch.no_grad()\n",
        "def _rmse(a, b):\n",
        "    return float(torch.sqrt(torch.mean((a - b) ** 2)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _mae(a, b):\n",
        "    return float(torch.mean(torch.abs(a - b)))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _r2(a, b):\n",
        "    ss_res = torch.sum((a - b) ** 2)\n",
        "    ss_tot = torch.sum((a - torch.mean(a)) ** 2) + 1e-12\n",
        "    return float(1 - ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# --- Load both models\n",
        "RESNET_CKPT = CHECK_DIR + \"/best_full_fusion_ROI_ODOC_CLIN1.pth\"\n",
        "SWIN_CKPT = CHECK_DIR + \"/best_full_fusion_swint.pth\"\n",
        "\n",
        "resnet = MobileNetV3L_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(\n",
        "    DEVICE\n",
        ")\n",
        "swin = SwinT_5ch_Clinical(clin_dim=CLIN_DIM, out_dim=NUM_POINTS, pretrained=False).to(DEVICE)\n",
        "\n",
        "resnet.load_state_dict(torch.load(RESNET_CKPT, map_location=DEVICE)[\"model\"])\n",
        "swin.load_state_dict(torch.load(SWIN_CKPT, map_location=DEVICE)[\"model\"])\n",
        "resnet.eval()\n",
        "swin.eval()\n",
        "\n",
        "# --- Collect full VAL predictions once for speed\n",
        "y_true_list, pred_resnet_list, pred_swin_list = [], [], []\n",
        "with torch.no_grad():\n",
        "    for x5, xclin, y in val_dl_full:\n",
        "        x5 = x5.to(DEVICE)\n",
        "        xclin = xclin.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        pr = resnet(x5, xclin)\n",
        "        ps = swin(x5, xclin)\n",
        "        y_true_list.append(y.cpu())\n",
        "        pred_resnet_list.append(pr.cpu())\n",
        "        pred_swin_list.append(ps.cpu())\n",
        "\n",
        "y_true = torch.cat(y_true_list, dim=0)  # (N, 59)\n",
        "pred_r = torch.cat(pred_resnet_list, dim=0)  # (N, 59)\n",
        "pred_s = torch.cat(pred_swin_list, dim=0)  # (N, 59)\n",
        "\n",
        "# --- Grid-search alpha in [0,1] to minimize pointwise MAE (you can switch to MS if preferred)\n",
        "best_alpha, best_mae = None, float(\"inf\")\n",
        "for a in [i / 20 for i in range(21)]:  # 0.00, 0.05, ..., 1.00\n",
        "    ens = a * pred_s + (1 - a) * pred_r\n",
        "    mae_pw = _mae(ens.reshape(-1), y_true.reshape(-1))\n",
        "    if mae_pw < best_mae:\n",
        "        best_mae = mae_pw\n",
        "        best_alpha = a\n",
        "\n",
        "# --- Final ensemble metrics with the chosen alpha\n",
        "ens = best_alpha * pred_s + (1 - best_alpha) * pred_r\n",
        "pw_true, pw_pred = y_true.reshape(-1), ens.reshape(-1)\n",
        "print(\n",
        "    f\"\\n[Ensemble] best alpha={best_alpha:.2f} (weights: SWIN={best_alpha:.2f}, MOBILENET={(1 - best_alpha):.2f})\"\n",
        ")\n",
        "print(\"POINTWISE:\")\n",
        "print(\n",
        "    f\"  RMSE={_rmse(pw_true, pw_pred):.4f} | MAE={_mae(pw_true, pw_pred):.4f} | R²={_r2(pw_true, pw_pred):.4f}\"\n",
        ")\n",
        "t_mean, p_mean = y_true.mean(dim=1), ens.mean(dim=1)\n",
        "print(\"MS (mean sensitivity):\")\n",
        "print(\n",
        "    f\"  RMSE={_rmse(t_mean, p_mean):.4f} | MAE={_mae(t_mean, p_mean):.4f} | R²={_r2(t_mean, p_mean):.4f}\"\n",
        ")\n",
        "\n",
        "# Save alpha if you want to reuse for test-time ensembling\n",
        "BEST_ALPHA = best_alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T06:35:33.870233Z",
          "iopub.status.busy": "2025-11-10T06:35:33.869481Z",
          "iopub.status.idle": "2025-11-10T06:35:34.053693Z",
          "shell.execute_reply": "2025-11-10T06:35:34.052990Z",
          "shell.execute_reply.started": "2025-11-10T06:35:33.870203Z"
        },
        "id": "fUWdft6qimAd",
        "outputId": "64a5a491-f5bd-4e95-8483-69b82dff45c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_full_fusion_ROI_ODOC_CLIN1.pth  best_mobilenetv3l_original_roi.pth\n",
            "best_full_fusion_swint.pth\t     best_mobilenetv3l_ROI_ODOC_CDR.pth\n",
            "best_mobilenetv3l_original_cfp.pth   best_mobilenetv3l_ROI_ODOC.pth\n"
          ]
        }
      ],
      "source": [
        "!ls $CHECK_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "execution": {
          "iopub.execute_input": "2025-11-10T06:37:19.823311Z",
          "iopub.status.busy": "2025-11-10T06:37:19.823011Z",
          "iopub.status.idle": "2025-11-10T06:37:49.186221Z",
          "shell.execute_reply": "2025-11-10T06:37:49.185475Z",
          "shell.execute_reply.started": "2025-11-10T06:37:19.823284Z"
        },
        "id": "QwbAuDhSimAd",
        "outputId": "d9c016d8-0219-4827-c6a3-93da138840b5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<a href='final_checkpoints_archive.zip' target='_blank'>final_checkpoints_archive.zip</a><br>"
            ],
            "text/plain": [
              "/content/final_checkpoints_archive.zip"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "from IPython.display import FileLink\n",
        "\n",
        "shutil.make_archive(\"final_checkpoints_archive\", \"zip\", CHECK_DIR)\n",
        "FileLink(\"final_checkpoints_archive.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "formats": "ipynb,py:percent"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 8671492,
          "sourceId": 13641781,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
